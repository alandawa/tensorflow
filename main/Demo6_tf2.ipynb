{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 平均損失函數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\shise\\AppData\\Local\\Temp\\ipykernel_9104\\2419310379.py:3: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
      "\n",
      "tf.Tensor(2.3536828, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "tf.compat.v1.reset_default_graph()\n",
    "class MNISTLoader():\n",
    "    def __init__(self):\n",
    "        mnist = tf.keras.datasets.mnist\n",
    "        (self.train_data, self.train_label), (self.test_data, self.test_label) = mnist.load_data()\n",
    "\n",
    "        self.train_data = np.expand_dims(self.train_data.astype(np.float32) / 255.0, axis=-1)      # [60000, 28, 28, 1]\n",
    "        self.test_data = np.expand_dims(self.test_data.astype(np.float32) / 255.0, axis=-1)        # [10000, 28, 28, 1]\n",
    "        self.train_label = self.train_label.astype(np.int32)    # [60000]\n",
    "        self.test_label = self.test_label.astype(np.int32)      # [10000]\n",
    "        self.num_train_data, self.num_test_data = self.train_data.shape[0], self.test_data.shape[0]\n",
    "\n",
    "    def get_batch(self, batch_size):\n",
    "\n",
    "        index = np.random.randint(0, self.num_train_data, batch_size)\n",
    "        return self.train_data[index, :], self.train_label[index]\n",
    "    \n",
    "class MLP(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = tf.keras.layers.Flatten()    \n",
    "        self.dense1 = tf.keras.layers.Dense(units=100, activation=tf.nn.relu)\n",
    "        self.dense2 = tf.keras.layers.Dense(units=10)\n",
    "\n",
    "    def call(self, inputs):         # [batch_size, 28, 28, 1]\n",
    "        x = self.flatten(inputs)    # [batch_size, 784]\n",
    "        x = self.dense1(x)          # [batch_size, 100]\n",
    "        x = self.dense2(x)          # [batch_size, 10]\n",
    "        output = tf.nn.softmax(x)\n",
    "        return output\n",
    "\n",
    "num_epochs = 5\n",
    "batch_size = 5\n",
    "learning_rate = 0.001\n",
    "\n",
    "model = MLP()\n",
    "data_loader = MNISTLoader()\n",
    "\n",
    "num_batches = int(data_loader.num_train_data // batch_size * num_epochs)\n",
    "\n",
    "X, y = data_loader.get_batch(batch_size)\n",
    "with tf.GradientTape() as tape:\n",
    "    y_pred = model(X)\n",
    "    loss = tf.keras.losses.sparse_categorical_crossentropy(y_true=y, y_pred=y_pred)\n",
    "    loss = tf.reduce_mean(loss)\n",
    "    print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 優化神經網路"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 0: loss 2.396677\n",
      "batch 1: loss 2.112873\n",
      "batch 2: loss 2.183515\n",
      "batch 3: loss 1.943295\n",
      "batch 4: loss 1.820923\n",
      "batch 5: loss 1.697846\n",
      "batch 6: loss 1.571865\n",
      "batch 7: loss 1.597428\n",
      "batch 8: loss 1.628342\n",
      "batch 9: loss 1.309693\n",
      "batch 10: loss 1.454890\n",
      "batch 11: loss 1.258303\n",
      "batch 12: loss 1.285961\n",
      "batch 13: loss 1.254707\n",
      "batch 14: loss 0.994358\n",
      "batch 15: loss 0.893446\n",
      "batch 16: loss 1.137083\n",
      "batch 17: loss 1.004136\n",
      "batch 18: loss 1.031638\n",
      "batch 19: loss 0.982998\n",
      "batch 20: loss 1.008984\n",
      "batch 21: loss 0.963277\n",
      "batch 22: loss 1.035021\n",
      "batch 23: loss 0.797306\n",
      "batch 24: loss 0.860072\n",
      "batch 25: loss 0.840207\n",
      "batch 26: loss 0.702268\n",
      "batch 27: loss 0.723299\n",
      "batch 28: loss 0.964678\n",
      "batch 29: loss 0.712005\n",
      "batch 30: loss 0.681655\n",
      "batch 31: loss 0.786665\n",
      "batch 32: loss 0.619811\n",
      "batch 33: loss 0.650310\n",
      "batch 34: loss 0.714649\n",
      "batch 35: loss 0.610785\n",
      "batch 36: loss 0.833940\n",
      "batch 37: loss 0.628161\n",
      "batch 38: loss 0.661477\n",
      "batch 39: loss 0.724105\n",
      "batch 40: loss 0.604361\n",
      "batch 41: loss 0.602755\n",
      "batch 42: loss 0.645004\n",
      "batch 43: loss 0.648488\n",
      "batch 44: loss 0.648570\n",
      "batch 45: loss 0.660449\n",
      "batch 46: loss 0.414498\n",
      "batch 47: loss 0.514822\n",
      "batch 48: loss 0.635295\n",
      "batch 49: loss 0.492066\n",
      "batch 50: loss 0.537626\n",
      "batch 51: loss 0.528162\n",
      "batch 52: loss 0.604095\n",
      "batch 53: loss 0.580979\n",
      "batch 54: loss 0.467751\n",
      "batch 55: loss 0.664320\n",
      "batch 56: loss 0.535672\n",
      "batch 57: loss 0.583294\n",
      "batch 58: loss 0.475276\n",
      "batch 59: loss 0.547358\n",
      "batch 60: loss 0.731060\n",
      "batch 61: loss 0.375860\n",
      "batch 62: loss 0.532790\n",
      "batch 63: loss 0.422035\n",
      "batch 64: loss 0.462789\n",
      "batch 65: loss 0.370928\n",
      "batch 66: loss 0.653713\n",
      "batch 67: loss 0.356253\n",
      "batch 68: loss 0.541352\n",
      "batch 69: loss 0.415037\n",
      "batch 70: loss 0.676986\n",
      "batch 71: loss 0.497761\n",
      "batch 72: loss 0.638351\n",
      "batch 73: loss 0.442429\n",
      "batch 74: loss 0.381867\n",
      "batch 75: loss 0.485087\n",
      "batch 76: loss 0.515794\n",
      "batch 77: loss 0.462379\n",
      "batch 78: loss 0.208225\n",
      "batch 79: loss 0.441380\n",
      "batch 80: loss 0.347451\n",
      "batch 81: loss 0.551302\n",
      "batch 82: loss 0.399126\n",
      "batch 83: loss 0.508182\n",
      "batch 84: loss 0.528096\n",
      "batch 85: loss 0.445633\n",
      "batch 86: loss 0.478026\n",
      "batch 87: loss 0.512503\n",
      "batch 88: loss 0.287083\n",
      "batch 89: loss 0.543547\n",
      "batch 90: loss 0.289308\n",
      "batch 91: loss 0.566605\n",
      "batch 92: loss 0.562361\n",
      "batch 93: loss 0.251192\n",
      "batch 94: loss 0.528721\n",
      "batch 95: loss 0.483363\n",
      "batch 96: loss 0.260948\n",
      "batch 97: loss 0.454956\n",
      "batch 98: loss 0.295904\n",
      "batch 99: loss 0.945269\n",
      "batch 100: loss 0.669780\n",
      "batch 101: loss 0.493374\n",
      "batch 102: loss 0.537539\n",
      "batch 103: loss 0.585702\n",
      "batch 104: loss 0.329935\n",
      "batch 105: loss 0.327855\n",
      "batch 106: loss 0.334634\n",
      "batch 107: loss 0.437275\n",
      "batch 108: loss 0.332275\n",
      "batch 109: loss 0.614158\n",
      "batch 110: loss 0.436764\n",
      "batch 111: loss 0.241592\n",
      "batch 112: loss 0.515942\n",
      "batch 113: loss 0.386755\n",
      "batch 114: loss 0.367428\n",
      "batch 115: loss 0.355644\n",
      "batch 116: loss 0.431311\n",
      "batch 117: loss 0.592790\n",
      "batch 118: loss 0.555149\n",
      "batch 119: loss 0.437302\n",
      "batch 120: loss 0.331943\n",
      "batch 121: loss 0.481132\n",
      "batch 122: loss 0.453973\n",
      "batch 123: loss 0.387931\n",
      "batch 124: loss 0.418809\n",
      "batch 125: loss 0.325821\n",
      "batch 126: loss 0.494728\n",
      "batch 127: loss 0.548063\n",
      "batch 128: loss 0.340368\n",
      "batch 129: loss 0.373355\n",
      "batch 130: loss 0.422038\n",
      "batch 131: loss 0.743694\n",
      "batch 132: loss 0.553067\n",
      "batch 133: loss 0.283057\n",
      "batch 134: loss 0.347728\n",
      "batch 135: loss 0.421729\n",
      "batch 136: loss 0.273376\n",
      "batch 137: loss 0.400966\n",
      "batch 138: loss 0.572578\n",
      "batch 139: loss 0.423897\n",
      "batch 140: loss 0.193791\n",
      "batch 141: loss 0.309761\n",
      "batch 142: loss 0.216192\n",
      "batch 143: loss 0.366315\n",
      "batch 144: loss 0.406723\n",
      "batch 145: loss 0.205405\n",
      "batch 146: loss 0.400857\n",
      "batch 147: loss 0.365707\n",
      "batch 148: loss 0.315857\n",
      "batch 149: loss 0.553192\n",
      "batch 150: loss 0.341136\n",
      "batch 151: loss 0.441673\n",
      "batch 152: loss 0.400934\n",
      "batch 153: loss 0.356367\n",
      "batch 154: loss 0.377257\n",
      "batch 155: loss 0.523059\n",
      "batch 156: loss 0.455821\n",
      "batch 157: loss 0.252494\n",
      "batch 158: loss 0.460402\n",
      "batch 159: loss 0.414116\n",
      "batch 160: loss 0.579349\n",
      "batch 161: loss 0.521521\n",
      "batch 162: loss 0.484840\n",
      "batch 163: loss 0.232494\n",
      "batch 164: loss 0.616344\n",
      "batch 165: loss 0.382224\n",
      "batch 166: loss 0.606873\n",
      "batch 167: loss 0.517416\n",
      "batch 168: loss 0.234022\n",
      "batch 169: loss 0.409400\n",
      "batch 170: loss 0.257738\n",
      "batch 171: loss 0.379694\n",
      "batch 172: loss 0.313670\n",
      "batch 173: loss 0.489292\n",
      "batch 174: loss 0.595493\n",
      "batch 175: loss 0.346379\n",
      "batch 176: loss 0.390046\n",
      "batch 177: loss 0.431126\n",
      "batch 178: loss 0.381923\n",
      "batch 179: loss 0.337879\n",
      "batch 180: loss 0.184108\n",
      "batch 181: loss 0.337977\n",
      "batch 182: loss 0.461611\n",
      "batch 183: loss 0.484434\n",
      "batch 184: loss 0.331855\n",
      "batch 185: loss 0.515552\n",
      "batch 186: loss 0.430097\n",
      "batch 187: loss 0.241964\n",
      "batch 188: loss 0.262341\n",
      "batch 189: loss 0.466920\n",
      "batch 190: loss 0.247884\n",
      "batch 191: loss 0.567815\n",
      "batch 192: loss 0.238666\n",
      "batch 193: loss 0.225548\n",
      "batch 194: loss 0.330548\n",
      "batch 195: loss 0.586367\n",
      "batch 196: loss 0.379449\n",
      "batch 197: loss 0.223364\n",
      "batch 198: loss 0.281062\n",
      "batch 199: loss 0.278877\n",
      "batch 200: loss 0.322248\n",
      "batch 201: loss 0.482403\n",
      "batch 202: loss 0.434196\n",
      "batch 203: loss 0.279337\n",
      "batch 204: loss 0.368517\n",
      "batch 205: loss 0.366608\n",
      "batch 206: loss 0.292800\n",
      "batch 207: loss 0.388955\n",
      "batch 208: loss 0.312518\n",
      "batch 209: loss 0.439192\n",
      "batch 210: loss 0.338869\n",
      "batch 211: loss 0.487311\n",
      "batch 212: loss 0.173659\n",
      "batch 213: loss 0.509527\n",
      "batch 214: loss 0.431561\n",
      "batch 215: loss 0.372232\n",
      "batch 216: loss 0.215559\n",
      "batch 217: loss 0.344084\n",
      "batch 218: loss 0.339221\n",
      "batch 219: loss 0.371243\n",
      "batch 220: loss 0.435487\n",
      "batch 221: loss 0.346589\n",
      "batch 222: loss 0.462041\n",
      "batch 223: loss 0.245483\n",
      "batch 224: loss 0.197810\n",
      "batch 225: loss 0.589060\n",
      "batch 226: loss 0.248748\n",
      "batch 227: loss 0.208824\n",
      "batch 228: loss 0.375792\n",
      "batch 229: loss 0.387451\n",
      "batch 230: loss 0.234374\n",
      "batch 231: loss 0.303093\n",
      "batch 232: loss 0.288306\n",
      "batch 233: loss 0.304482\n",
      "batch 234: loss 0.436843\n",
      "batch 235: loss 0.172620\n",
      "batch 236: loss 0.245223\n",
      "batch 237: loss 0.329869\n",
      "batch 238: loss 0.452090\n",
      "batch 239: loss 0.352307\n",
      "batch 240: loss 0.205250\n",
      "batch 241: loss 0.285134\n",
      "batch 242: loss 0.285932\n",
      "batch 243: loss 0.260991\n",
      "batch 244: loss 0.401865\n",
      "batch 245: loss 0.241549\n",
      "batch 246: loss 0.301042\n",
      "batch 247: loss 0.499777\n",
      "batch 248: loss 0.229269\n",
      "batch 249: loss 0.267231\n",
      "batch 250: loss 0.412053\n",
      "batch 251: loss 0.248988\n",
      "batch 252: loss 0.359493\n",
      "batch 253: loss 0.417713\n",
      "batch 254: loss 0.164809\n",
      "batch 255: loss 0.309114\n",
      "batch 256: loss 0.430489\n",
      "batch 257: loss 0.264200\n",
      "batch 258: loss 0.099663\n",
      "batch 259: loss 0.290757\n",
      "batch 260: loss 0.287228\n",
      "batch 261: loss 0.450058\n",
      "batch 262: loss 0.168573\n",
      "batch 263: loss 0.386184\n",
      "batch 264: loss 0.336612\n",
      "batch 265: loss 0.205197\n",
      "batch 266: loss 0.210968\n",
      "batch 267: loss 0.138706\n",
      "batch 268: loss 0.319624\n",
      "batch 269: loss 0.384571\n",
      "batch 270: loss 0.393170\n",
      "batch 271: loss 0.181978\n",
      "batch 272: loss 0.304703\n",
      "batch 273: loss 0.432648\n",
      "batch 274: loss 0.217445\n",
      "batch 275: loss 0.275883\n",
      "batch 276: loss 0.303788\n",
      "batch 277: loss 0.407593\n",
      "batch 278: loss 0.197473\n",
      "batch 279: loss 0.353178\n",
      "batch 280: loss 0.258142\n",
      "batch 281: loss 0.314759\n",
      "batch 282: loss 0.384690\n",
      "batch 283: loss 0.235729\n",
      "batch 284: loss 0.673850\n",
      "batch 285: loss 0.310142\n",
      "batch 286: loss 0.179954\n",
      "batch 287: loss 0.603124\n",
      "batch 288: loss 0.255097\n",
      "batch 289: loss 0.284839\n",
      "batch 290: loss 0.321211\n",
      "batch 291: loss 0.294632\n",
      "batch 292: loss 0.205115\n",
      "batch 293: loss 0.289198\n",
      "batch 294: loss 0.381934\n",
      "batch 295: loss 0.185289\n",
      "batch 296: loss 0.464164\n",
      "batch 297: loss 0.252944\n",
      "batch 298: loss 0.196267\n",
      "batch 299: loss 0.417721\n",
      "batch 300: loss 0.236192\n",
      "batch 301: loss 0.327738\n",
      "batch 302: loss 0.298487\n",
      "batch 303: loss 0.310787\n",
      "batch 304: loss 0.251460\n",
      "batch 305: loss 0.304837\n",
      "batch 306: loss 0.191991\n",
      "batch 307: loss 0.196761\n",
      "batch 308: loss 0.532499\n",
      "batch 309: loss 0.384858\n",
      "batch 310: loss 0.201254\n",
      "batch 311: loss 0.428465\n",
      "batch 312: loss 0.441392\n",
      "batch 313: loss 0.356089\n",
      "batch 314: loss 0.266302\n",
      "batch 315: loss 0.313034\n",
      "batch 316: loss 0.198655\n",
      "batch 317: loss 0.334203\n",
      "batch 318: loss 0.327307\n",
      "batch 319: loss 0.228676\n",
      "batch 320: loss 0.128429\n",
      "batch 321: loss 0.210006\n",
      "batch 322: loss 0.425920\n",
      "batch 323: loss 0.356107\n",
      "batch 324: loss 0.268313\n",
      "batch 325: loss 0.250779\n",
      "batch 326: loss 0.210694\n",
      "batch 327: loss 0.292473\n",
      "batch 328: loss 0.188040\n",
      "batch 329: loss 0.090787\n",
      "batch 330: loss 0.260943\n",
      "batch 331: loss 0.214948\n",
      "batch 332: loss 0.156858\n",
      "batch 333: loss 0.194228\n",
      "batch 334: loss 0.227938\n",
      "batch 335: loss 0.188116\n",
      "batch 336: loss 0.173592\n",
      "batch 337: loss 0.536844\n",
      "batch 338: loss 0.309407\n",
      "batch 339: loss 0.118541\n",
      "batch 340: loss 0.158969\n",
      "batch 341: loss 0.302202\n",
      "batch 342: loss 0.230431\n",
      "batch 343: loss 0.216007\n",
      "batch 344: loss 0.336370\n",
      "batch 345: loss 0.577504\n",
      "batch 346: loss 0.234761\n",
      "batch 347: loss 0.285631\n",
      "batch 348: loss 0.244106\n",
      "batch 349: loss 0.257392\n",
      "batch 350: loss 0.305453\n",
      "batch 351: loss 0.344168\n",
      "batch 352: loss 0.140300\n",
      "batch 353: loss 0.178816\n",
      "batch 354: loss 0.334254\n",
      "batch 355: loss 0.310405\n",
      "batch 356: loss 0.544200\n",
      "batch 357: loss 0.319989\n",
      "batch 358: loss 0.485318\n",
      "batch 359: loss 0.150130\n",
      "batch 360: loss 0.358674\n",
      "batch 361: loss 0.492612\n",
      "batch 362: loss 0.272663\n",
      "batch 363: loss 0.347384\n",
      "batch 364: loss 0.338788\n",
      "batch 365: loss 0.326326\n",
      "batch 366: loss 0.228072\n",
      "batch 367: loss 0.494900\n",
      "batch 368: loss 0.293829\n",
      "batch 369: loss 0.571056\n",
      "batch 370: loss 0.160371\n",
      "batch 371: loss 0.310828\n",
      "batch 372: loss 0.504510\n",
      "batch 373: loss 0.446003\n",
      "batch 374: loss 0.279695\n",
      "batch 375: loss 0.315624\n",
      "batch 376: loss 0.403011\n",
      "batch 377: loss 0.420178\n",
      "batch 378: loss 0.333141\n",
      "batch 379: loss 0.448260\n",
      "batch 380: loss 0.351722\n",
      "batch 381: loss 0.288543\n",
      "batch 382: loss 0.385786\n",
      "batch 383: loss 0.174093\n",
      "batch 384: loss 0.339831\n",
      "batch 385: loss 0.427591\n",
      "batch 386: loss 0.220476\n",
      "batch 387: loss 0.324399\n",
      "batch 388: loss 0.352295\n",
      "batch 389: loss 0.352851\n",
      "batch 390: loss 0.364666\n",
      "batch 391: loss 0.284668\n",
      "batch 392: loss 0.313063\n",
      "batch 393: loss 0.164344\n",
      "batch 394: loss 0.355951\n",
      "batch 395: loss 0.276741\n",
      "batch 396: loss 0.294345\n",
      "batch 397: loss 0.143799\n",
      "batch 398: loss 0.315048\n",
      "batch 399: loss 0.385599\n",
      "batch 400: loss 0.282137\n",
      "batch 401: loss 0.271466\n",
      "batch 402: loss 0.195587\n",
      "batch 403: loss 0.270612\n",
      "batch 404: loss 0.202405\n",
      "batch 405: loss 0.363216\n",
      "batch 406: loss 0.079950\n",
      "batch 407: loss 0.256348\n",
      "batch 408: loss 0.229658\n",
      "batch 409: loss 0.224384\n",
      "batch 410: loss 0.222676\n",
      "batch 411: loss 0.208072\n",
      "batch 412: loss 0.173214\n",
      "batch 413: loss 0.245301\n",
      "batch 414: loss 0.219166\n",
      "batch 415: loss 0.351880\n",
      "batch 416: loss 0.513292\n",
      "batch 417: loss 0.332037\n",
      "batch 418: loss 0.160888\n",
      "batch 419: loss 0.219668\n",
      "batch 420: loss 0.316859\n",
      "batch 421: loss 0.233567\n",
      "batch 422: loss 0.330649\n",
      "batch 423: loss 0.251646\n",
      "batch 424: loss 0.379552\n",
      "batch 425: loss 0.165643\n",
      "batch 426: loss 0.574636\n",
      "batch 427: loss 0.247587\n",
      "batch 428: loss 0.333532\n",
      "batch 429: loss 0.232314\n",
      "batch 430: loss 0.165152\n",
      "batch 431: loss 0.183768\n",
      "batch 432: loss 0.430534\n",
      "batch 433: loss 0.388797\n",
      "batch 434: loss 0.147801\n",
      "batch 435: loss 0.207379\n",
      "batch 436: loss 0.117885\n",
      "batch 437: loss 0.233373\n",
      "batch 438: loss 0.164364\n",
      "batch 439: loss 0.443733\n",
      "batch 440: loss 0.226250\n",
      "batch 441: loss 0.295720\n",
      "batch 442: loss 0.474589\n",
      "batch 443: loss 0.477509\n",
      "batch 444: loss 0.371106\n",
      "batch 445: loss 0.230896\n",
      "batch 446: loss 0.098991\n",
      "batch 447: loss 0.561400\n",
      "batch 448: loss 0.213208\n",
      "batch 449: loss 0.230251\n",
      "batch 450: loss 0.386882\n",
      "batch 451: loss 0.289931\n",
      "batch 452: loss 0.316837\n",
      "batch 453: loss 0.303397\n",
      "batch 454: loss 0.287675\n",
      "batch 455: loss 0.225762\n",
      "batch 456: loss 0.410067\n",
      "batch 457: loss 0.223891\n",
      "batch 458: loss 0.145783\n",
      "batch 459: loss 0.117014\n",
      "batch 460: loss 0.256453\n",
      "batch 461: loss 0.245475\n",
      "batch 462: loss 0.329777\n",
      "batch 463: loss 0.169344\n",
      "batch 464: loss 0.519285\n",
      "batch 465: loss 0.245256\n",
      "batch 466: loss 0.344026\n",
      "batch 467: loss 0.324747\n",
      "batch 468: loss 0.086209\n",
      "batch 469: loss 0.221989\n",
      "batch 470: loss 0.361295\n",
      "batch 471: loss 0.259702\n",
      "batch 472: loss 0.340561\n",
      "batch 473: loss 0.171286\n",
      "batch 474: loss 0.221367\n",
      "batch 475: loss 0.289078\n",
      "batch 476: loss 0.255848\n",
      "batch 477: loss 0.409101\n",
      "batch 478: loss 0.211123\n",
      "batch 479: loss 0.599395\n",
      "batch 480: loss 0.319264\n",
      "batch 481: loss 0.204934\n",
      "batch 482: loss 0.133964\n",
      "batch 483: loss 0.192336\n",
      "batch 484: loss 0.257833\n",
      "batch 485: loss 0.226621\n",
      "batch 486: loss 0.308282\n",
      "batch 487: loss 0.244735\n",
      "batch 488: loss 0.290262\n",
      "batch 489: loss 0.393981\n",
      "batch 490: loss 0.278310\n",
      "batch 491: loss 0.470639\n",
      "batch 492: loss 0.450779\n",
      "batch 493: loss 0.324270\n",
      "batch 494: loss 0.226830\n",
      "batch 495: loss 0.316564\n",
      "batch 496: loss 0.225006\n",
      "batch 497: loss 0.208312\n",
      "batch 498: loss 0.200352\n",
      "batch 499: loss 0.104044\n",
      "batch 500: loss 0.227058\n",
      "batch 501: loss 0.574446\n",
      "batch 502: loss 0.132891\n",
      "batch 503: loss 0.204791\n",
      "batch 504: loss 0.215989\n",
      "batch 505: loss 0.143063\n",
      "batch 506: loss 0.290892\n",
      "batch 507: loss 0.200266\n",
      "batch 508: loss 0.285575\n",
      "batch 509: loss 0.244551\n",
      "batch 510: loss 0.156430\n",
      "batch 511: loss 0.416378\n",
      "batch 512: loss 0.343055\n",
      "batch 513: loss 0.244304\n",
      "batch 514: loss 0.216873\n",
      "batch 515: loss 0.150378\n",
      "batch 516: loss 0.365394\n",
      "batch 517: loss 0.175619\n",
      "batch 518: loss 0.251994\n",
      "batch 519: loss 0.283863\n",
      "batch 520: loss 0.160429\n",
      "batch 521: loss 0.193341\n",
      "batch 522: loss 0.230170\n",
      "batch 523: loss 0.217506\n",
      "batch 524: loss 0.152410\n",
      "batch 525: loss 0.104676\n",
      "batch 526: loss 0.320406\n",
      "batch 527: loss 0.208117\n",
      "batch 528: loss 0.360424\n",
      "batch 529: loss 0.386162\n",
      "batch 530: loss 0.165857\n",
      "batch 531: loss 0.235364\n",
      "batch 532: loss 0.252410\n",
      "batch 533: loss 0.142939\n",
      "batch 534: loss 0.190188\n",
      "batch 535: loss 0.216431\n",
      "batch 536: loss 0.402498\n",
      "batch 537: loss 0.146609\n",
      "batch 538: loss 0.404852\n",
      "batch 539: loss 0.190522\n",
      "batch 540: loss 0.238551\n",
      "batch 541: loss 0.078736\n",
      "batch 542: loss 0.096275\n",
      "batch 543: loss 0.096770\n",
      "batch 544: loss 0.122291\n",
      "batch 545: loss 0.214527\n",
      "batch 546: loss 0.340217\n",
      "batch 547: loss 0.232473\n",
      "batch 548: loss 0.231568\n",
      "batch 549: loss 0.235963\n",
      "batch 550: loss 0.108088\n",
      "batch 551: loss 0.368002\n",
      "batch 552: loss 0.229448\n",
      "batch 553: loss 0.394669\n",
      "batch 554: loss 0.109005\n",
      "batch 555: loss 0.228449\n",
      "batch 556: loss 0.281611\n",
      "batch 557: loss 0.192118\n",
      "batch 558: loss 0.303669\n",
      "batch 559: loss 0.523946\n",
      "batch 560: loss 0.368979\n",
      "batch 561: loss 0.262485\n",
      "batch 562: loss 0.243119\n",
      "batch 563: loss 0.112349\n",
      "batch 564: loss 0.296334\n",
      "batch 565: loss 0.334742\n",
      "batch 566: loss 0.199335\n",
      "batch 567: loss 0.087970\n",
      "batch 568: loss 0.237018\n",
      "batch 569: loss 0.171653\n",
      "batch 570: loss 0.289218\n",
      "batch 571: loss 0.338648\n",
      "batch 572: loss 0.094738\n",
      "batch 573: loss 0.137220\n",
      "batch 574: loss 0.204953\n",
      "batch 575: loss 0.253081\n",
      "batch 576: loss 0.366298\n",
      "batch 577: loss 0.388803\n",
      "batch 578: loss 0.157052\n",
      "batch 579: loss 0.214953\n",
      "batch 580: loss 0.157631\n",
      "batch 581: loss 0.323126\n",
      "batch 582: loss 0.342321\n",
      "batch 583: loss 0.112922\n",
      "batch 584: loss 0.394597\n",
      "batch 585: loss 0.231184\n",
      "batch 586: loss 0.218670\n",
      "batch 587: loss 0.238559\n",
      "batch 588: loss 0.194674\n",
      "batch 589: loss 0.296548\n",
      "batch 590: loss 0.142757\n",
      "batch 591: loss 0.435508\n",
      "batch 592: loss 0.326840\n",
      "batch 593: loss 0.175893\n",
      "batch 594: loss 0.099414\n",
      "batch 595: loss 0.241263\n",
      "batch 596: loss 0.248337\n",
      "batch 597: loss 0.161656\n",
      "batch 598: loss 0.113035\n",
      "batch 599: loss 0.161402\n",
      "batch 600: loss 0.252722\n",
      "batch 601: loss 0.183588\n",
      "batch 602: loss 0.099031\n",
      "batch 603: loss 0.547800\n",
      "batch 604: loss 0.464670\n",
      "batch 605: loss 0.110689\n",
      "batch 606: loss 0.188114\n",
      "batch 607: loss 0.209103\n",
      "batch 608: loss 0.279120\n",
      "batch 609: loss 0.317838\n",
      "batch 610: loss 0.193677\n",
      "batch 611: loss 0.133063\n",
      "batch 612: loss 0.285349\n",
      "batch 613: loss 0.321368\n",
      "batch 614: loss 0.142032\n",
      "batch 615: loss 0.388847\n",
      "batch 616: loss 0.257389\n",
      "batch 617: loss 0.219016\n",
      "batch 618: loss 0.238494\n",
      "batch 619: loss 0.343256\n",
      "batch 620: loss 0.243315\n",
      "batch 621: loss 0.108108\n",
      "batch 622: loss 0.207780\n",
      "batch 623: loss 0.292541\n",
      "batch 624: loss 0.146260\n",
      "batch 625: loss 0.417089\n",
      "batch 626: loss 0.300232\n",
      "batch 627: loss 0.223706\n",
      "batch 628: loss 0.164840\n",
      "batch 629: loss 0.110325\n",
      "batch 630: loss 0.087129\n",
      "batch 631: loss 0.179363\n",
      "batch 632: loss 0.196534\n",
      "batch 633: loss 0.152455\n",
      "batch 634: loss 0.595157\n",
      "batch 635: loss 0.169189\n",
      "batch 636: loss 0.411761\n",
      "batch 637: loss 0.292398\n",
      "batch 638: loss 0.279805\n",
      "batch 639: loss 0.334014\n",
      "batch 640: loss 0.186024\n",
      "batch 641: loss 0.302534\n",
      "batch 642: loss 0.134917\n",
      "batch 643: loss 0.354823\n",
      "batch 644: loss 0.185710\n",
      "batch 645: loss 0.116609\n",
      "batch 646: loss 0.068716\n",
      "batch 647: loss 0.326404\n",
      "batch 648: loss 0.306678\n",
      "batch 649: loss 0.148161\n",
      "batch 650: loss 0.305223\n",
      "batch 651: loss 0.121544\n",
      "batch 652: loss 0.064544\n",
      "batch 653: loss 0.217700\n",
      "batch 654: loss 0.302704\n",
      "batch 655: loss 0.268015\n",
      "batch 656: loss 0.300799\n",
      "batch 657: loss 0.194668\n",
      "batch 658: loss 0.273726\n",
      "batch 659: loss 0.217007\n",
      "batch 660: loss 0.103228\n",
      "batch 661: loss 0.297225\n",
      "batch 662: loss 0.102341\n",
      "batch 663: loss 0.239103\n",
      "batch 664: loss 0.161441\n",
      "batch 665: loss 0.153500\n",
      "batch 666: loss 0.210965\n",
      "batch 667: loss 0.256359\n",
      "batch 668: loss 0.343511\n",
      "batch 669: loss 0.224001\n",
      "batch 670: loss 0.607731\n",
      "batch 671: loss 0.211708\n",
      "batch 672: loss 0.221458\n",
      "batch 673: loss 0.274001\n",
      "batch 674: loss 0.298193\n",
      "batch 675: loss 0.167725\n",
      "batch 676: loss 0.325899\n",
      "batch 677: loss 0.248193\n",
      "batch 678: loss 0.386090\n",
      "batch 679: loss 0.206662\n",
      "batch 680: loss 0.204916\n",
      "batch 681: loss 0.143822\n",
      "batch 682: loss 0.388274\n",
      "batch 683: loss 0.134141\n",
      "batch 684: loss 0.151462\n",
      "batch 685: loss 0.253254\n",
      "batch 686: loss 0.135784\n",
      "batch 687: loss 0.142039\n",
      "batch 688: loss 0.184270\n",
      "batch 689: loss 0.642132\n",
      "batch 690: loss 0.141502\n",
      "batch 691: loss 0.344678\n",
      "batch 692: loss 0.190307\n",
      "batch 693: loss 0.557521\n",
      "batch 694: loss 0.231839\n",
      "batch 695: loss 0.338121\n",
      "batch 696: loss 0.191843\n",
      "batch 697: loss 0.288163\n",
      "batch 698: loss 0.323616\n",
      "batch 699: loss 0.163371\n",
      "batch 700: loss 0.169736\n",
      "batch 701: loss 0.371497\n",
      "batch 702: loss 0.149585\n",
      "batch 703: loss 0.111733\n",
      "batch 704: loss 0.102161\n",
      "batch 705: loss 0.507150\n",
      "batch 706: loss 0.259587\n",
      "batch 707: loss 0.672368\n",
      "batch 708: loss 0.218193\n",
      "batch 709: loss 0.210827\n",
      "batch 710: loss 0.152608\n",
      "batch 711: loss 0.216539\n",
      "batch 712: loss 0.114280\n",
      "batch 713: loss 0.218536\n",
      "batch 714: loss 0.311505\n",
      "batch 715: loss 0.196753\n",
      "batch 716: loss 0.492875\n",
      "batch 717: loss 0.390384\n",
      "batch 718: loss 0.264893\n",
      "batch 719: loss 0.261218\n",
      "batch 720: loss 0.147463\n",
      "batch 721: loss 0.226090\n",
      "batch 722: loss 0.181143\n",
      "batch 723: loss 0.157036\n",
      "batch 724: loss 0.242738\n",
      "batch 725: loss 0.136010\n",
      "batch 726: loss 0.264674\n",
      "batch 727: loss 0.372370\n",
      "batch 728: loss 0.219869\n",
      "batch 729: loss 0.135239\n",
      "batch 730: loss 0.225545\n",
      "batch 731: loss 0.285572\n",
      "batch 732: loss 0.152604\n",
      "batch 733: loss 0.150521\n",
      "batch 734: loss 0.290631\n",
      "batch 735: loss 0.135002\n",
      "batch 736: loss 0.325256\n",
      "batch 737: loss 0.241562\n",
      "batch 738: loss 0.167999\n",
      "batch 739: loss 0.125141\n",
      "batch 740: loss 0.116058\n",
      "batch 741: loss 0.295941\n",
      "batch 742: loss 0.079679\n",
      "batch 743: loss 0.151915\n",
      "batch 744: loss 0.183004\n",
      "batch 745: loss 0.068924\n",
      "batch 746: loss 0.240523\n",
      "batch 747: loss 0.152275\n",
      "batch 748: loss 0.509632\n",
      "batch 749: loss 0.136032\n",
      "batch 750: loss 0.183802\n",
      "batch 751: loss 0.090200\n",
      "batch 752: loss 0.145527\n",
      "batch 753: loss 0.118596\n",
      "batch 754: loss 0.136987\n",
      "batch 755: loss 0.284234\n",
      "batch 756: loss 0.186201\n",
      "batch 757: loss 0.251350\n",
      "batch 758: loss 0.170594\n",
      "batch 759: loss 0.245623\n",
      "batch 760: loss 0.118717\n",
      "batch 761: loss 0.298002\n",
      "batch 762: loss 0.185809\n",
      "batch 763: loss 0.481864\n",
      "batch 764: loss 0.286544\n",
      "batch 765: loss 0.201655\n",
      "batch 766: loss 0.199023\n",
      "batch 767: loss 0.208976\n",
      "batch 768: loss 0.425665\n",
      "batch 769: loss 0.086426\n",
      "batch 770: loss 0.212506\n",
      "batch 771: loss 0.459539\n",
      "batch 772: loss 0.111217\n",
      "batch 773: loss 0.143754\n",
      "batch 774: loss 0.363319\n",
      "batch 775: loss 0.221924\n",
      "batch 776: loss 0.246232\n",
      "batch 777: loss 0.117883\n",
      "batch 778: loss 0.464660\n",
      "batch 779: loss 0.077860\n",
      "batch 780: loss 0.426622\n",
      "batch 781: loss 0.203582\n",
      "batch 782: loss 0.114509\n",
      "batch 783: loss 0.211225\n",
      "batch 784: loss 0.211828\n",
      "batch 785: loss 0.126975\n",
      "batch 786: loss 0.374258\n",
      "batch 787: loss 0.246576\n",
      "batch 788: loss 0.055412\n",
      "batch 789: loss 0.201294\n",
      "batch 790: loss 0.066564\n",
      "batch 791: loss 0.158058\n",
      "batch 792: loss 0.120165\n",
      "batch 793: loss 0.077867\n",
      "batch 794: loss 0.359463\n",
      "batch 795: loss 0.155862\n",
      "batch 796: loss 0.040954\n",
      "batch 797: loss 0.265078\n",
      "batch 798: loss 0.193925\n",
      "batch 799: loss 0.107753\n",
      "batch 800: loss 0.443633\n",
      "batch 801: loss 0.387597\n",
      "batch 802: loss 0.300771\n",
      "batch 803: loss 0.389111\n",
      "batch 804: loss 0.222200\n",
      "batch 805: loss 0.184261\n",
      "batch 806: loss 0.289449\n",
      "batch 807: loss 0.107473\n",
      "batch 808: loss 0.186217\n",
      "batch 809: loss 0.227484\n",
      "batch 810: loss 0.090279\n",
      "batch 811: loss 0.341697\n",
      "batch 812: loss 0.077157\n",
      "batch 813: loss 0.216062\n",
      "batch 814: loss 0.231884\n",
      "batch 815: loss 0.147394\n",
      "batch 816: loss 0.168123\n",
      "batch 817: loss 0.290049\n",
      "batch 818: loss 0.084574\n",
      "batch 819: loss 0.219740\n",
      "batch 820: loss 0.264129\n",
      "batch 821: loss 0.193671\n",
      "batch 822: loss 0.089323\n",
      "batch 823: loss 0.163910\n",
      "batch 824: loss 0.119558\n",
      "batch 825: loss 0.262432\n",
      "batch 826: loss 0.138304\n",
      "batch 827: loss 0.167595\n",
      "batch 828: loss 0.214080\n",
      "batch 829: loss 0.262568\n",
      "batch 830: loss 0.155424\n",
      "batch 831: loss 0.045772\n",
      "batch 832: loss 0.075493\n",
      "batch 833: loss 0.188316\n",
      "batch 834: loss 0.066252\n",
      "batch 835: loss 0.194315\n",
      "batch 836: loss 0.079101\n",
      "batch 837: loss 0.145896\n",
      "batch 838: loss 0.231571\n",
      "batch 839: loss 0.197902\n",
      "batch 840: loss 0.106451\n",
      "batch 841: loss 0.145125\n",
      "batch 842: loss 0.150055\n",
      "batch 843: loss 0.131822\n",
      "batch 844: loss 0.157774\n",
      "batch 845: loss 0.174907\n",
      "batch 846: loss 0.185801\n",
      "batch 847: loss 0.111153\n",
      "batch 848: loss 0.392715\n",
      "batch 849: loss 0.233960\n",
      "batch 850: loss 0.128763\n",
      "batch 851: loss 0.194645\n",
      "batch 852: loss 0.387303\n",
      "batch 853: loss 0.204519\n",
      "batch 854: loss 0.234206\n",
      "batch 855: loss 0.282770\n",
      "batch 856: loss 0.061998\n",
      "batch 857: loss 0.247208\n",
      "batch 858: loss 0.173640\n",
      "batch 859: loss 0.095090\n",
      "batch 860: loss 0.256516\n",
      "batch 861: loss 0.140892\n",
      "batch 862: loss 0.189639\n",
      "batch 863: loss 0.151053\n",
      "batch 864: loss 0.209081\n",
      "batch 865: loss 0.179093\n",
      "batch 866: loss 0.361561\n",
      "batch 867: loss 0.093661\n",
      "batch 868: loss 0.130107\n",
      "batch 869: loss 0.296147\n",
      "batch 870: loss 0.183541\n",
      "batch 871: loss 0.098331\n",
      "batch 872: loss 0.082150\n",
      "batch 873: loss 0.237006\n",
      "batch 874: loss 0.311346\n",
      "batch 875: loss 0.320361\n",
      "batch 876: loss 0.127210\n",
      "batch 877: loss 0.146933\n",
      "batch 878: loss 0.327543\n",
      "batch 879: loss 0.218038\n",
      "batch 880: loss 0.162647\n",
      "batch 881: loss 0.128556\n",
      "batch 882: loss 0.084283\n",
      "batch 883: loss 0.116127\n",
      "batch 884: loss 0.243911\n",
      "batch 885: loss 0.209719\n",
      "batch 886: loss 0.199826\n",
      "batch 887: loss 0.402484\n",
      "batch 888: loss 0.121603\n",
      "batch 889: loss 0.099006\n",
      "batch 890: loss 0.156724\n",
      "batch 891: loss 0.123002\n",
      "batch 892: loss 0.342961\n",
      "batch 893: loss 0.229728\n",
      "batch 894: loss 0.156005\n",
      "batch 895: loss 0.297359\n",
      "batch 896: loss 0.092828\n",
      "batch 897: loss 0.200209\n",
      "batch 898: loss 0.108174\n",
      "batch 899: loss 0.192034\n",
      "batch 900: loss 0.155031\n",
      "batch 901: loss 0.198885\n",
      "batch 902: loss 0.248095\n",
      "batch 903: loss 0.347622\n",
      "batch 904: loss 0.188800\n",
      "batch 905: loss 0.376647\n",
      "batch 906: loss 0.485585\n",
      "batch 907: loss 0.252406\n",
      "batch 908: loss 0.064410\n",
      "batch 909: loss 0.202016\n",
      "batch 910: loss 0.083570\n",
      "batch 911: loss 0.149763\n",
      "batch 912: loss 0.401823\n",
      "batch 913: loss 0.246023\n",
      "batch 914: loss 0.074068\n",
      "batch 915: loss 0.193219\n",
      "batch 916: loss 0.162021\n",
      "batch 917: loss 0.367493\n",
      "batch 918: loss 0.427730\n",
      "batch 919: loss 0.066261\n",
      "batch 920: loss 0.162592\n",
      "batch 921: loss 0.146809\n",
      "batch 922: loss 0.059066\n",
      "batch 923: loss 0.178093\n",
      "batch 924: loss 0.275678\n",
      "batch 925: loss 0.049792\n",
      "batch 926: loss 0.112452\n",
      "batch 927: loss 0.207852\n",
      "batch 928: loss 0.354684\n",
      "batch 929: loss 0.095053\n",
      "batch 930: loss 0.096543\n",
      "batch 931: loss 0.173982\n",
      "batch 932: loss 0.229673\n",
      "batch 933: loss 0.114831\n",
      "batch 934: loss 0.164066\n",
      "batch 935: loss 0.074505\n",
      "batch 936: loss 0.292461\n",
      "batch 937: loss 0.301461\n",
      "batch 938: loss 0.136509\n",
      "batch 939: loss 0.158541\n",
      "batch 940: loss 0.254552\n",
      "batch 941: loss 0.158007\n",
      "batch 942: loss 0.118758\n",
      "batch 943: loss 0.101972\n",
      "batch 944: loss 0.243503\n",
      "batch 945: loss 0.171634\n",
      "batch 946: loss 0.059395\n",
      "batch 947: loss 0.331330\n",
      "batch 948: loss 0.092714\n",
      "batch 949: loss 0.052780\n",
      "batch 950: loss 0.333138\n",
      "batch 951: loss 0.112233\n",
      "batch 952: loss 0.076942\n",
      "batch 953: loss 0.192699\n",
      "batch 954: loss 0.106187\n",
      "batch 955: loss 0.304460\n",
      "batch 956: loss 0.171499\n",
      "batch 957: loss 0.241542\n",
      "batch 958: loss 0.176857\n",
      "batch 959: loss 0.124310\n",
      "batch 960: loss 0.327378\n",
      "batch 961: loss 0.243657\n",
      "batch 962: loss 0.205927\n",
      "batch 963: loss 0.297360\n",
      "batch 964: loss 0.089678\n",
      "batch 965: loss 0.088063\n",
      "batch 966: loss 0.059778\n",
      "batch 967: loss 0.153013\n",
      "batch 968: loss 0.396790\n",
      "batch 969: loss 0.100486\n",
      "batch 970: loss 0.152258\n",
      "batch 971: loss 0.168813\n",
      "batch 972: loss 0.151262\n",
      "batch 973: loss 0.100934\n",
      "batch 974: loss 0.287320\n",
      "batch 975: loss 0.258381\n",
      "batch 976: loss 0.075751\n",
      "batch 977: loss 0.129217\n",
      "batch 978: loss 0.131195\n",
      "batch 979: loss 0.167307\n",
      "batch 980: loss 0.258878\n",
      "batch 981: loss 0.066577\n",
      "batch 982: loss 0.261778\n",
      "batch 983: loss 0.087098\n",
      "batch 984: loss 0.104917\n",
      "batch 985: loss 0.158244\n",
      "batch 986: loss 0.198988\n",
      "batch 987: loss 0.268010\n",
      "batch 988: loss 0.066813\n",
      "batch 989: loss 0.198660\n",
      "batch 990: loss 0.254502\n",
      "batch 991: loss 0.266991\n",
      "batch 992: loss 0.178363\n",
      "batch 993: loss 0.064852\n",
      "batch 994: loss 0.106795\n",
      "batch 995: loss 0.148208\n",
      "batch 996: loss 0.241474\n",
      "batch 997: loss 0.051388\n",
      "batch 998: loss 0.231236\n",
      "batch 999: loss 0.149455\n",
      "batch 1000: loss 0.183656\n",
      "batch 1001: loss 0.287825\n",
      "batch 1002: loss 0.229330\n",
      "batch 1003: loss 0.187129\n",
      "batch 1004: loss 0.146163\n",
      "batch 1005: loss 0.274347\n",
      "batch 1006: loss 0.060724\n",
      "batch 1007: loss 0.159468\n",
      "batch 1008: loss 0.122224\n",
      "batch 1009: loss 0.125120\n",
      "batch 1010: loss 0.151127\n",
      "batch 1011: loss 0.170274\n",
      "batch 1012: loss 0.408286\n",
      "batch 1013: loss 0.115068\n",
      "batch 1014: loss 0.140383\n",
      "batch 1015: loss 0.332377\n",
      "batch 1016: loss 0.195611\n",
      "batch 1017: loss 0.199208\n",
      "batch 1018: loss 0.273267\n",
      "batch 1019: loss 0.182365\n",
      "batch 1020: loss 0.111771\n",
      "batch 1021: loss 0.336329\n",
      "batch 1022: loss 0.230663\n",
      "batch 1023: loss 0.256280\n",
      "batch 1024: loss 0.338005\n",
      "batch 1025: loss 0.458116\n",
      "batch 1026: loss 0.361070\n",
      "batch 1027: loss 0.103040\n",
      "batch 1028: loss 0.138767\n",
      "batch 1029: loss 0.110856\n",
      "batch 1030: loss 0.124888\n",
      "batch 1031: loss 0.435681\n",
      "batch 1032: loss 0.216768\n",
      "batch 1033: loss 0.098811\n",
      "batch 1034: loss 0.434488\n",
      "batch 1035: loss 0.235313\n",
      "batch 1036: loss 0.074100\n",
      "batch 1037: loss 0.293679\n",
      "batch 1038: loss 0.228381\n",
      "batch 1039: loss 0.235001\n",
      "batch 1040: loss 0.157902\n",
      "batch 1041: loss 0.105695\n",
      "batch 1042: loss 0.185391\n",
      "batch 1043: loss 0.092218\n",
      "batch 1044: loss 0.287780\n",
      "batch 1045: loss 0.125348\n",
      "batch 1046: loss 0.069782\n",
      "batch 1047: loss 0.138004\n",
      "batch 1048: loss 0.263319\n",
      "batch 1049: loss 0.275174\n",
      "batch 1050: loss 0.297476\n",
      "batch 1051: loss 0.119644\n",
      "batch 1052: loss 0.247302\n",
      "batch 1053: loss 0.305494\n",
      "batch 1054: loss 0.152651\n",
      "batch 1055: loss 0.262611\n",
      "batch 1056: loss 0.253365\n",
      "batch 1057: loss 0.134818\n",
      "batch 1058: loss 0.045144\n",
      "batch 1059: loss 0.480632\n",
      "batch 1060: loss 0.253559\n",
      "batch 1061: loss 0.124376\n",
      "batch 1062: loss 0.192835\n",
      "batch 1063: loss 0.231484\n",
      "batch 1064: loss 0.134241\n",
      "batch 1065: loss 0.136890\n",
      "batch 1066: loss 0.178260\n",
      "batch 1067: loss 0.262133\n",
      "batch 1068: loss 0.108626\n",
      "batch 1069: loss 0.308047\n",
      "batch 1070: loss 0.298060\n",
      "batch 1071: loss 0.196470\n",
      "batch 1072: loss 0.107080\n",
      "batch 1073: loss 0.218746\n",
      "batch 1074: loss 0.099108\n",
      "batch 1075: loss 0.274583\n",
      "batch 1076: loss 0.127816\n",
      "batch 1077: loss 0.105287\n",
      "batch 1078: loss 0.050033\n",
      "batch 1079: loss 0.091339\n",
      "batch 1080: loss 0.206055\n",
      "batch 1081: loss 0.176659\n",
      "batch 1082: loss 0.109230\n",
      "batch 1083: loss 0.241300\n",
      "batch 1084: loss 0.069641\n",
      "batch 1085: loss 0.115588\n",
      "batch 1086: loss 0.171936\n",
      "batch 1087: loss 0.121016\n",
      "batch 1088: loss 0.227060\n",
      "batch 1089: loss 0.286706\n",
      "batch 1090: loss 0.202727\n",
      "batch 1091: loss 0.262881\n",
      "batch 1092: loss 0.200364\n",
      "batch 1093: loss 0.167704\n",
      "batch 1094: loss 0.186282\n",
      "batch 1095: loss 0.448399\n",
      "batch 1096: loss 0.204535\n",
      "batch 1097: loss 0.289703\n",
      "batch 1098: loss 0.165849\n",
      "batch 1099: loss 0.172961\n",
      "batch 1100: loss 0.191495\n",
      "batch 1101: loss 0.478793\n",
      "batch 1102: loss 0.356934\n",
      "batch 1103: loss 0.231224\n",
      "batch 1104: loss 0.372076\n",
      "batch 1105: loss 0.212801\n",
      "batch 1106: loss 0.135516\n",
      "batch 1107: loss 0.190782\n",
      "batch 1108: loss 0.150283\n",
      "batch 1109: loss 0.226375\n",
      "batch 1110: loss 0.228050\n",
      "batch 1111: loss 0.197732\n",
      "batch 1112: loss 0.101638\n",
      "batch 1113: loss 0.287088\n",
      "batch 1114: loss 0.097677\n",
      "batch 1115: loss 0.077749\n",
      "batch 1116: loss 0.158012\n",
      "batch 1117: loss 0.310495\n",
      "batch 1118: loss 0.148627\n",
      "batch 1119: loss 0.165391\n",
      "batch 1120: loss 0.084368\n",
      "batch 1121: loss 0.132393\n",
      "batch 1122: loss 0.111790\n",
      "batch 1123: loss 0.277434\n",
      "batch 1124: loss 0.277405\n",
      "batch 1125: loss 0.076859\n",
      "batch 1126: loss 0.189445\n",
      "batch 1127: loss 0.141340\n",
      "batch 1128: loss 0.265126\n",
      "batch 1129: loss 0.083559\n",
      "batch 1130: loss 0.187733\n",
      "batch 1131: loss 0.294643\n",
      "batch 1132: loss 0.042461\n",
      "batch 1133: loss 0.341105\n",
      "batch 1134: loss 0.116069\n",
      "batch 1135: loss 0.146624\n",
      "batch 1136: loss 0.212653\n",
      "batch 1137: loss 0.207128\n",
      "batch 1138: loss 0.155767\n",
      "batch 1139: loss 0.168570\n",
      "batch 1140: loss 0.174115\n",
      "batch 1141: loss 0.183217\n",
      "batch 1142: loss 0.021828\n",
      "batch 1143: loss 0.183481\n",
      "batch 1144: loss 0.075678\n",
      "batch 1145: loss 0.533583\n",
      "batch 1146: loss 0.266614\n",
      "batch 1147: loss 0.158820\n",
      "batch 1148: loss 0.123060\n",
      "batch 1149: loss 0.114149\n",
      "batch 1150: loss 0.159005\n",
      "batch 1151: loss 0.145736\n",
      "batch 1152: loss 0.172393\n",
      "batch 1153: loss 0.114492\n",
      "batch 1154: loss 0.168029\n",
      "batch 1155: loss 0.099900\n",
      "batch 1156: loss 0.064549\n",
      "batch 1157: loss 0.120362\n",
      "batch 1158: loss 0.178147\n",
      "batch 1159: loss 0.154637\n",
      "batch 1160: loss 0.150453\n",
      "batch 1161: loss 0.324519\n",
      "batch 1162: loss 0.142714\n",
      "batch 1163: loss 0.302306\n",
      "batch 1164: loss 0.075073\n",
      "batch 1165: loss 0.132297\n",
      "batch 1166: loss 0.156829\n",
      "batch 1167: loss 0.175846\n",
      "batch 1168: loss 0.193244\n",
      "batch 1169: loss 0.110190\n",
      "batch 1170: loss 0.060991\n",
      "batch 1171: loss 0.130891\n",
      "batch 1172: loss 0.052320\n",
      "batch 1173: loss 0.232139\n",
      "batch 1174: loss 0.172699\n",
      "batch 1175: loss 0.214908\n",
      "batch 1176: loss 0.201763\n",
      "batch 1177: loss 0.160259\n",
      "batch 1178: loss 0.169684\n",
      "batch 1179: loss 0.107483\n",
      "batch 1180: loss 0.214968\n",
      "batch 1181: loss 0.187713\n",
      "batch 1182: loss 0.100420\n",
      "batch 1183: loss 0.141581\n",
      "batch 1184: loss 0.059074\n",
      "batch 1185: loss 0.279407\n",
      "batch 1186: loss 0.127671\n",
      "batch 1187: loss 0.190373\n",
      "batch 1188: loss 0.102013\n",
      "batch 1189: loss 0.115093\n",
      "batch 1190: loss 0.153071\n",
      "batch 1191: loss 0.365402\n",
      "batch 1192: loss 0.271107\n",
      "batch 1193: loss 0.180282\n",
      "batch 1194: loss 0.144432\n",
      "batch 1195: loss 0.248948\n",
      "batch 1196: loss 0.098490\n",
      "batch 1197: loss 0.170159\n",
      "batch 1198: loss 0.109019\n",
      "batch 1199: loss 0.322700\n",
      "batch 1200: loss 0.102217\n",
      "batch 1201: loss 0.191718\n",
      "batch 1202: loss 0.196331\n",
      "batch 1203: loss 0.123851\n",
      "batch 1204: loss 0.266073\n",
      "batch 1205: loss 0.265428\n",
      "batch 1206: loss 0.252808\n",
      "batch 1207: loss 0.086767\n",
      "batch 1208: loss 0.116043\n",
      "batch 1209: loss 0.052863\n",
      "batch 1210: loss 0.113606\n",
      "batch 1211: loss 0.166489\n",
      "batch 1212: loss 0.159296\n",
      "batch 1213: loss 0.065225\n",
      "batch 1214: loss 0.180447\n",
      "batch 1215: loss 0.183109\n",
      "batch 1216: loss 0.334423\n",
      "batch 1217: loss 0.069740\n",
      "batch 1218: loss 0.110867\n",
      "batch 1219: loss 0.258221\n",
      "batch 1220: loss 0.133632\n",
      "batch 1221: loss 0.172460\n",
      "batch 1222: loss 0.262252\n",
      "batch 1223: loss 0.067811\n",
      "batch 1224: loss 0.138505\n",
      "batch 1225: loss 0.347683\n",
      "batch 1226: loss 0.060335\n",
      "batch 1227: loss 0.283488\n",
      "batch 1228: loss 0.234919\n",
      "batch 1229: loss 0.160610\n",
      "batch 1230: loss 0.504055\n",
      "batch 1231: loss 0.168926\n",
      "batch 1232: loss 0.235956\n",
      "batch 1233: loss 0.140540\n",
      "batch 1234: loss 0.075685\n",
      "batch 1235: loss 0.364065\n",
      "batch 1236: loss 0.042638\n",
      "batch 1237: loss 0.172069\n",
      "batch 1238: loss 0.097981\n",
      "batch 1239: loss 0.172631\n",
      "batch 1240: loss 0.171042\n",
      "batch 1241: loss 0.215822\n",
      "batch 1242: loss 0.309737\n",
      "batch 1243: loss 0.134176\n",
      "batch 1244: loss 0.133233\n",
      "batch 1245: loss 0.205361\n",
      "batch 1246: loss 0.050136\n",
      "batch 1247: loss 0.243446\n",
      "batch 1248: loss 0.219403\n",
      "batch 1249: loss 0.088154\n",
      "batch 1250: loss 0.100951\n",
      "batch 1251: loss 0.133385\n",
      "batch 1252: loss 0.153163\n",
      "batch 1253: loss 0.187740\n",
      "batch 1254: loss 0.199689\n",
      "batch 1255: loss 0.060243\n",
      "batch 1256: loss 0.096573\n",
      "batch 1257: loss 0.053422\n",
      "batch 1258: loss 0.399791\n",
      "batch 1259: loss 0.315952\n",
      "batch 1260: loss 0.255843\n",
      "batch 1261: loss 0.309330\n",
      "batch 1262: loss 0.083833\n",
      "batch 1263: loss 0.245783\n",
      "batch 1264: loss 0.191027\n",
      "batch 1265: loss 0.165513\n",
      "batch 1266: loss 0.209801\n",
      "batch 1267: loss 0.120281\n",
      "batch 1268: loss 0.125714\n",
      "batch 1269: loss 0.110845\n",
      "batch 1270: loss 0.101146\n",
      "batch 1271: loss 0.173501\n",
      "batch 1272: loss 0.353257\n",
      "batch 1273: loss 0.041792\n",
      "batch 1274: loss 0.121000\n",
      "batch 1275: loss 0.093971\n",
      "batch 1276: loss 0.033855\n",
      "batch 1277: loss 0.060092\n",
      "batch 1278: loss 0.189854\n",
      "batch 1279: loss 0.069640\n",
      "batch 1280: loss 0.068114\n",
      "batch 1281: loss 0.120721\n",
      "batch 1282: loss 0.147622\n",
      "batch 1283: loss 0.128510\n",
      "batch 1284: loss 0.058347\n",
      "batch 1285: loss 0.194434\n",
      "batch 1286: loss 0.102485\n",
      "batch 1287: loss 0.154295\n",
      "batch 1288: loss 0.130851\n",
      "batch 1289: loss 0.204848\n",
      "batch 1290: loss 0.080320\n",
      "batch 1291: loss 0.153495\n",
      "batch 1292: loss 0.280628\n",
      "batch 1293: loss 0.073185\n",
      "batch 1294: loss 0.069021\n",
      "batch 1295: loss 0.128498\n",
      "batch 1296: loss 0.237442\n",
      "batch 1297: loss 0.225470\n",
      "batch 1298: loss 0.076120\n",
      "batch 1299: loss 0.173745\n",
      "batch 1300: loss 0.342450\n",
      "batch 1301: loss 0.119606\n",
      "batch 1302: loss 0.157521\n",
      "batch 1303: loss 0.200264\n",
      "batch 1304: loss 0.102317\n",
      "batch 1305: loss 0.199078\n",
      "batch 1306: loss 0.130276\n",
      "batch 1307: loss 0.266782\n",
      "batch 1308: loss 0.107004\n",
      "batch 1309: loss 0.294175\n",
      "batch 1310: loss 0.099251\n",
      "batch 1311: loss 0.217866\n",
      "batch 1312: loss 0.096057\n",
      "batch 1313: loss 0.051205\n",
      "batch 1314: loss 0.125003\n",
      "batch 1315: loss 0.049457\n",
      "batch 1316: loss 0.236297\n",
      "batch 1317: loss 0.057762\n",
      "batch 1318: loss 0.209559\n",
      "batch 1319: loss 0.089501\n",
      "batch 1320: loss 0.169601\n",
      "batch 1321: loss 0.160569\n",
      "batch 1322: loss 0.136725\n",
      "batch 1323: loss 0.374705\n",
      "batch 1324: loss 0.087673\n",
      "batch 1325: loss 0.057962\n",
      "batch 1326: loss 0.206929\n",
      "batch 1327: loss 0.210392\n",
      "batch 1328: loss 0.322721\n",
      "batch 1329: loss 0.279597\n",
      "batch 1330: loss 0.115726\n",
      "batch 1331: loss 0.041114\n",
      "batch 1332: loss 0.041512\n",
      "batch 1333: loss 0.114052\n",
      "batch 1334: loss 0.152534\n",
      "batch 1335: loss 0.236359\n",
      "batch 1336: loss 0.157521\n",
      "batch 1337: loss 0.169655\n",
      "batch 1338: loss 0.234266\n",
      "batch 1339: loss 0.108769\n",
      "batch 1340: loss 0.102177\n",
      "batch 1341: loss 0.101152\n",
      "batch 1342: loss 0.058458\n",
      "batch 1343: loss 0.346624\n",
      "batch 1344: loss 0.246902\n",
      "batch 1345: loss 0.295995\n",
      "batch 1346: loss 0.084390\n",
      "batch 1347: loss 0.086161\n",
      "batch 1348: loss 0.262993\n",
      "batch 1349: loss 0.093075\n",
      "batch 1350: loss 0.151210\n",
      "batch 1351: loss 0.081792\n",
      "batch 1352: loss 0.208613\n",
      "batch 1353: loss 0.283653\n",
      "batch 1354: loss 0.086288\n",
      "batch 1355: loss 0.220422\n",
      "batch 1356: loss 0.222726\n",
      "batch 1357: loss 0.047922\n",
      "batch 1358: loss 0.158206\n",
      "batch 1359: loss 0.209587\n",
      "batch 1360: loss 0.312844\n",
      "batch 1361: loss 0.204682\n",
      "batch 1362: loss 0.305425\n",
      "batch 1363: loss 0.180608\n",
      "batch 1364: loss 0.160573\n",
      "batch 1365: loss 0.049941\n",
      "batch 1366: loss 0.090354\n",
      "batch 1367: loss 0.340004\n",
      "batch 1368: loss 0.176823\n",
      "batch 1369: loss 0.101486\n",
      "batch 1370: loss 0.172312\n",
      "batch 1371: loss 0.308932\n",
      "batch 1372: loss 0.180905\n",
      "batch 1373: loss 0.152446\n",
      "batch 1374: loss 0.101729\n",
      "batch 1375: loss 0.085254\n",
      "batch 1376: loss 0.105019\n",
      "batch 1377: loss 0.198821\n",
      "batch 1378: loss 0.243068\n",
      "batch 1379: loss 0.033514\n",
      "batch 1380: loss 0.265053\n",
      "batch 1381: loss 0.147714\n",
      "batch 1382: loss 0.125536\n",
      "batch 1383: loss 0.131977\n",
      "batch 1384: loss 0.151879\n",
      "batch 1385: loss 0.149292\n",
      "batch 1386: loss 0.169827\n",
      "batch 1387: loss 0.125963\n",
      "batch 1388: loss 0.144481\n",
      "batch 1389: loss 0.090842\n",
      "batch 1390: loss 0.146259\n",
      "batch 1391: loss 0.129570\n",
      "batch 1392: loss 0.121048\n",
      "batch 1393: loss 0.232674\n",
      "batch 1394: loss 0.114947\n",
      "batch 1395: loss 0.056453\n",
      "batch 1396: loss 0.181301\n",
      "batch 1397: loss 0.183185\n",
      "batch 1398: loss 0.160206\n",
      "batch 1399: loss 0.058049\n",
      "batch 1400: loss 0.142063\n",
      "batch 1401: loss 0.031785\n",
      "batch 1402: loss 0.080440\n",
      "batch 1403: loss 0.173322\n",
      "batch 1404: loss 0.304309\n",
      "batch 1405: loss 0.023854\n",
      "batch 1406: loss 0.126279\n",
      "batch 1407: loss 0.167592\n",
      "batch 1408: loss 0.089966\n",
      "batch 1409: loss 0.091111\n",
      "batch 1410: loss 0.253344\n",
      "batch 1411: loss 0.120411\n",
      "batch 1412: loss 0.129996\n",
      "batch 1413: loss 0.094962\n",
      "batch 1414: loss 0.262438\n",
      "batch 1415: loss 0.058506\n",
      "batch 1416: loss 0.339366\n",
      "batch 1417: loss 0.221036\n",
      "batch 1418: loss 0.126800\n",
      "batch 1419: loss 0.282582\n",
      "batch 1420: loss 0.040346\n",
      "batch 1421: loss 0.316186\n",
      "batch 1422: loss 0.023141\n",
      "batch 1423: loss 0.216287\n",
      "batch 1424: loss 0.214968\n",
      "batch 1425: loss 0.124109\n",
      "batch 1426: loss 0.282421\n",
      "batch 1427: loss 0.124078\n",
      "batch 1428: loss 0.268302\n",
      "batch 1429: loss 0.086239\n",
      "batch 1430: loss 0.302815\n",
      "batch 1431: loss 0.072417\n",
      "batch 1432: loss 0.219868\n",
      "batch 1433: loss 0.181841\n",
      "batch 1434: loss 0.095447\n",
      "batch 1435: loss 0.159708\n",
      "batch 1436: loss 0.160429\n",
      "batch 1437: loss 0.097000\n",
      "batch 1438: loss 0.095333\n",
      "batch 1439: loss 0.143477\n",
      "batch 1440: loss 0.087714\n",
      "batch 1441: loss 0.051908\n",
      "batch 1442: loss 0.153305\n",
      "batch 1443: loss 0.063019\n",
      "batch 1444: loss 0.205736\n",
      "batch 1445: loss 0.039486\n",
      "batch 1446: loss 0.111036\n",
      "batch 1447: loss 0.080483\n",
      "batch 1448: loss 0.074955\n",
      "batch 1449: loss 0.119470\n",
      "batch 1450: loss 0.141699\n",
      "batch 1451: loss 0.149742\n",
      "batch 1452: loss 0.109936\n",
      "batch 1453: loss 0.153078\n",
      "batch 1454: loss 0.107338\n",
      "batch 1455: loss 0.092643\n",
      "batch 1456: loss 0.163035\n",
      "batch 1457: loss 0.067979\n",
      "batch 1458: loss 0.245788\n",
      "batch 1459: loss 0.315326\n",
      "batch 1460: loss 0.179195\n",
      "batch 1461: loss 0.160853\n",
      "batch 1462: loss 0.284965\n",
      "batch 1463: loss 0.046052\n",
      "batch 1464: loss 0.171152\n",
      "batch 1465: loss 0.056635\n",
      "batch 1466: loss 0.052622\n",
      "batch 1467: loss 0.159653\n",
      "batch 1468: loss 0.038497\n",
      "batch 1469: loss 0.112850\n",
      "batch 1470: loss 0.179541\n",
      "batch 1471: loss 0.081644\n",
      "batch 1472: loss 0.113453\n",
      "batch 1473: loss 0.223032\n",
      "batch 1474: loss 0.104866\n",
      "batch 1475: loss 0.164763\n",
      "batch 1476: loss 0.163226\n",
      "batch 1477: loss 0.080941\n",
      "batch 1478: loss 0.101054\n",
      "batch 1479: loss 0.179595\n",
      "batch 1480: loss 0.172315\n",
      "batch 1481: loss 0.055768\n",
      "batch 1482: loss 0.034314\n",
      "batch 1483: loss 0.281892\n",
      "batch 1484: loss 0.226000\n",
      "batch 1485: loss 0.173766\n",
      "batch 1486: loss 0.127577\n",
      "batch 1487: loss 0.083443\n",
      "batch 1488: loss 0.029275\n",
      "batch 1489: loss 0.020149\n",
      "batch 1490: loss 0.134574\n",
      "batch 1491: loss 0.147112\n",
      "batch 1492: loss 0.178188\n",
      "batch 1493: loss 0.142378\n",
      "batch 1494: loss 0.171053\n",
      "batch 1495: loss 0.159940\n",
      "batch 1496: loss 0.095222\n",
      "batch 1497: loss 0.368406\n",
      "batch 1498: loss 0.224860\n",
      "batch 1499: loss 0.038499\n",
      "batch 1500: loss 0.066093\n",
      "batch 1501: loss 0.076934\n",
      "batch 1502: loss 0.084639\n",
      "batch 1503: loss 0.103825\n",
      "batch 1504: loss 0.155297\n",
      "batch 1505: loss 0.039038\n",
      "batch 1506: loss 0.118358\n",
      "batch 1507: loss 0.057740\n",
      "batch 1508: loss 0.514637\n",
      "batch 1509: loss 0.130403\n",
      "batch 1510: loss 0.190641\n",
      "batch 1511: loss 0.160792\n",
      "batch 1512: loss 0.077342\n",
      "batch 1513: loss 0.185478\n",
      "batch 1514: loss 0.321521\n",
      "batch 1515: loss 0.209932\n",
      "batch 1516: loss 0.090455\n",
      "batch 1517: loss 0.263530\n",
      "batch 1518: loss 0.038634\n",
      "batch 1519: loss 0.077986\n",
      "batch 1520: loss 0.219393\n",
      "batch 1521: loss 0.114175\n",
      "batch 1522: loss 0.331642\n",
      "batch 1523: loss 0.081167\n",
      "batch 1524: loss 0.100680\n",
      "batch 1525: loss 0.126745\n",
      "batch 1526: loss 0.153447\n",
      "batch 1527: loss 0.145084\n",
      "batch 1528: loss 0.378178\n",
      "batch 1529: loss 0.072276\n",
      "batch 1530: loss 0.094188\n",
      "batch 1531: loss 0.070722\n",
      "batch 1532: loss 0.122896\n",
      "batch 1533: loss 0.300250\n",
      "batch 1534: loss 0.132806\n",
      "batch 1535: loss 0.051788\n",
      "batch 1536: loss 0.109870\n",
      "batch 1537: loss 0.051215\n",
      "batch 1538: loss 0.117368\n",
      "batch 1539: loss 0.236431\n",
      "batch 1540: loss 0.122608\n",
      "batch 1541: loss 0.185178\n",
      "batch 1542: loss 0.079162\n",
      "batch 1543: loss 0.102890\n",
      "batch 1544: loss 0.067657\n",
      "batch 1545: loss 0.063435\n",
      "batch 1546: loss 0.262786\n",
      "batch 1547: loss 0.089675\n",
      "batch 1548: loss 0.172409\n",
      "batch 1549: loss 0.113981\n",
      "batch 1550: loss 0.049746\n",
      "batch 1551: loss 0.344206\n",
      "batch 1552: loss 0.254364\n",
      "batch 1553: loss 0.041686\n",
      "batch 1554: loss 0.075646\n",
      "batch 1555: loss 0.164944\n",
      "batch 1556: loss 0.119861\n",
      "batch 1557: loss 0.262594\n",
      "batch 1558: loss 0.311427\n",
      "batch 1559: loss 0.063437\n",
      "batch 1560: loss 0.183654\n",
      "batch 1561: loss 0.047387\n",
      "batch 1562: loss 0.095049\n",
      "batch 1563: loss 0.115178\n",
      "batch 1564: loss 0.101489\n",
      "batch 1565: loss 0.073626\n",
      "batch 1566: loss 0.181525\n",
      "batch 1567: loss 0.134764\n",
      "batch 1568: loss 0.064511\n",
      "batch 1569: loss 0.245345\n",
      "batch 1570: loss 0.084607\n",
      "batch 1571: loss 0.116320\n",
      "batch 1572: loss 0.111389\n",
      "batch 1573: loss 0.158483\n",
      "batch 1574: loss 0.078334\n",
      "batch 1575: loss 0.038567\n",
      "batch 1576: loss 0.285228\n",
      "batch 1577: loss 0.196527\n",
      "batch 1578: loss 0.094369\n",
      "batch 1579: loss 0.128253\n",
      "batch 1580: loss 0.124348\n",
      "batch 1581: loss 0.281670\n",
      "batch 1582: loss 0.127433\n",
      "batch 1583: loss 0.204529\n",
      "batch 1584: loss 0.073482\n",
      "batch 1585: loss 0.160970\n",
      "batch 1586: loss 0.234891\n",
      "batch 1587: loss 0.098890\n",
      "batch 1588: loss 0.137806\n",
      "batch 1589: loss 0.104222\n",
      "batch 1590: loss 0.147210\n",
      "batch 1591: loss 0.142700\n",
      "batch 1592: loss 0.049652\n",
      "batch 1593: loss 0.054943\n",
      "batch 1594: loss 0.145525\n",
      "batch 1595: loss 0.194685\n",
      "batch 1596: loss 0.153191\n",
      "batch 1597: loss 0.200353\n",
      "batch 1598: loss 0.101914\n",
      "batch 1599: loss 0.189735\n",
      "batch 1600: loss 0.078238\n",
      "batch 1601: loss 0.209657\n",
      "batch 1602: loss 0.191126\n",
      "batch 1603: loss 0.062043\n",
      "batch 1604: loss 0.084313\n",
      "batch 1605: loss 0.140171\n",
      "batch 1606: loss 0.123261\n",
      "batch 1607: loss 0.370265\n",
      "batch 1608: loss 0.142825\n",
      "batch 1609: loss 0.089704\n",
      "batch 1610: loss 0.121629\n",
      "batch 1611: loss 0.261728\n",
      "batch 1612: loss 0.083013\n",
      "batch 1613: loss 0.284920\n",
      "batch 1614: loss 0.043758\n",
      "batch 1615: loss 0.160608\n",
      "batch 1616: loss 0.037547\n",
      "batch 1617: loss 0.121383\n",
      "batch 1618: loss 0.150922\n",
      "batch 1619: loss 0.181472\n",
      "batch 1620: loss 0.090556\n",
      "batch 1621: loss 0.128178\n",
      "batch 1622: loss 0.145938\n",
      "batch 1623: loss 0.303422\n",
      "batch 1624: loss 0.179501\n",
      "batch 1625: loss 0.061208\n",
      "batch 1626: loss 0.117699\n",
      "batch 1627: loss 0.120414\n",
      "batch 1628: loss 0.139007\n",
      "batch 1629: loss 0.291875\n",
      "batch 1630: loss 0.051917\n",
      "batch 1631: loss 0.186940\n",
      "batch 1632: loss 0.054021\n",
      "batch 1633: loss 0.051658\n",
      "batch 1634: loss 0.105707\n",
      "batch 1635: loss 0.089179\n",
      "batch 1636: loss 0.130590\n",
      "batch 1637: loss 0.075994\n",
      "batch 1638: loss 0.165840\n",
      "batch 1639: loss 0.085607\n",
      "batch 1640: loss 0.105582\n",
      "batch 1641: loss 0.051414\n",
      "batch 1642: loss 0.289964\n",
      "batch 1643: loss 0.343078\n",
      "batch 1644: loss 0.258186\n",
      "batch 1645: loss 0.067592\n",
      "batch 1646: loss 0.148898\n",
      "batch 1647: loss 0.137843\n",
      "batch 1648: loss 0.074098\n",
      "batch 1649: loss 0.028952\n",
      "batch 1650: loss 0.232491\n",
      "batch 1651: loss 0.124990\n",
      "batch 1652: loss 0.058741\n",
      "batch 1653: loss 0.168886\n",
      "batch 1654: loss 0.064126\n",
      "batch 1655: loss 0.212512\n",
      "batch 1656: loss 0.302752\n",
      "batch 1657: loss 0.063849\n",
      "batch 1658: loss 0.142323\n",
      "batch 1659: loss 0.125753\n",
      "batch 1660: loss 0.261100\n",
      "batch 1661: loss 0.096212\n",
      "batch 1662: loss 0.153555\n",
      "batch 1663: loss 0.297553\n",
      "batch 1664: loss 0.046984\n",
      "batch 1665: loss 0.092276\n",
      "batch 1666: loss 0.110909\n",
      "batch 1667: loss 0.032664\n",
      "batch 1668: loss 0.175429\n",
      "batch 1669: loss 0.225551\n",
      "batch 1670: loss 0.091622\n",
      "batch 1671: loss 0.079187\n",
      "batch 1672: loss 0.173482\n",
      "batch 1673: loss 0.131064\n",
      "batch 1674: loss 0.172360\n",
      "batch 1675: loss 0.082021\n",
      "batch 1676: loss 0.079060\n",
      "batch 1677: loss 0.162414\n",
      "batch 1678: loss 0.102685\n",
      "batch 1679: loss 0.047791\n",
      "batch 1680: loss 0.086801\n",
      "batch 1681: loss 0.045200\n",
      "batch 1682: loss 0.246637\n",
      "batch 1683: loss 0.108161\n",
      "batch 1684: loss 0.149810\n",
      "batch 1685: loss 0.122848\n",
      "batch 1686: loss 0.055861\n",
      "batch 1687: loss 0.111787\n",
      "batch 1688: loss 0.103973\n",
      "batch 1689: loss 0.087850\n",
      "batch 1690: loss 0.070522\n",
      "batch 1691: loss 0.091159\n",
      "batch 1692: loss 0.265207\n",
      "batch 1693: loss 0.091254\n",
      "batch 1694: loss 0.097547\n",
      "batch 1695: loss 0.066525\n",
      "batch 1696: loss 0.305348\n",
      "batch 1697: loss 0.063182\n",
      "batch 1698: loss 0.071650\n",
      "batch 1699: loss 0.086722\n",
      "batch 1700: loss 0.210099\n",
      "batch 1701: loss 0.060875\n",
      "batch 1702: loss 0.220415\n",
      "batch 1703: loss 0.076137\n",
      "batch 1704: loss 0.048736\n",
      "batch 1705: loss 0.135503\n",
      "batch 1706: loss 0.073157\n",
      "batch 1707: loss 0.088450\n",
      "batch 1708: loss 0.193032\n",
      "batch 1709: loss 0.140364\n",
      "batch 1710: loss 0.053311\n",
      "batch 1711: loss 0.181468\n",
      "batch 1712: loss 0.413783\n",
      "batch 1713: loss 0.074285\n",
      "batch 1714: loss 0.191556\n",
      "batch 1715: loss 0.105295\n",
      "batch 1716: loss 0.140577\n",
      "batch 1717: loss 0.138478\n",
      "batch 1718: loss 0.082747\n",
      "batch 1719: loss 0.089059\n",
      "batch 1720: loss 0.153259\n",
      "batch 1721: loss 0.488457\n",
      "batch 1722: loss 0.064951\n",
      "batch 1723: loss 0.060939\n",
      "batch 1724: loss 0.212358\n",
      "batch 1725: loss 0.102429\n",
      "batch 1726: loss 0.082092\n",
      "batch 1727: loss 0.042821\n",
      "batch 1728: loss 0.052674\n",
      "batch 1729: loss 0.219525\n",
      "batch 1730: loss 0.062036\n",
      "batch 1731: loss 0.170464\n",
      "batch 1732: loss 0.061810\n",
      "batch 1733: loss 0.038913\n",
      "batch 1734: loss 0.210659\n",
      "batch 1735: loss 0.049623\n",
      "batch 1736: loss 0.041646\n",
      "batch 1737: loss 0.156173\n",
      "batch 1738: loss 0.196652\n",
      "batch 1739: loss 0.147111\n",
      "batch 1740: loss 0.141808\n",
      "batch 1741: loss 0.073434\n",
      "batch 1742: loss 0.162076\n",
      "batch 1743: loss 0.076440\n",
      "batch 1744: loss 0.033329\n",
      "batch 1745: loss 0.130569\n",
      "batch 1746: loss 0.237011\n",
      "batch 1747: loss 0.278614\n",
      "batch 1748: loss 0.108171\n",
      "batch 1749: loss 0.051651\n",
      "batch 1750: loss 0.123093\n",
      "batch 1751: loss 0.291250\n",
      "batch 1752: loss 0.116015\n",
      "batch 1753: loss 0.139216\n",
      "batch 1754: loss 0.076508\n",
      "batch 1755: loss 0.043765\n",
      "batch 1756: loss 0.076128\n",
      "batch 1757: loss 0.072286\n",
      "batch 1758: loss 0.026528\n",
      "batch 1759: loss 0.115359\n",
      "batch 1760: loss 0.082976\n",
      "batch 1761: loss 0.061015\n",
      "batch 1762: loss 0.031351\n",
      "batch 1763: loss 0.064038\n",
      "batch 1764: loss 0.156260\n",
      "batch 1765: loss 0.150075\n",
      "batch 1766: loss 0.102047\n",
      "batch 1767: loss 0.122813\n",
      "batch 1768: loss 0.123066\n",
      "batch 1769: loss 0.121876\n",
      "batch 1770: loss 0.238531\n",
      "batch 1771: loss 0.104535\n",
      "batch 1772: loss 0.067067\n",
      "batch 1773: loss 0.092143\n",
      "batch 1774: loss 0.154997\n",
      "batch 1775: loss 0.161701\n",
      "batch 1776: loss 0.162456\n",
      "batch 1777: loss 0.146013\n",
      "batch 1778: loss 0.140411\n",
      "batch 1779: loss 0.154571\n",
      "batch 1780: loss 0.146807\n",
      "batch 1781: loss 0.085355\n",
      "batch 1782: loss 0.097681\n",
      "batch 1783: loss 0.053826\n",
      "batch 1784: loss 0.177936\n",
      "batch 1785: loss 0.071858\n",
      "batch 1786: loss 0.071001\n",
      "batch 1787: loss 0.047668\n",
      "batch 1788: loss 0.054821\n",
      "batch 1789: loss 0.117670\n",
      "batch 1790: loss 0.100129\n",
      "batch 1791: loss 0.081216\n",
      "batch 1792: loss 0.110387\n",
      "batch 1793: loss 0.047824\n",
      "batch 1794: loss 0.075573\n",
      "batch 1795: loss 0.135244\n",
      "batch 1796: loss 0.055190\n",
      "batch 1797: loss 0.073177\n",
      "batch 1798: loss 0.075982\n",
      "batch 1799: loss 0.195187\n",
      "batch 1800: loss 0.231879\n",
      "batch 1801: loss 0.106005\n",
      "batch 1802: loss 0.086989\n",
      "batch 1803: loss 0.242488\n",
      "batch 1804: loss 0.087189\n",
      "batch 1805: loss 0.086741\n",
      "batch 1806: loss 0.136087\n",
      "batch 1807: loss 0.059522\n",
      "batch 1808: loss 0.178296\n",
      "batch 1809: loss 0.139813\n",
      "batch 1810: loss 0.193350\n",
      "batch 1811: loss 0.109696\n",
      "batch 1812: loss 0.066886\n",
      "batch 1813: loss 0.230685\n",
      "batch 1814: loss 0.098104\n",
      "batch 1815: loss 0.191217\n",
      "batch 1816: loss 0.021746\n",
      "batch 1817: loss 0.203221\n",
      "batch 1818: loss 0.131000\n",
      "batch 1819: loss 0.120799\n",
      "batch 1820: loss 0.134280\n",
      "batch 1821: loss 0.029894\n",
      "batch 1822: loss 0.033006\n",
      "batch 1823: loss 0.099740\n",
      "batch 1824: loss 0.042740\n",
      "batch 1825: loss 0.261730\n",
      "batch 1826: loss 0.368124\n",
      "batch 1827: loss 0.069134\n",
      "batch 1828: loss 0.017177\n",
      "batch 1829: loss 0.124311\n",
      "batch 1830: loss 0.061012\n",
      "batch 1831: loss 0.089960\n",
      "batch 1832: loss 0.089098\n",
      "batch 1833: loss 0.052295\n",
      "batch 1834: loss 0.119279\n",
      "batch 1835: loss 0.105662\n",
      "batch 1836: loss 0.098613\n",
      "batch 1837: loss 0.040034\n",
      "batch 1838: loss 0.121387\n",
      "batch 1839: loss 0.053032\n",
      "batch 1840: loss 0.111482\n",
      "batch 1841: loss 0.047545\n",
      "batch 1842: loss 0.040760\n",
      "batch 1843: loss 0.028911\n",
      "batch 1844: loss 0.188839\n",
      "batch 1845: loss 0.177589\n",
      "batch 1846: loss 0.093880\n",
      "batch 1847: loss 0.035071\n",
      "batch 1848: loss 0.064167\n",
      "batch 1849: loss 0.044441\n",
      "batch 1850: loss 0.129048\n",
      "batch 1851: loss 0.152213\n",
      "batch 1852: loss 0.095113\n",
      "batch 1853: loss 0.271029\n",
      "batch 1854: loss 0.195545\n",
      "batch 1855: loss 0.070754\n",
      "batch 1856: loss 0.104809\n",
      "batch 1857: loss 0.050915\n",
      "batch 1858: loss 0.172244\n",
      "batch 1859: loss 0.079935\n",
      "batch 1860: loss 0.251840\n",
      "batch 1861: loss 0.105260\n",
      "batch 1862: loss 0.109435\n",
      "batch 1863: loss 0.116795\n",
      "batch 1864: loss 0.256245\n",
      "batch 1865: loss 0.074713\n",
      "batch 1866: loss 0.112091\n",
      "batch 1867: loss 0.211650\n",
      "batch 1868: loss 0.119161\n",
      "batch 1869: loss 0.189937\n",
      "batch 1870: loss 0.060749\n",
      "batch 1871: loss 0.188970\n",
      "batch 1872: loss 0.089952\n",
      "batch 1873: loss 0.241695\n",
      "batch 1874: loss 0.078212\n",
      "batch 1875: loss 0.214978\n",
      "batch 1876: loss 0.292571\n",
      "batch 1877: loss 0.099041\n",
      "batch 1878: loss 0.054864\n",
      "batch 1879: loss 0.052026\n",
      "batch 1880: loss 0.062851\n",
      "batch 1881: loss 0.072532\n",
      "batch 1882: loss 0.238053\n",
      "batch 1883: loss 0.092837\n",
      "batch 1884: loss 0.107368\n",
      "batch 1885: loss 0.128386\n",
      "batch 1886: loss 0.176136\n",
      "batch 1887: loss 0.101443\n",
      "batch 1888: loss 0.094071\n",
      "batch 1889: loss 0.092774\n",
      "batch 1890: loss 0.123835\n",
      "batch 1891: loss 0.105446\n",
      "batch 1892: loss 0.073402\n",
      "batch 1893: loss 0.052416\n",
      "batch 1894: loss 0.140569\n",
      "batch 1895: loss 0.166394\n",
      "batch 1896: loss 0.112078\n",
      "batch 1897: loss 0.424344\n",
      "batch 1898: loss 0.127846\n",
      "batch 1899: loss 0.126081\n",
      "batch 1900: loss 0.207167\n",
      "batch 1901: loss 0.100287\n",
      "batch 1902: loss 0.143135\n",
      "batch 1903: loss 0.148238\n",
      "batch 1904: loss 0.032385\n",
      "batch 1905: loss 0.210688\n",
      "batch 1906: loss 0.088600\n",
      "batch 1907: loss 0.152709\n",
      "batch 1908: loss 0.279569\n",
      "batch 1909: loss 0.197073\n",
      "batch 1910: loss 0.260703\n",
      "batch 1911: loss 0.125939\n",
      "batch 1912: loss 0.111259\n",
      "batch 1913: loss 0.060281\n",
      "batch 1914: loss 0.065236\n",
      "batch 1915: loss 0.135994\n",
      "batch 1916: loss 0.149815\n",
      "batch 1917: loss 0.082344\n",
      "batch 1918: loss 0.113663\n",
      "batch 1919: loss 0.024177\n",
      "batch 1920: loss 0.121955\n",
      "batch 1921: loss 0.028803\n",
      "batch 1922: loss 0.073074\n",
      "batch 1923: loss 0.059896\n",
      "batch 1924: loss 0.218677\n",
      "batch 1925: loss 0.090846\n",
      "batch 1926: loss 0.052644\n",
      "batch 1927: loss 0.244774\n",
      "batch 1928: loss 0.085101\n",
      "batch 1929: loss 0.221316\n",
      "batch 1930: loss 0.046358\n",
      "batch 1931: loss 0.029539\n",
      "batch 1932: loss 0.044825\n",
      "batch 1933: loss 0.060910\n",
      "batch 1934: loss 0.157641\n",
      "batch 1935: loss 0.200255\n",
      "batch 1936: loss 0.164263\n",
      "batch 1937: loss 0.205989\n",
      "batch 1938: loss 0.165414\n",
      "batch 1939: loss 0.142544\n",
      "batch 1940: loss 0.259717\n",
      "batch 1941: loss 0.108941\n",
      "batch 1942: loss 0.249339\n",
      "batch 1943: loss 0.031318\n",
      "batch 1944: loss 0.028688\n",
      "batch 1945: loss 0.062171\n",
      "batch 1946: loss 0.126179\n",
      "batch 1947: loss 0.167577\n",
      "batch 1948: loss 0.205019\n",
      "batch 1949: loss 0.065709\n",
      "batch 1950: loss 0.074824\n",
      "batch 1951: loss 0.053993\n",
      "batch 1952: loss 0.155396\n",
      "batch 1953: loss 0.072995\n",
      "batch 1954: loss 0.194465\n",
      "batch 1955: loss 0.059990\n",
      "batch 1956: loss 0.054124\n",
      "batch 1957: loss 0.086932\n",
      "batch 1958: loss 0.121863\n",
      "batch 1959: loss 0.168515\n",
      "batch 1960: loss 0.131916\n",
      "batch 1961: loss 0.166026\n",
      "batch 1962: loss 0.041868\n",
      "batch 1963: loss 0.063458\n",
      "batch 1964: loss 0.380973\n",
      "batch 1965: loss 0.184106\n",
      "batch 1966: loss 0.175005\n",
      "batch 1967: loss 0.116082\n",
      "batch 1968: loss 0.339442\n",
      "batch 1969: loss 0.041589\n",
      "batch 1970: loss 0.150365\n",
      "batch 1971: loss 0.035619\n",
      "batch 1972: loss 0.063429\n",
      "batch 1973: loss 0.139015\n",
      "batch 1974: loss 0.139142\n",
      "batch 1975: loss 0.077211\n",
      "batch 1976: loss 0.122988\n",
      "batch 1977: loss 0.105103\n",
      "batch 1978: loss 0.076139\n",
      "batch 1979: loss 0.134345\n",
      "batch 1980: loss 0.137241\n",
      "batch 1981: loss 0.070488\n",
      "batch 1982: loss 0.106075\n",
      "batch 1983: loss 0.068428\n",
      "batch 1984: loss 0.109917\n",
      "batch 1985: loss 0.085029\n",
      "batch 1986: loss 0.093528\n",
      "batch 1987: loss 0.045167\n",
      "batch 1988: loss 0.070588\n",
      "batch 1989: loss 0.033915\n",
      "batch 1990: loss 0.178022\n",
      "batch 1991: loss 0.049915\n",
      "batch 1992: loss 0.096867\n",
      "batch 1993: loss 0.042286\n",
      "batch 1994: loss 0.076790\n",
      "batch 1995: loss 0.063665\n",
      "batch 1996: loss 0.064542\n",
      "batch 1997: loss 0.089168\n",
      "batch 1998: loss 0.117917\n",
      "batch 1999: loss 0.087106\n",
      "batch 2000: loss 0.065429\n",
      "batch 2001: loss 0.070623\n",
      "batch 2002: loss 0.210603\n",
      "batch 2003: loss 0.119076\n",
      "batch 2004: loss 0.169131\n",
      "batch 2005: loss 0.076746\n",
      "batch 2006: loss 0.127270\n",
      "batch 2007: loss 0.115493\n",
      "batch 2008: loss 0.156397\n",
      "batch 2009: loss 0.058826\n",
      "batch 2010: loss 0.198922\n",
      "batch 2011: loss 0.071213\n",
      "batch 2012: loss 0.106692\n",
      "batch 2013: loss 0.114385\n",
      "batch 2014: loss 0.110578\n",
      "batch 2015: loss 0.232993\n",
      "batch 2016: loss 0.256731\n",
      "batch 2017: loss 0.065640\n",
      "batch 2018: loss 0.043966\n",
      "batch 2019: loss 0.070730\n",
      "batch 2020: loss 0.096581\n",
      "batch 2021: loss 0.144258\n",
      "batch 2022: loss 0.026549\n",
      "batch 2023: loss 0.145846\n",
      "batch 2024: loss 0.131431\n",
      "batch 2025: loss 0.033788\n",
      "batch 2026: loss 0.182913\n",
      "batch 2027: loss 0.041059\n",
      "batch 2028: loss 0.248350\n",
      "batch 2029: loss 0.097431\n",
      "batch 2030: loss 0.206232\n",
      "batch 2031: loss 0.144212\n",
      "batch 2032: loss 0.090695\n",
      "batch 2033: loss 0.090887\n",
      "batch 2034: loss 0.039691\n",
      "batch 2035: loss 0.034026\n",
      "batch 2036: loss 0.053321\n",
      "batch 2037: loss 0.230463\n",
      "batch 2038: loss 0.157670\n",
      "batch 2039: loss 0.165625\n",
      "batch 2040: loss 0.026819\n",
      "batch 2041: loss 0.133175\n",
      "batch 2042: loss 0.046981\n",
      "batch 2043: loss 0.051206\n",
      "batch 2044: loss 0.022522\n",
      "batch 2045: loss 0.233227\n",
      "batch 2046: loss 0.124438\n",
      "batch 2047: loss 0.087434\n",
      "batch 2048: loss 0.055735\n",
      "batch 2049: loss 0.163124\n",
      "batch 2050: loss 0.154733\n",
      "batch 2051: loss 0.047293\n",
      "batch 2052: loss 0.054516\n",
      "batch 2053: loss 0.018748\n",
      "batch 2054: loss 0.105553\n",
      "batch 2055: loss 0.258242\n",
      "batch 2056: loss 0.182291\n",
      "batch 2057: loss 0.275132\n",
      "batch 2058: loss 0.219847\n",
      "batch 2059: loss 0.131428\n",
      "batch 2060: loss 0.073762\n",
      "batch 2061: loss 0.108124\n",
      "batch 2062: loss 0.233608\n",
      "batch 2063: loss 0.114205\n",
      "batch 2064: loss 0.081721\n",
      "batch 2065: loss 0.176371\n",
      "batch 2066: loss 0.167836\n",
      "batch 2067: loss 0.162187\n",
      "batch 2068: loss 0.048148\n",
      "batch 2069: loss 0.054694\n",
      "batch 2070: loss 0.252656\n",
      "batch 2071: loss 0.119467\n",
      "batch 2072: loss 0.055360\n",
      "batch 2073: loss 0.079897\n",
      "batch 2074: loss 0.291772\n",
      "batch 2075: loss 0.138126\n",
      "batch 2076: loss 0.051573\n",
      "batch 2077: loss 0.150041\n",
      "batch 2078: loss 0.157469\n",
      "batch 2079: loss 0.052711\n",
      "batch 2080: loss 0.103099\n",
      "batch 2081: loss 0.055186\n",
      "batch 2082: loss 0.029506\n",
      "batch 2083: loss 0.150173\n",
      "batch 2084: loss 0.152080\n",
      "batch 2085: loss 0.032328\n",
      "batch 2086: loss 0.276320\n",
      "batch 2087: loss 0.160455\n",
      "batch 2088: loss 0.096670\n",
      "batch 2089: loss 0.300637\n",
      "batch 2090: loss 0.117827\n",
      "batch 2091: loss 0.096856\n",
      "batch 2092: loss 0.150220\n",
      "batch 2093: loss 0.088314\n",
      "batch 2094: loss 0.078028\n",
      "batch 2095: loss 0.068033\n",
      "batch 2096: loss 0.116013\n",
      "batch 2097: loss 0.065369\n",
      "batch 2098: loss 0.219889\n",
      "batch 2099: loss 0.133578\n",
      "batch 2100: loss 0.195009\n",
      "batch 2101: loss 0.064115\n",
      "batch 2102: loss 0.140208\n",
      "batch 2103: loss 0.241412\n",
      "batch 2104: loss 0.099875\n",
      "batch 2105: loss 0.142048\n",
      "batch 2106: loss 0.030715\n",
      "batch 2107: loss 0.069442\n",
      "batch 2108: loss 0.154689\n",
      "batch 2109: loss 0.221636\n",
      "batch 2110: loss 0.075231\n",
      "batch 2111: loss 0.170812\n",
      "batch 2112: loss 0.094776\n",
      "batch 2113: loss 0.063995\n",
      "batch 2114: loss 0.169769\n",
      "batch 2115: loss 0.188921\n",
      "batch 2116: loss 0.114765\n",
      "batch 2117: loss 0.061627\n",
      "batch 2118: loss 0.062591\n",
      "batch 2119: loss 0.042151\n",
      "batch 2120: loss 0.028875\n",
      "batch 2121: loss 0.138163\n",
      "batch 2122: loss 0.174165\n",
      "batch 2123: loss 0.149627\n",
      "batch 2124: loss 0.148147\n",
      "batch 2125: loss 0.036562\n",
      "batch 2126: loss 0.075785\n",
      "batch 2127: loss 0.198537\n",
      "batch 2128: loss 0.078581\n",
      "batch 2129: loss 0.082919\n",
      "batch 2130: loss 0.095004\n",
      "batch 2131: loss 0.074793\n",
      "batch 2132: loss 0.101889\n",
      "batch 2133: loss 0.170267\n",
      "batch 2134: loss 0.103459\n",
      "batch 2135: loss 0.115186\n",
      "batch 2136: loss 0.097316\n",
      "batch 2137: loss 0.088619\n",
      "batch 2138: loss 0.091190\n",
      "batch 2139: loss 0.219980\n",
      "batch 2140: loss 0.169081\n",
      "batch 2141: loss 0.140145\n",
      "batch 2142: loss 0.039961\n",
      "batch 2143: loss 0.057128\n",
      "batch 2144: loss 0.081183\n",
      "batch 2145: loss 0.043772\n",
      "batch 2146: loss 0.073569\n",
      "batch 2147: loss 0.036946\n",
      "batch 2148: loss 0.131825\n",
      "batch 2149: loss 0.048953\n",
      "batch 2150: loss 0.241746\n",
      "batch 2151: loss 0.092447\n",
      "batch 2152: loss 0.163757\n",
      "batch 2153: loss 0.179812\n",
      "batch 2154: loss 0.263850\n",
      "batch 2155: loss 0.069414\n",
      "batch 2156: loss 0.041906\n",
      "batch 2157: loss 0.223078\n",
      "batch 2158: loss 0.035516\n",
      "batch 2159: loss 0.094979\n",
      "batch 2160: loss 0.093129\n",
      "batch 2161: loss 0.160796\n",
      "batch 2162: loss 0.090872\n",
      "batch 2163: loss 0.141478\n",
      "batch 2164: loss 0.036983\n",
      "batch 2165: loss 0.087930\n",
      "batch 2166: loss 0.070345\n",
      "batch 2167: loss 0.140517\n",
      "batch 2168: loss 0.171414\n",
      "batch 2169: loss 0.031922\n",
      "batch 2170: loss 0.095123\n",
      "batch 2171: loss 0.133090\n",
      "batch 2172: loss 0.036070\n",
      "batch 2173: loss 0.041731\n",
      "batch 2174: loss 0.109942\n",
      "batch 2175: loss 0.040165\n",
      "batch 2176: loss 0.115158\n",
      "batch 2177: loss 0.096754\n",
      "batch 2178: loss 0.044712\n",
      "batch 2179: loss 0.126753\n",
      "batch 2180: loss 0.116860\n",
      "batch 2181: loss 0.203553\n",
      "batch 2182: loss 0.275848\n",
      "batch 2183: loss 0.079625\n",
      "batch 2184: loss 0.276704\n",
      "batch 2185: loss 0.048824\n",
      "batch 2186: loss 0.164117\n",
      "batch 2187: loss 0.027302\n",
      "batch 2188: loss 0.042677\n",
      "batch 2189: loss 0.084966\n",
      "batch 2190: loss 0.132016\n",
      "batch 2191: loss 0.036557\n",
      "batch 2192: loss 0.228659\n",
      "batch 2193: loss 0.119785\n",
      "batch 2194: loss 0.032318\n",
      "batch 2195: loss 0.041510\n",
      "batch 2196: loss 0.031900\n",
      "batch 2197: loss 0.074286\n",
      "batch 2198: loss 0.105085\n",
      "batch 2199: loss 0.031936\n",
      "batch 2200: loss 0.026132\n",
      "batch 2201: loss 0.139455\n",
      "batch 2202: loss 0.045350\n",
      "batch 2203: loss 0.210900\n",
      "batch 2204: loss 0.297761\n",
      "batch 2205: loss 0.106037\n",
      "batch 2206: loss 0.197488\n",
      "batch 2207: loss 0.128922\n",
      "batch 2208: loss 0.064501\n",
      "batch 2209: loss 0.224973\n",
      "batch 2210: loss 0.037408\n",
      "batch 2211: loss 0.219497\n",
      "batch 2212: loss 0.109791\n",
      "batch 2213: loss 0.141926\n",
      "batch 2214: loss 0.081490\n",
      "batch 2215: loss 0.021712\n",
      "batch 2216: loss 0.070467\n",
      "batch 2217: loss 0.088799\n",
      "batch 2218: loss 0.086061\n",
      "batch 2219: loss 0.057778\n",
      "batch 2220: loss 0.019497\n",
      "batch 2221: loss 0.143823\n",
      "batch 2222: loss 0.088021\n",
      "batch 2223: loss 0.098086\n",
      "batch 2224: loss 0.065248\n",
      "batch 2225: loss 0.068287\n",
      "batch 2226: loss 0.242955\n",
      "batch 2227: loss 0.022068\n",
      "batch 2228: loss 0.121642\n",
      "batch 2229: loss 0.073496\n",
      "batch 2230: loss 0.052332\n",
      "batch 2231: loss 0.097062\n",
      "batch 2232: loss 0.028139\n",
      "batch 2233: loss 0.072070\n",
      "batch 2234: loss 0.225066\n",
      "batch 2235: loss 0.164438\n",
      "batch 2236: loss 0.083499\n",
      "batch 2237: loss 0.048094\n",
      "batch 2238: loss 0.110894\n",
      "batch 2239: loss 0.089041\n",
      "batch 2240: loss 0.084784\n",
      "batch 2241: loss 0.301693\n",
      "batch 2242: loss 0.024065\n",
      "batch 2243: loss 0.016505\n",
      "batch 2244: loss 0.040709\n",
      "batch 2245: loss 0.098092\n",
      "batch 2246: loss 0.059486\n",
      "batch 2247: loss 0.169469\n",
      "batch 2248: loss 0.195752\n",
      "batch 2249: loss 0.126454\n",
      "batch 2250: loss 0.238260\n",
      "batch 2251: loss 0.119282\n",
      "batch 2252: loss 0.075607\n",
      "batch 2253: loss 0.156347\n",
      "batch 2254: loss 0.044233\n",
      "batch 2255: loss 0.069674\n",
      "batch 2256: loss 0.020199\n",
      "batch 2257: loss 0.097779\n",
      "batch 2258: loss 0.081252\n",
      "batch 2259: loss 0.128477\n",
      "batch 2260: loss 0.121692\n",
      "batch 2261: loss 0.129591\n",
      "batch 2262: loss 0.060459\n",
      "batch 2263: loss 0.072649\n",
      "batch 2264: loss 0.084633\n",
      "batch 2265: loss 0.235392\n",
      "batch 2266: loss 0.125966\n",
      "batch 2267: loss 0.057736\n",
      "batch 2268: loss 0.110864\n",
      "batch 2269: loss 0.125992\n",
      "batch 2270: loss 0.112968\n",
      "batch 2271: loss 0.206274\n",
      "batch 2272: loss 0.093871\n",
      "batch 2273: loss 0.105538\n",
      "batch 2274: loss 0.062497\n",
      "batch 2275: loss 0.178320\n",
      "batch 2276: loss 0.033586\n",
      "batch 2277: loss 0.049858\n",
      "batch 2278: loss 0.039081\n",
      "batch 2279: loss 0.268480\n",
      "batch 2280: loss 0.022631\n",
      "batch 2281: loss 0.194001\n",
      "batch 2282: loss 0.232972\n",
      "batch 2283: loss 0.245202\n",
      "batch 2284: loss 0.071377\n",
      "batch 2285: loss 0.289517\n",
      "batch 2286: loss 0.091617\n",
      "batch 2287: loss 0.096343\n",
      "batch 2288: loss 0.043734\n",
      "batch 2289: loss 0.044995\n",
      "batch 2290: loss 0.178121\n",
      "batch 2291: loss 0.256160\n",
      "batch 2292: loss 0.053187\n",
      "batch 2293: loss 0.048380\n",
      "batch 2294: loss 0.110014\n",
      "batch 2295: loss 0.049613\n",
      "batch 2296: loss 0.187552\n",
      "batch 2297: loss 0.135856\n",
      "batch 2298: loss 0.075221\n",
      "batch 2299: loss 0.115241\n",
      "batch 2300: loss 0.251667\n",
      "batch 2301: loss 0.298596\n",
      "batch 2302: loss 0.089869\n",
      "batch 2303: loss 0.200532\n",
      "batch 2304: loss 0.124832\n",
      "batch 2305: loss 0.072678\n",
      "batch 2306: loss 0.105552\n",
      "batch 2307: loss 0.098572\n",
      "batch 2308: loss 0.052980\n",
      "batch 2309: loss 0.205039\n",
      "batch 2310: loss 0.113942\n",
      "batch 2311: loss 0.025002\n",
      "batch 2312: loss 0.173805\n",
      "batch 2313: loss 0.091385\n",
      "batch 2314: loss 0.143796\n",
      "batch 2315: loss 0.094941\n",
      "batch 2316: loss 0.029493\n",
      "batch 2317: loss 0.054943\n",
      "batch 2318: loss 0.168220\n",
      "batch 2319: loss 0.041495\n",
      "batch 2320: loss 0.090819\n",
      "batch 2321: loss 0.041449\n",
      "batch 2322: loss 0.260787\n",
      "batch 2323: loss 0.038769\n",
      "batch 2324: loss 0.013229\n",
      "batch 2325: loss 0.134528\n",
      "batch 2326: loss 0.048304\n",
      "batch 2327: loss 0.060600\n",
      "batch 2328: loss 0.115016\n",
      "batch 2329: loss 0.008646\n",
      "batch 2330: loss 0.020073\n",
      "batch 2331: loss 0.059753\n",
      "batch 2332: loss 0.096377\n",
      "batch 2333: loss 0.103235\n",
      "batch 2334: loss 0.091335\n",
      "batch 2335: loss 0.195164\n",
      "batch 2336: loss 0.155797\n",
      "batch 2337: loss 0.032280\n",
      "batch 2338: loss 0.145833\n",
      "batch 2339: loss 0.120450\n",
      "batch 2340: loss 0.240626\n",
      "batch 2341: loss 0.102168\n",
      "batch 2342: loss 0.092714\n",
      "batch 2343: loss 0.125333\n",
      "batch 2344: loss 0.095656\n",
      "batch 2345: loss 0.106520\n",
      "batch 2346: loss 0.066777\n",
      "batch 2347: loss 0.078278\n",
      "batch 2348: loss 0.111405\n",
      "batch 2349: loss 0.023726\n",
      "batch 2350: loss 0.028698\n",
      "batch 2351: loss 0.104104\n",
      "batch 2352: loss 0.092822\n",
      "batch 2353: loss 0.223466\n",
      "batch 2354: loss 0.048895\n",
      "batch 2355: loss 0.183686\n",
      "batch 2356: loss 0.135128\n",
      "batch 2357: loss 0.036061\n",
      "batch 2358: loss 0.092943\n",
      "batch 2359: loss 0.316947\n",
      "batch 2360: loss 0.033402\n",
      "batch 2361: loss 0.190653\n",
      "batch 2362: loss 0.120348\n",
      "batch 2363: loss 0.190620\n",
      "batch 2364: loss 0.223964\n",
      "batch 2365: loss 0.178525\n",
      "batch 2366: loss 0.092903\n",
      "batch 2367: loss 0.281747\n",
      "batch 2368: loss 0.049467\n",
      "batch 2369: loss 0.233612\n",
      "batch 2370: loss 0.055027\n",
      "batch 2371: loss 0.043103\n",
      "batch 2372: loss 0.115424\n",
      "batch 2373: loss 0.117213\n",
      "batch 2374: loss 0.025353\n",
      "batch 2375: loss 0.207579\n",
      "batch 2376: loss 0.202607\n",
      "batch 2377: loss 0.098692\n",
      "batch 2378: loss 0.102614\n",
      "batch 2379: loss 0.181754\n",
      "batch 2380: loss 0.015335\n",
      "batch 2381: loss 0.082541\n",
      "batch 2382: loss 0.088524\n",
      "batch 2383: loss 0.054387\n",
      "batch 2384: loss 0.113840\n",
      "batch 2385: loss 0.125929\n",
      "batch 2386: loss 0.047086\n",
      "batch 2387: loss 0.477089\n",
      "batch 2388: loss 0.158266\n",
      "batch 2389: loss 0.124834\n",
      "batch 2390: loss 0.169455\n",
      "batch 2391: loss 0.073732\n",
      "batch 2392: loss 0.079830\n",
      "batch 2393: loss 0.035104\n",
      "batch 2394: loss 0.089463\n",
      "batch 2395: loss 0.284065\n",
      "batch 2396: loss 0.216319\n",
      "batch 2397: loss 0.027447\n",
      "batch 2398: loss 0.156302\n",
      "batch 2399: loss 0.151482\n",
      "batch 2400: loss 0.050653\n",
      "batch 2401: loss 0.112496\n",
      "batch 2402: loss 0.178167\n",
      "batch 2403: loss 0.107821\n",
      "batch 2404: loss 0.311877\n",
      "batch 2405: loss 0.153085\n",
      "batch 2406: loss 0.161299\n",
      "batch 2407: loss 0.226142\n",
      "batch 2408: loss 0.089824\n",
      "batch 2409: loss 0.030286\n",
      "batch 2410: loss 0.072830\n",
      "batch 2411: loss 0.013285\n",
      "batch 2412: loss 0.117454\n",
      "batch 2413: loss 0.164802\n",
      "batch 2414: loss 0.036269\n",
      "batch 2415: loss 0.033671\n",
      "batch 2416: loss 0.051460\n",
      "batch 2417: loss 0.063738\n",
      "batch 2418: loss 0.216527\n",
      "batch 2419: loss 0.028371\n",
      "batch 2420: loss 0.046156\n",
      "batch 2421: loss 0.201817\n",
      "batch 2422: loss 0.050805\n",
      "batch 2423: loss 0.219366\n",
      "batch 2424: loss 0.261589\n",
      "batch 2425: loss 0.176183\n",
      "batch 2426: loss 0.091770\n",
      "batch 2427: loss 0.194566\n",
      "batch 2428: loss 0.040890\n",
      "batch 2429: loss 0.238324\n",
      "batch 2430: loss 0.184371\n",
      "batch 2431: loss 0.043260\n",
      "batch 2432: loss 0.157812\n",
      "batch 2433: loss 0.124278\n",
      "batch 2434: loss 0.091445\n",
      "batch 2435: loss 0.081609\n",
      "batch 2436: loss 0.113606\n",
      "batch 2437: loss 0.060278\n",
      "batch 2438: loss 0.163273\n",
      "batch 2439: loss 0.135152\n",
      "batch 2440: loss 0.070186\n",
      "batch 2441: loss 0.092981\n",
      "batch 2442: loss 0.225504\n",
      "batch 2443: loss 0.091231\n",
      "batch 2444: loss 0.085701\n",
      "batch 2445: loss 0.122102\n",
      "batch 2446: loss 0.103231\n",
      "batch 2447: loss 0.081388\n",
      "batch 2448: loss 0.057972\n",
      "batch 2449: loss 0.015044\n",
      "batch 2450: loss 0.108619\n",
      "batch 2451: loss 0.069484\n",
      "batch 2452: loss 0.026021\n",
      "batch 2453: loss 0.176946\n",
      "batch 2454: loss 0.276171\n",
      "batch 2455: loss 0.078253\n",
      "batch 2456: loss 0.215048\n",
      "batch 2457: loss 0.050867\n",
      "batch 2458: loss 0.115144\n",
      "batch 2459: loss 0.190073\n",
      "batch 2460: loss 0.031975\n",
      "batch 2461: loss 0.105891\n",
      "batch 2462: loss 0.134310\n",
      "batch 2463: loss 0.162090\n",
      "batch 2464: loss 0.043335\n",
      "batch 2465: loss 0.077176\n",
      "batch 2466: loss 0.040473\n",
      "batch 2467: loss 0.092027\n",
      "batch 2468: loss 0.054059\n",
      "batch 2469: loss 0.084978\n",
      "batch 2470: loss 0.029843\n",
      "batch 2471: loss 0.079715\n",
      "batch 2472: loss 0.191618\n",
      "batch 2473: loss 0.023831\n",
      "batch 2474: loss 0.086428\n",
      "batch 2475: loss 0.114786\n",
      "batch 2476: loss 0.103638\n",
      "batch 2477: loss 0.117009\n",
      "batch 2478: loss 0.043869\n",
      "batch 2479: loss 0.050800\n",
      "batch 2480: loss 0.116319\n",
      "batch 2481: loss 0.028970\n",
      "batch 2482: loss 0.357298\n",
      "batch 2483: loss 0.083745\n",
      "batch 2484: loss 0.021077\n",
      "batch 2485: loss 0.116136\n",
      "batch 2486: loss 0.050717\n",
      "batch 2487: loss 0.135676\n",
      "batch 2488: loss 0.090777\n",
      "batch 2489: loss 0.044056\n",
      "batch 2490: loss 0.082809\n",
      "batch 2491: loss 0.159461\n",
      "batch 2492: loss 0.076386\n",
      "batch 2493: loss 0.011405\n",
      "batch 2494: loss 0.040621\n",
      "batch 2495: loss 0.140240\n",
      "batch 2496: loss 0.074364\n",
      "batch 2497: loss 0.055135\n",
      "batch 2498: loss 0.027527\n",
      "batch 2499: loss 0.029432\n",
      "batch 2500: loss 0.201660\n",
      "batch 2501: loss 0.030575\n",
      "batch 2502: loss 0.203029\n",
      "batch 2503: loss 0.141630\n",
      "batch 2504: loss 0.085241\n",
      "batch 2505: loss 0.091149\n",
      "batch 2506: loss 0.128246\n",
      "batch 2507: loss 0.059342\n",
      "batch 2508: loss 0.114520\n",
      "batch 2509: loss 0.013315\n",
      "batch 2510: loss 0.094193\n",
      "batch 2511: loss 0.184084\n",
      "batch 2512: loss 0.049378\n",
      "batch 2513: loss 0.088553\n",
      "batch 2514: loss 0.027726\n",
      "batch 2515: loss 0.022021\n",
      "batch 2516: loss 0.086518\n",
      "batch 2517: loss 0.106148\n",
      "batch 2518: loss 0.188900\n",
      "batch 2519: loss 0.137269\n",
      "batch 2520: loss 0.068857\n",
      "batch 2521: loss 0.124019\n",
      "batch 2522: loss 0.193243\n",
      "batch 2523: loss 0.038819\n",
      "batch 2524: loss 0.187969\n",
      "batch 2525: loss 0.104550\n",
      "batch 2526: loss 0.136364\n",
      "batch 2527: loss 0.080362\n",
      "batch 2528: loss 0.116923\n",
      "batch 2529: loss 0.085875\n",
      "batch 2530: loss 0.090354\n",
      "batch 2531: loss 0.086045\n",
      "batch 2532: loss 0.025614\n",
      "batch 2533: loss 0.047248\n",
      "batch 2534: loss 0.122723\n",
      "batch 2535: loss 0.098854\n",
      "batch 2536: loss 0.058657\n",
      "batch 2537: loss 0.180858\n",
      "batch 2538: loss 0.097031\n",
      "batch 2539: loss 0.081979\n",
      "batch 2540: loss 0.075834\n",
      "batch 2541: loss 0.105047\n",
      "batch 2542: loss 0.037518\n",
      "batch 2543: loss 0.033172\n",
      "batch 2544: loss 0.170613\n",
      "batch 2545: loss 0.160254\n",
      "batch 2546: loss 0.044649\n",
      "batch 2547: loss 0.208754\n",
      "batch 2548: loss 0.102450\n",
      "batch 2549: loss 0.300266\n",
      "batch 2550: loss 0.126349\n",
      "batch 2551: loss 0.017918\n",
      "batch 2552: loss 0.034741\n",
      "batch 2553: loss 0.144205\n",
      "batch 2554: loss 0.145406\n",
      "batch 2555: loss 0.027571\n",
      "batch 2556: loss 0.032458\n",
      "batch 2557: loss 0.016991\n",
      "batch 2558: loss 0.062297\n",
      "batch 2559: loss 0.085351\n",
      "batch 2560: loss 0.077455\n",
      "batch 2561: loss 0.078121\n",
      "batch 2562: loss 0.029461\n",
      "batch 2563: loss 0.137607\n",
      "batch 2564: loss 0.183082\n",
      "batch 2565: loss 0.089822\n",
      "batch 2566: loss 0.061724\n",
      "batch 2567: loss 0.355733\n",
      "batch 2568: loss 0.213583\n",
      "batch 2569: loss 0.078785\n",
      "batch 2570: loss 0.139015\n",
      "batch 2571: loss 0.028312\n",
      "batch 2572: loss 0.040977\n",
      "batch 2573: loss 0.080914\n",
      "batch 2574: loss 0.076729\n",
      "batch 2575: loss 0.245966\n",
      "batch 2576: loss 0.051648\n",
      "batch 2577: loss 0.239692\n",
      "batch 2578: loss 0.105284\n",
      "batch 2579: loss 0.116359\n",
      "batch 2580: loss 0.091023\n",
      "batch 2581: loss 0.078858\n",
      "batch 2582: loss 0.011291\n",
      "batch 2583: loss 0.062068\n",
      "batch 2584: loss 0.098517\n",
      "batch 2585: loss 0.181127\n",
      "batch 2586: loss 0.185300\n",
      "batch 2587: loss 0.195013\n",
      "batch 2588: loss 0.085199\n",
      "batch 2589: loss 0.185379\n",
      "batch 2590: loss 0.113548\n",
      "batch 2591: loss 0.100967\n",
      "batch 2592: loss 0.058242\n",
      "batch 2593: loss 0.310326\n",
      "batch 2594: loss 0.115324\n",
      "batch 2595: loss 0.014557\n",
      "batch 2596: loss 0.283362\n",
      "batch 2597: loss 0.090059\n",
      "batch 2598: loss 0.142830\n",
      "batch 2599: loss 0.041498\n",
      "batch 2600: loss 0.063732\n",
      "batch 2601: loss 0.241986\n",
      "batch 2602: loss 0.106321\n",
      "batch 2603: loss 0.157538\n",
      "batch 2604: loss 0.085782\n",
      "batch 2605: loss 0.049019\n",
      "batch 2606: loss 0.074962\n",
      "batch 2607: loss 0.109737\n",
      "batch 2608: loss 0.150053\n",
      "batch 2609: loss 0.141976\n",
      "batch 2610: loss 0.014082\n",
      "batch 2611: loss 0.149380\n",
      "batch 2612: loss 0.085808\n",
      "batch 2613: loss 0.036230\n",
      "batch 2614: loss 0.173158\n",
      "batch 2615: loss 0.148402\n",
      "batch 2616: loss 0.111615\n",
      "batch 2617: loss 0.021326\n",
      "batch 2618: loss 0.069622\n",
      "batch 2619: loss 0.219844\n",
      "batch 2620: loss 0.089063\n",
      "batch 2621: loss 0.136998\n",
      "batch 2622: loss 0.055561\n",
      "batch 2623: loss 0.010091\n",
      "batch 2624: loss 0.068981\n",
      "batch 2625: loss 0.135789\n",
      "batch 2626: loss 0.099664\n",
      "batch 2627: loss 0.084592\n",
      "batch 2628: loss 0.106290\n",
      "batch 2629: loss 0.036769\n",
      "batch 2630: loss 0.036762\n",
      "batch 2631: loss 0.165836\n",
      "batch 2632: loss 0.034942\n",
      "batch 2633: loss 0.120851\n",
      "batch 2634: loss 0.139381\n",
      "batch 2635: loss 0.142741\n",
      "batch 2636: loss 0.040049\n",
      "batch 2637: loss 0.279053\n",
      "batch 2638: loss 0.085606\n",
      "batch 2639: loss 0.071236\n",
      "batch 2640: loss 0.022122\n",
      "batch 2641: loss 0.068603\n",
      "batch 2642: loss 0.183790\n",
      "batch 2643: loss 0.024286\n",
      "batch 2644: loss 0.046117\n",
      "batch 2645: loss 0.059496\n",
      "batch 2646: loss 0.099722\n",
      "batch 2647: loss 0.151358\n",
      "batch 2648: loss 0.232392\n",
      "batch 2649: loss 0.223190\n",
      "batch 2650: loss 0.054146\n",
      "batch 2651: loss 0.073329\n",
      "batch 2652: loss 0.233067\n",
      "batch 2653: loss 0.126994\n",
      "batch 2654: loss 0.028930\n",
      "batch 2655: loss 0.127243\n",
      "batch 2656: loss 0.089600\n",
      "batch 2657: loss 0.101738\n",
      "batch 2658: loss 0.265333\n",
      "batch 2659: loss 0.084870\n",
      "batch 2660: loss 0.064684\n",
      "batch 2661: loss 0.120506\n",
      "batch 2662: loss 0.265124\n",
      "batch 2663: loss 0.136708\n",
      "batch 2664: loss 0.308970\n",
      "batch 2665: loss 0.155918\n",
      "batch 2666: loss 0.092577\n",
      "batch 2667: loss 0.243273\n",
      "batch 2668: loss 0.115094\n",
      "batch 2669: loss 0.061029\n",
      "batch 2670: loss 0.032223\n",
      "batch 2671: loss 0.236464\n",
      "batch 2672: loss 0.148991\n",
      "batch 2673: loss 0.052922\n",
      "batch 2674: loss 0.131644\n",
      "batch 2675: loss 0.208150\n",
      "batch 2676: loss 0.034555\n",
      "batch 2677: loss 0.159610\n",
      "batch 2678: loss 0.043759\n",
      "batch 2679: loss 0.044180\n",
      "batch 2680: loss 0.062795\n",
      "batch 2681: loss 0.084781\n",
      "batch 2682: loss 0.147827\n",
      "batch 2683: loss 0.118018\n",
      "batch 2684: loss 0.025312\n",
      "batch 2685: loss 0.072815\n",
      "batch 2686: loss 0.272139\n",
      "batch 2687: loss 0.129888\n",
      "batch 2688: loss 0.219747\n",
      "batch 2689: loss 0.083791\n",
      "batch 2690: loss 0.128130\n",
      "batch 2691: loss 0.177590\n",
      "batch 2692: loss 0.034999\n",
      "batch 2693: loss 0.043636\n",
      "batch 2694: loss 0.031433\n",
      "batch 2695: loss 0.033330\n",
      "batch 2696: loss 0.042159\n",
      "batch 2697: loss 0.042608\n",
      "batch 2698: loss 0.089474\n",
      "batch 2699: loss 0.107166\n",
      "batch 2700: loss 0.333454\n",
      "batch 2701: loss 0.109486\n",
      "batch 2702: loss 0.123523\n",
      "batch 2703: loss 0.067014\n",
      "batch 2704: loss 0.049423\n",
      "batch 2705: loss 0.083632\n",
      "batch 2706: loss 0.111928\n",
      "batch 2707: loss 0.063757\n",
      "batch 2708: loss 0.087028\n",
      "batch 2709: loss 0.073909\n",
      "batch 2710: loss 0.145412\n",
      "batch 2711: loss 0.087681\n",
      "batch 2712: loss 0.103003\n",
      "batch 2713: loss 0.051278\n",
      "batch 2714: loss 0.045534\n",
      "batch 2715: loss 0.094015\n",
      "batch 2716: loss 0.206462\n",
      "batch 2717: loss 0.058532\n",
      "batch 2718: loss 0.162606\n",
      "batch 2719: loss 0.180168\n",
      "batch 2720: loss 0.047980\n",
      "batch 2721: loss 0.132679\n",
      "batch 2722: loss 0.018324\n",
      "batch 2723: loss 0.106556\n",
      "batch 2724: loss 0.035097\n",
      "batch 2725: loss 0.133225\n",
      "batch 2726: loss 0.054691\n",
      "batch 2727: loss 0.120815\n",
      "batch 2728: loss 0.048091\n",
      "batch 2729: loss 0.036073\n",
      "batch 2730: loss 0.331330\n",
      "batch 2731: loss 0.023271\n",
      "batch 2732: loss 0.179817\n",
      "batch 2733: loss 0.172799\n",
      "batch 2734: loss 0.147630\n",
      "batch 2735: loss 0.030614\n",
      "batch 2736: loss 0.052893\n",
      "batch 2737: loss 0.079006\n",
      "batch 2738: loss 0.023031\n",
      "batch 2739: loss 0.190080\n",
      "batch 2740: loss 0.091561\n",
      "batch 2741: loss 0.125111\n",
      "batch 2742: loss 0.073958\n",
      "batch 2743: loss 0.141958\n",
      "batch 2744: loss 0.041731\n",
      "batch 2745: loss 0.072146\n",
      "batch 2746: loss 0.077613\n",
      "batch 2747: loss 0.034809\n",
      "batch 2748: loss 0.137868\n",
      "batch 2749: loss 0.012450\n",
      "batch 2750: loss 0.088171\n",
      "batch 2751: loss 0.055696\n",
      "batch 2752: loss 0.069632\n",
      "batch 2753: loss 0.195183\n",
      "batch 2754: loss 0.103726\n",
      "batch 2755: loss 0.139148\n",
      "batch 2756: loss 0.030156\n",
      "batch 2757: loss 0.065624\n",
      "batch 2758: loss 0.048863\n",
      "batch 2759: loss 0.200678\n",
      "batch 2760: loss 0.020400\n",
      "batch 2761: loss 0.064202\n",
      "batch 2762: loss 0.105721\n",
      "batch 2763: loss 0.216926\n",
      "batch 2764: loss 0.187961\n",
      "batch 2765: loss 0.077215\n",
      "batch 2766: loss 0.087623\n",
      "batch 2767: loss 0.118918\n",
      "batch 2768: loss 0.097885\n",
      "batch 2769: loss 0.024567\n",
      "batch 2770: loss 0.424551\n",
      "batch 2771: loss 0.024550\n",
      "batch 2772: loss 0.057110\n",
      "batch 2773: loss 0.072330\n",
      "batch 2774: loss 0.129098\n",
      "batch 2775: loss 0.047274\n",
      "batch 2776: loss 0.084395\n",
      "batch 2777: loss 0.036977\n",
      "batch 2778: loss 0.128219\n",
      "batch 2779: loss 0.190407\n",
      "batch 2780: loss 0.072242\n",
      "batch 2781: loss 0.029480\n",
      "batch 2782: loss 0.106088\n",
      "batch 2783: loss 0.135476\n",
      "batch 2784: loss 0.066629\n",
      "batch 2785: loss 0.016243\n",
      "batch 2786: loss 0.080046\n",
      "batch 2787: loss 0.015330\n",
      "batch 2788: loss 0.016318\n",
      "batch 2789: loss 0.051085\n",
      "batch 2790: loss 0.078769\n",
      "batch 2791: loss 0.073634\n",
      "batch 2792: loss 0.151741\n",
      "batch 2793: loss 0.213190\n",
      "batch 2794: loss 0.158635\n",
      "batch 2795: loss 0.008886\n",
      "batch 2796: loss 0.058951\n",
      "batch 2797: loss 0.052240\n",
      "batch 2798: loss 0.040069\n",
      "batch 2799: loss 0.023240\n",
      "batch 2800: loss 0.172342\n",
      "batch 2801: loss 0.081437\n",
      "batch 2802: loss 0.210645\n",
      "batch 2803: loss 0.143828\n",
      "batch 2804: loss 0.047244\n",
      "batch 2805: loss 0.148842\n",
      "batch 2806: loss 0.057475\n",
      "batch 2807: loss 0.095028\n",
      "batch 2808: loss 0.173001\n",
      "batch 2809: loss 0.016270\n",
      "batch 2810: loss 0.050627\n",
      "batch 2811: loss 0.052171\n",
      "batch 2812: loss 0.018922\n",
      "batch 2813: loss 0.081889\n",
      "batch 2814: loss 0.081596\n",
      "batch 2815: loss 0.053933\n",
      "batch 2816: loss 0.080182\n",
      "batch 2817: loss 0.164635\n",
      "batch 2818: loss 0.078723\n",
      "batch 2819: loss 0.019705\n",
      "batch 2820: loss 0.082220\n",
      "batch 2821: loss 0.097036\n",
      "batch 2822: loss 0.092198\n",
      "batch 2823: loss 0.024130\n",
      "batch 2824: loss 0.038761\n",
      "batch 2825: loss 0.059863\n",
      "batch 2826: loss 0.085371\n",
      "batch 2827: loss 0.141947\n",
      "batch 2828: loss 0.070360\n",
      "batch 2829: loss 0.040658\n",
      "batch 2830: loss 0.100220\n",
      "batch 2831: loss 0.021280\n",
      "batch 2832: loss 0.104191\n",
      "batch 2833: loss 0.432950\n",
      "batch 2834: loss 0.074445\n",
      "batch 2835: loss 0.061834\n",
      "batch 2836: loss 0.177690\n",
      "batch 2837: loss 0.108241\n",
      "batch 2838: loss 0.145197\n",
      "batch 2839: loss 0.202506\n",
      "batch 2840: loss 0.193773\n",
      "batch 2841: loss 0.452573\n",
      "batch 2842: loss 0.036208\n",
      "batch 2843: loss 0.089599\n",
      "batch 2844: loss 0.175231\n",
      "batch 2845: loss 0.073996\n",
      "batch 2846: loss 0.153652\n",
      "batch 2847: loss 0.029666\n",
      "batch 2848: loss 0.198917\n",
      "batch 2849: loss 0.038828\n",
      "batch 2850: loss 0.300275\n",
      "batch 2851: loss 0.113128\n",
      "batch 2852: loss 0.038141\n",
      "batch 2853: loss 0.171890\n",
      "batch 2854: loss 0.021390\n",
      "batch 2855: loss 0.107571\n",
      "batch 2856: loss 0.219040\n",
      "batch 2857: loss 0.039957\n",
      "batch 2858: loss 0.030030\n",
      "batch 2859: loss 0.104264\n",
      "batch 2860: loss 0.109574\n",
      "batch 2861: loss 0.103075\n",
      "batch 2862: loss 0.022472\n",
      "batch 2863: loss 0.167266\n",
      "batch 2864: loss 0.201651\n",
      "batch 2865: loss 0.029024\n",
      "batch 2866: loss 0.059412\n",
      "batch 2867: loss 0.025395\n",
      "batch 2868: loss 0.061919\n",
      "batch 2869: loss 0.224454\n",
      "batch 2870: loss 0.218936\n",
      "batch 2871: loss 0.021508\n",
      "batch 2872: loss 0.016493\n",
      "batch 2873: loss 0.013771\n",
      "batch 2874: loss 0.063358\n",
      "batch 2875: loss 0.064407\n",
      "batch 2876: loss 0.097447\n",
      "batch 2877: loss 0.073989\n",
      "batch 2878: loss 0.109177\n",
      "batch 2879: loss 0.110961\n",
      "batch 2880: loss 0.076853\n",
      "batch 2881: loss 0.148060\n",
      "batch 2882: loss 0.208419\n",
      "batch 2883: loss 0.131271\n",
      "batch 2884: loss 0.050975\n",
      "batch 2885: loss 0.109329\n",
      "batch 2886: loss 0.026429\n",
      "batch 2887: loss 0.298286\n",
      "batch 2888: loss 0.031819\n",
      "batch 2889: loss 0.081028\n",
      "batch 2890: loss 0.129508\n",
      "batch 2891: loss 0.158353\n",
      "batch 2892: loss 0.117395\n",
      "batch 2893: loss 0.123621\n",
      "batch 2894: loss 0.060409\n",
      "batch 2895: loss 0.043711\n",
      "batch 2896: loss 0.021933\n",
      "batch 2897: loss 0.118290\n",
      "batch 2898: loss 0.187636\n",
      "batch 2899: loss 0.084998\n",
      "batch 2900: loss 0.038598\n",
      "batch 2901: loss 0.120449\n",
      "batch 2902: loss 0.038421\n",
      "batch 2903: loss 0.087904\n",
      "batch 2904: loss 0.029579\n",
      "batch 2905: loss 0.026862\n",
      "batch 2906: loss 0.063050\n",
      "batch 2907: loss 0.078846\n",
      "batch 2908: loss 0.096868\n",
      "batch 2909: loss 0.180286\n",
      "batch 2910: loss 0.022610\n",
      "batch 2911: loss 0.035769\n",
      "batch 2912: loss 0.017393\n",
      "batch 2913: loss 0.051349\n",
      "batch 2914: loss 0.032675\n",
      "batch 2915: loss 0.270037\n",
      "batch 2916: loss 0.031743\n",
      "batch 2917: loss 0.082382\n",
      "batch 2918: loss 0.172139\n",
      "batch 2919: loss 0.016778\n",
      "batch 2920: loss 0.188960\n",
      "batch 2921: loss 0.018905\n",
      "batch 2922: loss 0.072115\n",
      "batch 2923: loss 0.058100\n",
      "batch 2924: loss 0.022245\n",
      "batch 2925: loss 0.066412\n",
      "batch 2926: loss 0.101809\n",
      "batch 2927: loss 0.027286\n",
      "batch 2928: loss 0.051867\n",
      "batch 2929: loss 0.021383\n",
      "batch 2930: loss 0.293276\n",
      "batch 2931: loss 0.169194\n",
      "batch 2932: loss 0.051242\n",
      "batch 2933: loss 0.079388\n",
      "batch 2934: loss 0.017573\n",
      "batch 2935: loss 0.021281\n",
      "batch 2936: loss 0.185663\n",
      "batch 2937: loss 0.036666\n",
      "batch 2938: loss 0.047971\n",
      "batch 2939: loss 0.008537\n",
      "batch 2940: loss 0.072899\n",
      "batch 2941: loss 0.031944\n",
      "batch 2942: loss 0.047136\n",
      "batch 2943: loss 0.090613\n",
      "batch 2944: loss 0.083689\n",
      "batch 2945: loss 0.019935\n",
      "batch 2946: loss 0.057966\n",
      "batch 2947: loss 0.183319\n",
      "batch 2948: loss 0.074544\n",
      "batch 2949: loss 0.040734\n",
      "batch 2950: loss 0.209194\n",
      "batch 2951: loss 0.060937\n",
      "batch 2952: loss 0.048371\n",
      "batch 2953: loss 0.050659\n",
      "batch 2954: loss 0.024774\n",
      "batch 2955: loss 0.218239\n",
      "batch 2956: loss 0.163449\n",
      "batch 2957: loss 0.201489\n",
      "batch 2958: loss 0.021090\n",
      "batch 2959: loss 0.051251\n",
      "batch 2960: loss 0.042538\n",
      "batch 2961: loss 0.168543\n",
      "batch 2962: loss 0.092003\n",
      "batch 2963: loss 0.028921\n",
      "batch 2964: loss 0.102911\n",
      "batch 2965: loss 0.028740\n",
      "batch 2966: loss 0.195257\n",
      "batch 2967: loss 0.131778\n",
      "batch 2968: loss 0.058601\n",
      "batch 2969: loss 0.018999\n",
      "batch 2970: loss 0.083031\n",
      "batch 2971: loss 0.159072\n",
      "batch 2972: loss 0.110076\n",
      "batch 2973: loss 0.060737\n",
      "batch 2974: loss 0.021276\n",
      "batch 2975: loss 0.182717\n",
      "batch 2976: loss 0.025288\n",
      "batch 2977: loss 0.029006\n",
      "batch 2978: loss 0.069512\n",
      "batch 2979: loss 0.106589\n",
      "batch 2980: loss 0.074036\n",
      "batch 2981: loss 0.188976\n",
      "batch 2982: loss 0.043836\n",
      "batch 2983: loss 0.074549\n",
      "batch 2984: loss 0.087755\n",
      "batch 2985: loss 0.032708\n",
      "batch 2986: loss 0.053343\n",
      "batch 2987: loss 0.041063\n",
      "batch 2988: loss 0.131269\n",
      "batch 2989: loss 0.147014\n",
      "batch 2990: loss 0.104455\n",
      "batch 2991: loss 0.038761\n",
      "batch 2992: loss 0.042255\n",
      "batch 2993: loss 0.030591\n",
      "batch 2994: loss 0.044219\n",
      "batch 2995: loss 0.184158\n",
      "batch 2996: loss 0.030568\n",
      "batch 2997: loss 0.087858\n",
      "batch 2998: loss 0.086585\n",
      "batch 2999: loss 0.141803\n",
      "batch 3000: loss 0.062087\n",
      "batch 3001: loss 0.234822\n",
      "batch 3002: loss 0.023331\n",
      "batch 3003: loss 0.133199\n",
      "batch 3004: loss 0.073940\n",
      "batch 3005: loss 0.036874\n",
      "batch 3006: loss 0.093146\n",
      "batch 3007: loss 0.024184\n",
      "batch 3008: loss 0.043914\n",
      "batch 3009: loss 0.126121\n",
      "batch 3010: loss 0.063506\n",
      "batch 3011: loss 0.065679\n",
      "batch 3012: loss 0.024010\n",
      "batch 3013: loss 0.192637\n",
      "batch 3014: loss 0.117771\n",
      "batch 3015: loss 0.190114\n",
      "batch 3016: loss 0.068054\n",
      "batch 3017: loss 0.147158\n",
      "batch 3018: loss 0.093865\n",
      "batch 3019: loss 0.322538\n",
      "batch 3020: loss 0.062516\n",
      "batch 3021: loss 0.095212\n",
      "batch 3022: loss 0.079877\n",
      "batch 3023: loss 0.072003\n",
      "batch 3024: loss 0.149335\n",
      "batch 3025: loss 0.085602\n",
      "batch 3026: loss 0.148873\n",
      "batch 3027: loss 0.029968\n",
      "batch 3028: loss 0.042559\n",
      "batch 3029: loss 0.043986\n",
      "batch 3030: loss 0.009292\n",
      "batch 3031: loss 0.036886\n",
      "batch 3032: loss 0.220355\n",
      "batch 3033: loss 0.064844\n",
      "batch 3034: loss 0.019477\n",
      "batch 3035: loss 0.313807\n",
      "batch 3036: loss 0.171809\n",
      "batch 3037: loss 0.035649\n",
      "batch 3038: loss 0.097165\n",
      "batch 3039: loss 0.176947\n",
      "batch 3040: loss 0.147046\n",
      "batch 3041: loss 0.109180\n",
      "batch 3042: loss 0.046018\n",
      "batch 3043: loss 0.120982\n",
      "batch 3044: loss 0.089912\n",
      "batch 3045: loss 0.043296\n",
      "batch 3046: loss 0.016593\n",
      "batch 3047: loss 0.052850\n",
      "batch 3048: loss 0.021436\n",
      "batch 3049: loss 0.150102\n",
      "batch 3050: loss 0.039286\n",
      "batch 3051: loss 0.044490\n",
      "batch 3052: loss 0.096082\n",
      "batch 3053: loss 0.115595\n",
      "batch 3054: loss 0.025942\n",
      "batch 3055: loss 0.087969\n",
      "batch 3056: loss 0.067150\n",
      "batch 3057: loss 0.084972\n",
      "batch 3058: loss 0.032070\n",
      "batch 3059: loss 0.091777\n",
      "batch 3060: loss 0.074756\n",
      "batch 3061: loss 0.073861\n",
      "batch 3062: loss 0.058178\n",
      "batch 3063: loss 0.031292\n",
      "batch 3064: loss 0.146823\n",
      "batch 3065: loss 0.122407\n",
      "batch 3066: loss 0.021833\n",
      "batch 3067: loss 0.078015\n",
      "batch 3068: loss 0.068929\n",
      "batch 3069: loss 0.028044\n",
      "batch 3070: loss 0.018885\n",
      "batch 3071: loss 0.157895\n",
      "batch 3072: loss 0.135609\n",
      "batch 3073: loss 0.013895\n",
      "batch 3074: loss 0.041345\n",
      "batch 3075: loss 0.088042\n",
      "batch 3076: loss 0.097166\n",
      "batch 3077: loss 0.170516\n",
      "batch 3078: loss 0.030636\n",
      "batch 3079: loss 0.040730\n",
      "batch 3080: loss 0.079617\n",
      "batch 3081: loss 0.167059\n",
      "batch 3082: loss 0.033785\n",
      "batch 3083: loss 0.060113\n",
      "batch 3084: loss 0.224975\n",
      "batch 3085: loss 0.010291\n",
      "batch 3086: loss 0.150959\n",
      "batch 3087: loss 0.016211\n",
      "batch 3088: loss 0.193102\n",
      "batch 3089: loss 0.042142\n",
      "batch 3090: loss 0.030510\n",
      "batch 3091: loss 0.108440\n",
      "batch 3092: loss 0.102238\n",
      "batch 3093: loss 0.079999\n",
      "batch 3094: loss 0.037784\n",
      "batch 3095: loss 0.058071\n",
      "batch 3096: loss 0.152097\n",
      "batch 3097: loss 0.047332\n",
      "batch 3098: loss 0.184227\n",
      "batch 3099: loss 0.061342\n",
      "batch 3100: loss 0.087686\n",
      "batch 3101: loss 0.051865\n",
      "batch 3102: loss 0.057048\n",
      "batch 3103: loss 0.125836\n",
      "batch 3104: loss 0.028266\n",
      "batch 3105: loss 0.035859\n",
      "batch 3106: loss 0.089953\n",
      "batch 3107: loss 0.114557\n",
      "batch 3108: loss 0.077675\n",
      "batch 3109: loss 0.046745\n",
      "batch 3110: loss 0.068355\n",
      "batch 3111: loss 0.184299\n",
      "batch 3112: loss 0.040611\n",
      "batch 3113: loss 0.013084\n",
      "batch 3114: loss 0.018493\n",
      "batch 3115: loss 0.079338\n",
      "batch 3116: loss 0.153161\n",
      "batch 3117: loss 0.068130\n",
      "batch 3118: loss 0.037374\n",
      "batch 3119: loss 0.099554\n",
      "batch 3120: loss 0.051893\n",
      "batch 3121: loss 0.033412\n",
      "batch 3122: loss 0.082568\n",
      "batch 3123: loss 0.053595\n",
      "batch 3124: loss 0.026379\n",
      "batch 3125: loss 0.037096\n",
      "batch 3126: loss 0.026917\n",
      "batch 3127: loss 0.064768\n",
      "batch 3128: loss 0.065943\n",
      "batch 3129: loss 0.074916\n",
      "batch 3130: loss 0.078663\n",
      "batch 3131: loss 0.123600\n",
      "batch 3132: loss 0.095972\n",
      "batch 3133: loss 0.093447\n",
      "batch 3134: loss 0.062790\n",
      "batch 3135: loss 0.153691\n",
      "batch 3136: loss 0.139104\n",
      "batch 3137: loss 0.005417\n",
      "batch 3138: loss 0.081250\n",
      "batch 3139: loss 0.026510\n",
      "batch 3140: loss 0.105566\n",
      "batch 3141: loss 0.038817\n",
      "batch 3142: loss 0.018005\n",
      "batch 3143: loss 0.040017\n",
      "batch 3144: loss 0.088775\n",
      "batch 3145: loss 0.068013\n",
      "batch 3146: loss 0.155685\n",
      "batch 3147: loss 0.125633\n",
      "batch 3148: loss 0.123523\n",
      "batch 3149: loss 0.090946\n",
      "batch 3150: loss 0.156562\n",
      "batch 3151: loss 0.096019\n",
      "batch 3152: loss 0.023967\n",
      "batch 3153: loss 0.050203\n",
      "batch 3154: loss 0.096552\n",
      "batch 3155: loss 0.180018\n",
      "batch 3156: loss 0.042924\n",
      "batch 3157: loss 0.097995\n",
      "batch 3158: loss 0.032452\n",
      "batch 3159: loss 0.097531\n",
      "batch 3160: loss 0.079826\n",
      "batch 3161: loss 0.009275\n",
      "batch 3162: loss 0.174622\n",
      "batch 3163: loss 0.084203\n",
      "batch 3164: loss 0.092648\n",
      "batch 3165: loss 0.053679\n",
      "batch 3166: loss 0.074045\n",
      "batch 3167: loss 0.042823\n",
      "batch 3168: loss 0.031615\n",
      "batch 3169: loss 0.029137\n",
      "batch 3170: loss 0.012717\n",
      "batch 3171: loss 0.037376\n",
      "batch 3172: loss 0.067528\n",
      "batch 3173: loss 0.165288\n",
      "batch 3174: loss 0.107643\n",
      "batch 3175: loss 0.161528\n",
      "batch 3176: loss 0.069822\n",
      "batch 3177: loss 0.059273\n",
      "batch 3178: loss 0.141311\n",
      "batch 3179: loss 0.061525\n",
      "batch 3180: loss 0.140389\n",
      "batch 3181: loss 0.167520\n",
      "batch 3182: loss 0.017775\n",
      "batch 3183: loss 0.035098\n",
      "batch 3184: loss 0.082788\n",
      "batch 3185: loss 0.124023\n",
      "batch 3186: loss 0.081531\n",
      "batch 3187: loss 0.013939\n",
      "batch 3188: loss 0.107407\n",
      "batch 3189: loss 0.024202\n",
      "batch 3190: loss 0.018711\n",
      "batch 3191: loss 0.067247\n",
      "batch 3192: loss 0.151571\n",
      "batch 3193: loss 0.087150\n",
      "batch 3194: loss 0.086549\n",
      "batch 3195: loss 0.074812\n",
      "batch 3196: loss 0.005787\n",
      "batch 3197: loss 0.039651\n",
      "batch 3198: loss 0.043771\n",
      "batch 3199: loss 0.047702\n",
      "batch 3200: loss 0.046000\n",
      "batch 3201: loss 0.137805\n",
      "batch 3202: loss 0.018988\n",
      "batch 3203: loss 0.018195\n",
      "batch 3204: loss 0.037516\n",
      "batch 3205: loss 0.032569\n",
      "batch 3206: loss 0.112076\n",
      "batch 3207: loss 0.131081\n",
      "batch 3208: loss 0.127477\n",
      "batch 3209: loss 0.054878\n",
      "batch 3210: loss 0.137042\n",
      "batch 3211: loss 0.018881\n",
      "batch 3212: loss 0.039276\n",
      "batch 3213: loss 0.062743\n",
      "batch 3214: loss 0.047952\n",
      "batch 3215: loss 0.113999\n",
      "batch 3216: loss 0.025540\n",
      "batch 3217: loss 0.135667\n",
      "batch 3218: loss 0.043362\n",
      "batch 3219: loss 0.035922\n",
      "batch 3220: loss 0.026749\n",
      "batch 3221: loss 0.208799\n",
      "batch 3222: loss 0.025237\n",
      "batch 3223: loss 0.046862\n",
      "batch 3224: loss 0.151563\n",
      "batch 3225: loss 0.116896\n",
      "batch 3226: loss 0.075965\n",
      "batch 3227: loss 0.017206\n",
      "batch 3228: loss 0.125702\n",
      "batch 3229: loss 0.110531\n",
      "batch 3230: loss 0.105974\n",
      "batch 3231: loss 0.053152\n",
      "batch 3232: loss 0.049303\n",
      "batch 3233: loss 0.092495\n",
      "batch 3234: loss 0.044836\n",
      "batch 3235: loss 0.011914\n",
      "batch 3236: loss 0.096212\n",
      "batch 3237: loss 0.155020\n",
      "batch 3238: loss 0.375966\n",
      "batch 3239: loss 0.032366\n",
      "batch 3240: loss 0.072487\n",
      "batch 3241: loss 0.070901\n",
      "batch 3242: loss 0.062774\n",
      "batch 3243: loss 0.007351\n",
      "batch 3244: loss 0.043062\n",
      "batch 3245: loss 0.028099\n",
      "batch 3246: loss 0.134546\n",
      "batch 3247: loss 0.057773\n",
      "batch 3248: loss 0.113666\n",
      "batch 3249: loss 0.054672\n",
      "batch 3250: loss 0.065928\n",
      "batch 3251: loss 0.021518\n",
      "batch 3252: loss 0.025383\n",
      "batch 3253: loss 0.031347\n",
      "batch 3254: loss 0.116355\n",
      "batch 3255: loss 0.155869\n",
      "batch 3256: loss 0.063193\n",
      "batch 3257: loss 0.010502\n",
      "batch 3258: loss 0.143981\n",
      "batch 3259: loss 0.037005\n",
      "batch 3260: loss 0.014688\n",
      "batch 3261: loss 0.072473\n",
      "batch 3262: loss 0.075554\n",
      "batch 3263: loss 0.308243\n",
      "batch 3264: loss 0.071601\n",
      "batch 3265: loss 0.062514\n",
      "batch 3266: loss 0.035044\n",
      "batch 3267: loss 0.024367\n",
      "batch 3268: loss 0.072804\n",
      "batch 3269: loss 0.073431\n",
      "batch 3270: loss 0.054345\n",
      "batch 3271: loss 0.036959\n",
      "batch 3272: loss 0.110919\n",
      "batch 3273: loss 0.053707\n",
      "batch 3274: loss 0.094381\n",
      "batch 3275: loss 0.063754\n",
      "batch 3276: loss 0.035239\n",
      "batch 3277: loss 0.080178\n",
      "batch 3278: loss 0.192350\n",
      "batch 3279: loss 0.093778\n",
      "batch 3280: loss 0.083999\n",
      "batch 3281: loss 0.008726\n",
      "batch 3282: loss 0.104227\n",
      "batch 3283: loss 0.136580\n",
      "batch 3284: loss 0.041405\n",
      "batch 3285: loss 0.088270\n",
      "batch 3286: loss 0.028694\n",
      "batch 3287: loss 0.039803\n",
      "batch 3288: loss 0.032671\n",
      "batch 3289: loss 0.030185\n",
      "batch 3290: loss 0.012909\n",
      "batch 3291: loss 0.093713\n",
      "batch 3292: loss 0.067312\n",
      "batch 3293: loss 0.012291\n",
      "batch 3294: loss 0.111627\n",
      "batch 3295: loss 0.074474\n",
      "batch 3296: loss 0.069881\n",
      "batch 3297: loss 0.103470\n",
      "batch 3298: loss 0.101162\n",
      "batch 3299: loss 0.019423\n",
      "batch 3300: loss 0.057118\n",
      "batch 3301: loss 0.016531\n",
      "batch 3302: loss 0.051412\n",
      "batch 3303: loss 0.072136\n",
      "batch 3304: loss 0.079711\n",
      "batch 3305: loss 0.059144\n",
      "batch 3306: loss 0.052514\n",
      "batch 3307: loss 0.073849\n",
      "batch 3308: loss 0.092666\n",
      "batch 3309: loss 0.126225\n",
      "batch 3310: loss 0.104619\n",
      "batch 3311: loss 0.088855\n",
      "batch 3312: loss 0.018374\n",
      "batch 3313: loss 0.090281\n",
      "batch 3314: loss 0.114531\n",
      "batch 3315: loss 0.237669\n",
      "batch 3316: loss 0.149013\n",
      "batch 3317: loss 0.013856\n",
      "batch 3318: loss 0.025400\n",
      "batch 3319: loss 0.195747\n",
      "batch 3320: loss 0.170757\n",
      "batch 3321: loss 0.040985\n",
      "batch 3322: loss 0.008723\n",
      "batch 3323: loss 0.075527\n",
      "batch 3324: loss 0.278323\n",
      "batch 3325: loss 0.064381\n",
      "batch 3326: loss 0.039439\n",
      "batch 3327: loss 0.097113\n",
      "batch 3328: loss 0.272650\n",
      "batch 3329: loss 0.035378\n",
      "batch 3330: loss 0.024581\n",
      "batch 3331: loss 0.013133\n",
      "batch 3332: loss 0.282154\n",
      "batch 3333: loss 0.054791\n",
      "batch 3334: loss 0.015715\n",
      "batch 3335: loss 0.136879\n",
      "batch 3336: loss 0.060203\n",
      "batch 3337: loss 0.140406\n",
      "batch 3338: loss 0.227189\n",
      "batch 3339: loss 0.044162\n",
      "batch 3340: loss 0.085529\n",
      "batch 3341: loss 0.114627\n",
      "batch 3342: loss 0.055302\n",
      "batch 3343: loss 0.049232\n",
      "batch 3344: loss 0.069271\n",
      "batch 3345: loss 0.026865\n",
      "batch 3346: loss 0.023553\n",
      "batch 3347: loss 0.023395\n",
      "batch 3348: loss 0.013679\n",
      "batch 3349: loss 0.040501\n",
      "batch 3350: loss 0.074217\n",
      "batch 3351: loss 0.110388\n",
      "batch 3352: loss 0.051537\n",
      "batch 3353: loss 0.024658\n",
      "batch 3354: loss 0.197087\n",
      "batch 3355: loss 0.065848\n",
      "batch 3356: loss 0.006996\n",
      "batch 3357: loss 0.104493\n",
      "batch 3358: loss 0.190878\n",
      "batch 3359: loss 0.047816\n",
      "batch 3360: loss 0.106656\n",
      "batch 3361: loss 0.217998\n",
      "batch 3362: loss 0.053230\n",
      "batch 3363: loss 0.175838\n",
      "batch 3364: loss 0.175126\n",
      "batch 3365: loss 0.047505\n",
      "batch 3366: loss 0.153812\n",
      "batch 3367: loss 0.065343\n",
      "batch 3368: loss 0.040075\n",
      "batch 3369: loss 0.042832\n",
      "batch 3370: loss 0.019891\n",
      "batch 3371: loss 0.049847\n",
      "batch 3372: loss 0.029554\n",
      "batch 3373: loss 0.483409\n",
      "batch 3374: loss 0.042546\n",
      "batch 3375: loss 0.170595\n",
      "batch 3376: loss 0.102457\n",
      "batch 3377: loss 0.245875\n",
      "batch 3378: loss 0.045979\n",
      "batch 3379: loss 0.041476\n",
      "batch 3380: loss 0.195195\n",
      "batch 3381: loss 0.034638\n",
      "batch 3382: loss 0.183784\n",
      "batch 3383: loss 0.023945\n",
      "batch 3384: loss 0.030200\n",
      "batch 3385: loss 0.103405\n",
      "batch 3386: loss 0.177590\n",
      "batch 3387: loss 0.011199\n",
      "batch 3388: loss 0.212134\n",
      "batch 3389: loss 0.074498\n",
      "batch 3390: loss 0.059059\n",
      "batch 3391: loss 0.068088\n",
      "batch 3392: loss 0.096141\n",
      "batch 3393: loss 0.013943\n",
      "batch 3394: loss 0.079287\n",
      "batch 3395: loss 0.081219\n",
      "batch 3396: loss 0.084875\n",
      "batch 3397: loss 0.039808\n",
      "batch 3398: loss 0.010523\n",
      "batch 3399: loss 0.146366\n",
      "batch 3400: loss 0.137359\n",
      "batch 3401: loss 0.066259\n",
      "batch 3402: loss 0.086057\n",
      "batch 3403: loss 0.110229\n",
      "batch 3404: loss 0.029448\n",
      "batch 3405: loss 0.017196\n",
      "batch 3406: loss 0.032928\n",
      "batch 3407: loss 0.123601\n",
      "batch 3408: loss 0.450380\n",
      "batch 3409: loss 0.048059\n",
      "batch 3410: loss 0.039304\n",
      "batch 3411: loss 0.039279\n",
      "batch 3412: loss 0.009896\n",
      "batch 3413: loss 0.018164\n",
      "batch 3414: loss 0.026108\n",
      "batch 3415: loss 0.059518\n",
      "batch 3416: loss 0.057828\n",
      "batch 3417: loss 0.035788\n",
      "batch 3418: loss 0.042124\n",
      "batch 3419: loss 0.036515\n",
      "batch 3420: loss 0.250057\n",
      "batch 3421: loss 0.070802\n",
      "batch 3422: loss 0.037347\n",
      "batch 3423: loss 0.124058\n",
      "batch 3424: loss 0.026062\n",
      "batch 3425: loss 0.086339\n",
      "batch 3426: loss 0.021685\n",
      "batch 3427: loss 0.011051\n",
      "batch 3428: loss 0.040348\n",
      "batch 3429: loss 0.224756\n",
      "batch 3430: loss 0.028111\n",
      "batch 3431: loss 0.030553\n",
      "batch 3432: loss 0.026560\n",
      "batch 3433: loss 0.010997\n",
      "batch 3434: loss 0.037420\n",
      "batch 3435: loss 0.151713\n",
      "batch 3436: loss 0.056891\n",
      "batch 3437: loss 0.120672\n",
      "batch 3438: loss 0.037336\n",
      "batch 3439: loss 0.032729\n",
      "batch 3440: loss 0.099056\n",
      "batch 3441: loss 0.139153\n",
      "batch 3442: loss 0.086941\n",
      "batch 3443: loss 0.072719\n",
      "batch 3444: loss 0.159168\n",
      "batch 3445: loss 0.035756\n",
      "batch 3446: loss 0.051626\n",
      "batch 3447: loss 0.016141\n",
      "batch 3448: loss 0.196118\n",
      "batch 3449: loss 0.029703\n",
      "batch 3450: loss 0.056370\n",
      "batch 3451: loss 0.042895\n",
      "batch 3452: loss 0.015649\n",
      "batch 3453: loss 0.203572\n",
      "batch 3454: loss 0.149908\n",
      "batch 3455: loss 0.092995\n",
      "batch 3456: loss 0.049057\n",
      "batch 3457: loss 0.249182\n",
      "batch 3458: loss 0.040451\n",
      "batch 3459: loss 0.013805\n",
      "batch 3460: loss 0.084712\n",
      "batch 3461: loss 0.064897\n",
      "batch 3462: loss 0.083836\n",
      "batch 3463: loss 0.174864\n",
      "batch 3464: loss 0.061054\n",
      "batch 3465: loss 0.247601\n",
      "batch 3466: loss 0.040042\n",
      "batch 3467: loss 0.086563\n",
      "batch 3468: loss 0.041366\n",
      "batch 3469: loss 0.025927\n",
      "batch 3470: loss 0.011039\n",
      "batch 3471: loss 0.078475\n",
      "batch 3472: loss 0.087787\n",
      "batch 3473: loss 0.037485\n",
      "batch 3474: loss 0.052949\n",
      "batch 3475: loss 0.019351\n",
      "batch 3476: loss 0.059924\n",
      "batch 3477: loss 0.013923\n",
      "batch 3478: loss 0.031966\n",
      "batch 3479: loss 0.026702\n",
      "batch 3480: loss 0.118179\n",
      "batch 3481: loss 0.035979\n",
      "batch 3482: loss 0.124567\n",
      "batch 3483: loss 0.265806\n",
      "batch 3484: loss 0.057807\n",
      "batch 3485: loss 0.121171\n",
      "batch 3486: loss 0.226843\n",
      "batch 3487: loss 0.066790\n",
      "batch 3488: loss 0.016540\n",
      "batch 3489: loss 0.401198\n",
      "batch 3490: loss 0.460821\n",
      "batch 3491: loss 0.138737\n",
      "batch 3492: loss 0.234777\n",
      "batch 3493: loss 0.011371\n",
      "batch 3494: loss 0.109479\n",
      "batch 3495: loss 0.016412\n",
      "batch 3496: loss 0.026535\n",
      "batch 3497: loss 0.112200\n",
      "batch 3498: loss 0.059545\n",
      "batch 3499: loss 0.028221\n",
      "batch 3500: loss 0.040337\n",
      "batch 3501: loss 0.035839\n",
      "batch 3502: loss 0.006026\n",
      "batch 3503: loss 0.156506\n",
      "batch 3504: loss 0.064510\n",
      "batch 3505: loss 0.231839\n",
      "batch 3506: loss 0.074999\n",
      "batch 3507: loss 0.230718\n",
      "batch 3508: loss 0.072797\n",
      "batch 3509: loss 0.104601\n",
      "batch 3510: loss 0.160661\n",
      "batch 3511: loss 0.040113\n",
      "batch 3512: loss 0.046866\n",
      "batch 3513: loss 0.047460\n",
      "batch 3514: loss 0.068430\n",
      "batch 3515: loss 0.028480\n",
      "batch 3516: loss 0.030530\n",
      "batch 3517: loss 0.033964\n",
      "batch 3518: loss 0.090701\n",
      "batch 3519: loss 0.076692\n",
      "batch 3520: loss 0.071550\n",
      "batch 3521: loss 0.072527\n",
      "batch 3522: loss 0.012966\n",
      "batch 3523: loss 0.201057\n",
      "batch 3524: loss 0.021919\n",
      "batch 3525: loss 0.058917\n",
      "batch 3526: loss 0.135694\n",
      "batch 3527: loss 0.095382\n",
      "batch 3528: loss 0.013515\n",
      "batch 3529: loss 0.050464\n",
      "batch 3530: loss 0.107449\n",
      "batch 3531: loss 0.056015\n",
      "batch 3532: loss 0.082479\n",
      "batch 3533: loss 0.015836\n",
      "batch 3534: loss 0.162651\n",
      "batch 3535: loss 0.038384\n",
      "batch 3536: loss 0.037281\n",
      "batch 3537: loss 0.052344\n",
      "batch 3538: loss 0.169153\n",
      "batch 3539: loss 0.014148\n",
      "batch 3540: loss 0.016976\n",
      "batch 3541: loss 0.052208\n",
      "batch 3542: loss 0.113992\n",
      "batch 3543: loss 0.098835\n",
      "batch 3544: loss 0.050657\n",
      "batch 3545: loss 0.033267\n",
      "batch 3546: loss 0.245223\n",
      "batch 3547: loss 0.019589\n",
      "batch 3548: loss 0.090073\n",
      "batch 3549: loss 0.067818\n",
      "batch 3550: loss 0.040276\n",
      "batch 3551: loss 0.018354\n",
      "batch 3552: loss 0.223221\n",
      "batch 3553: loss 0.043559\n",
      "batch 3554: loss 0.021647\n",
      "batch 3555: loss 0.069435\n",
      "batch 3556: loss 0.120336\n",
      "batch 3557: loss 0.008046\n",
      "batch 3558: loss 0.239474\n",
      "batch 3559: loss 0.032556\n",
      "batch 3560: loss 0.026088\n",
      "batch 3561: loss 0.114882\n",
      "batch 3562: loss 0.039934\n",
      "batch 3563: loss 0.231609\n",
      "batch 3564: loss 0.035325\n",
      "batch 3565: loss 0.095425\n",
      "batch 3566: loss 0.066451\n",
      "batch 3567: loss 0.103737\n",
      "batch 3568: loss 0.067506\n",
      "batch 3569: loss 0.097715\n",
      "batch 3570: loss 0.019264\n",
      "batch 3571: loss 0.095807\n",
      "batch 3572: loss 0.014533\n",
      "batch 3573: loss 0.077655\n",
      "batch 3574: loss 0.023342\n",
      "batch 3575: loss 0.054237\n",
      "batch 3576: loss 0.080525\n",
      "batch 3577: loss 0.110207\n",
      "batch 3578: loss 0.244909\n",
      "batch 3579: loss 0.073683\n",
      "batch 3580: loss 0.081680\n",
      "batch 3581: loss 0.028377\n",
      "batch 3582: loss 0.070857\n",
      "batch 3583: loss 0.017461\n",
      "batch 3584: loss 0.009519\n",
      "batch 3585: loss 0.112669\n",
      "batch 3586: loss 0.025418\n",
      "batch 3587: loss 0.047682\n",
      "batch 3588: loss 0.033927\n",
      "batch 3589: loss 0.065065\n",
      "batch 3590: loss 0.152064\n",
      "batch 3591: loss 0.054772\n",
      "batch 3592: loss 0.087119\n",
      "batch 3593: loss 0.074391\n",
      "batch 3594: loss 0.061498\n",
      "batch 3595: loss 0.093257\n",
      "batch 3596: loss 0.045645\n",
      "batch 3597: loss 0.009908\n",
      "batch 3598: loss 0.028370\n",
      "batch 3599: loss 0.037504\n",
      "batch 3600: loss 0.011069\n",
      "batch 3601: loss 0.068587\n",
      "batch 3602: loss 0.042955\n",
      "batch 3603: loss 0.202629\n",
      "batch 3604: loss 0.061072\n",
      "batch 3605: loss 0.075189\n",
      "batch 3606: loss 0.013132\n",
      "batch 3607: loss 0.220096\n",
      "batch 3608: loss 0.048364\n",
      "batch 3609: loss 0.016734\n",
      "batch 3610: loss 0.131537\n",
      "batch 3611: loss 0.069112\n",
      "batch 3612: loss 0.095975\n",
      "batch 3613: loss 0.046299\n",
      "batch 3614: loss 0.050079\n",
      "batch 3615: loss 0.047244\n",
      "batch 3616: loss 0.037419\n",
      "batch 3617: loss 0.067596\n",
      "batch 3618: loss 0.012340\n",
      "batch 3619: loss 0.222543\n",
      "batch 3620: loss 0.035769\n",
      "batch 3621: loss 0.158942\n",
      "batch 3622: loss 0.084092\n",
      "batch 3623: loss 0.045877\n",
      "batch 3624: loss 0.085496\n",
      "batch 3625: loss 0.269120\n",
      "batch 3626: loss 0.034594\n",
      "batch 3627: loss 0.115335\n",
      "batch 3628: loss 0.258671\n",
      "batch 3629: loss 0.122898\n",
      "batch 3630: loss 0.042319\n",
      "batch 3631: loss 0.080993\n",
      "batch 3632: loss 0.128242\n",
      "batch 3633: loss 0.274053\n",
      "batch 3634: loss 0.082302\n",
      "batch 3635: loss 0.088059\n",
      "batch 3636: loss 0.153651\n",
      "batch 3637: loss 0.053488\n",
      "batch 3638: loss 0.095679\n",
      "batch 3639: loss 0.121289\n",
      "batch 3640: loss 0.048320\n",
      "batch 3641: loss 0.026347\n",
      "batch 3642: loss 0.160932\n",
      "batch 3643: loss 0.049951\n",
      "batch 3644: loss 0.027249\n",
      "batch 3645: loss 0.097944\n",
      "batch 3646: loss 0.180471\n",
      "batch 3647: loss 0.154072\n",
      "batch 3648: loss 0.115705\n",
      "batch 3649: loss 0.029569\n",
      "batch 3650: loss 0.155889\n",
      "batch 3651: loss 0.065169\n",
      "batch 3652: loss 0.047399\n",
      "batch 3653: loss 0.026150\n",
      "batch 3654: loss 0.013333\n",
      "batch 3655: loss 0.110326\n",
      "batch 3656: loss 0.160061\n",
      "batch 3657: loss 0.066685\n",
      "batch 3658: loss 0.025028\n",
      "batch 3659: loss 0.073430\n",
      "batch 3660: loss 0.021044\n",
      "batch 3661: loss 0.150872\n",
      "batch 3662: loss 0.111957\n",
      "batch 3663: loss 0.011250\n",
      "batch 3664: loss 0.027560\n",
      "batch 3665: loss 0.273969\n",
      "batch 3666: loss 0.053393\n",
      "batch 3667: loss 0.112142\n",
      "batch 3668: loss 0.124975\n",
      "batch 3669: loss 0.009705\n",
      "batch 3670: loss 0.142204\n",
      "batch 3671: loss 0.068803\n",
      "batch 3672: loss 0.043339\n",
      "batch 3673: loss 0.047039\n",
      "batch 3674: loss 0.019091\n",
      "batch 3675: loss 0.031493\n",
      "batch 3676: loss 0.142642\n",
      "batch 3677: loss 0.035045\n",
      "batch 3678: loss 0.167733\n",
      "batch 3679: loss 0.083329\n",
      "batch 3680: loss 0.008880\n",
      "batch 3681: loss 0.108186\n",
      "batch 3682: loss 0.076820\n",
      "batch 3683: loss 0.209543\n",
      "batch 3684: loss 0.039490\n",
      "batch 3685: loss 0.019930\n",
      "batch 3686: loss 0.092310\n",
      "batch 3687: loss 0.079912\n",
      "batch 3688: loss 0.166556\n",
      "batch 3689: loss 0.044704\n",
      "batch 3690: loss 0.075123\n",
      "batch 3691: loss 0.119035\n",
      "batch 3692: loss 0.040282\n",
      "batch 3693: loss 0.234191\n",
      "batch 3694: loss 0.194693\n",
      "batch 3695: loss 0.037831\n",
      "batch 3696: loss 0.107650\n",
      "batch 3697: loss 0.084607\n",
      "batch 3698: loss 0.047857\n",
      "batch 3699: loss 0.054435\n",
      "batch 3700: loss 0.328829\n",
      "batch 3701: loss 0.079687\n",
      "batch 3702: loss 0.058751\n",
      "batch 3703: loss 0.151169\n",
      "batch 3704: loss 0.036352\n",
      "batch 3705: loss 0.012318\n",
      "batch 3706: loss 0.060362\n",
      "batch 3707: loss 0.003631\n",
      "batch 3708: loss 0.052821\n",
      "batch 3709: loss 0.022278\n",
      "batch 3710: loss 0.063688\n",
      "batch 3711: loss 0.023321\n",
      "batch 3712: loss 0.087435\n",
      "batch 3713: loss 0.147580\n",
      "batch 3714: loss 0.007154\n",
      "batch 3715: loss 0.237730\n",
      "batch 3716: loss 0.036288\n",
      "batch 3717: loss 0.069495\n",
      "batch 3718: loss 0.077342\n",
      "batch 3719: loss 0.063458\n",
      "batch 3720: loss 0.069959\n",
      "batch 3721: loss 0.134538\n",
      "batch 3722: loss 0.151292\n",
      "batch 3723: loss 0.338357\n",
      "batch 3724: loss 0.008864\n",
      "batch 3725: loss 0.104699\n",
      "batch 3726: loss 0.187343\n",
      "batch 3727: loss 0.119358\n",
      "batch 3728: loss 0.063393\n",
      "batch 3729: loss 0.032495\n",
      "batch 3730: loss 0.034477\n",
      "batch 3731: loss 0.021487\n",
      "batch 3732: loss 0.027961\n",
      "batch 3733: loss 0.068906\n",
      "batch 3734: loss 0.136383\n",
      "batch 3735: loss 0.028086\n",
      "batch 3736: loss 0.062557\n",
      "batch 3737: loss 0.027406\n",
      "batch 3738: loss 0.036319\n",
      "batch 3739: loss 0.047810\n",
      "batch 3740: loss 0.007420\n",
      "batch 3741: loss 0.018721\n",
      "batch 3742: loss 0.033191\n",
      "batch 3743: loss 0.106189\n",
      "batch 3744: loss 0.017370\n",
      "batch 3745: loss 0.112145\n",
      "batch 3746: loss 0.155213\n",
      "batch 3747: loss 0.314848\n",
      "batch 3748: loss 0.036908\n",
      "batch 3749: loss 0.160712\n",
      "batch 3750: loss 0.065918\n",
      "batch 3751: loss 0.035509\n",
      "batch 3752: loss 0.126699\n",
      "batch 3753: loss 0.086822\n",
      "batch 3754: loss 0.121687\n",
      "batch 3755: loss 0.094326\n",
      "batch 3756: loss 0.092444\n",
      "batch 3757: loss 0.053989\n",
      "batch 3758: loss 0.112246\n",
      "batch 3759: loss 0.172117\n",
      "batch 3760: loss 0.110767\n",
      "batch 3761: loss 0.088670\n",
      "batch 3762: loss 0.020242\n",
      "batch 3763: loss 0.101499\n",
      "batch 3764: loss 0.038397\n",
      "batch 3765: loss 0.026754\n",
      "batch 3766: loss 0.157377\n",
      "batch 3767: loss 0.072934\n",
      "batch 3768: loss 0.029292\n",
      "batch 3769: loss 0.127828\n",
      "batch 3770: loss 0.077054\n",
      "batch 3771: loss 0.023451\n",
      "batch 3772: loss 0.047327\n",
      "batch 3773: loss 0.124651\n",
      "batch 3774: loss 0.095915\n",
      "batch 3775: loss 0.014956\n",
      "batch 3776: loss 0.018448\n",
      "batch 3777: loss 0.056030\n",
      "batch 3778: loss 0.104707\n",
      "batch 3779: loss 0.057251\n",
      "batch 3780: loss 0.105217\n",
      "batch 3781: loss 0.054955\n",
      "batch 3782: loss 0.056054\n",
      "batch 3783: loss 0.197167\n",
      "batch 3784: loss 0.015658\n",
      "batch 3785: loss 0.067261\n",
      "batch 3786: loss 0.026284\n",
      "batch 3787: loss 0.101502\n",
      "batch 3788: loss 0.141830\n",
      "batch 3789: loss 0.087364\n",
      "batch 3790: loss 0.150688\n",
      "batch 3791: loss 0.020096\n",
      "batch 3792: loss 0.041811\n",
      "batch 3793: loss 0.008339\n",
      "batch 3794: loss 0.032474\n",
      "batch 3795: loss 0.126206\n",
      "batch 3796: loss 0.102530\n",
      "batch 3797: loss 0.072605\n",
      "batch 3798: loss 0.073494\n",
      "batch 3799: loss 0.029605\n",
      "batch 3800: loss 0.076327\n",
      "batch 3801: loss 0.066689\n",
      "batch 3802: loss 0.060248\n",
      "batch 3803: loss 0.015883\n",
      "batch 3804: loss 0.185584\n",
      "batch 3805: loss 0.017275\n",
      "batch 3806: loss 0.055190\n",
      "batch 3807: loss 0.069389\n",
      "batch 3808: loss 0.019785\n",
      "batch 3809: loss 0.071579\n",
      "batch 3810: loss 0.018125\n",
      "batch 3811: loss 0.026202\n",
      "batch 3812: loss 0.043906\n",
      "batch 3813: loss 0.029914\n",
      "batch 3814: loss 0.055720\n",
      "batch 3815: loss 0.067981\n",
      "batch 3816: loss 0.008988\n",
      "batch 3817: loss 0.056374\n",
      "batch 3818: loss 0.019368\n",
      "batch 3819: loss 0.145214\n",
      "batch 3820: loss 0.080851\n",
      "batch 3821: loss 0.054054\n",
      "batch 3822: loss 0.087235\n",
      "batch 3823: loss 0.058089\n",
      "batch 3824: loss 0.013127\n",
      "batch 3825: loss 0.182435\n",
      "batch 3826: loss 0.309397\n",
      "batch 3827: loss 0.025859\n",
      "batch 3828: loss 0.019135\n",
      "batch 3829: loss 0.088726\n",
      "batch 3830: loss 0.049382\n",
      "batch 3831: loss 0.038772\n",
      "batch 3832: loss 0.010981\n",
      "batch 3833: loss 0.069944\n",
      "batch 3834: loss 0.032881\n",
      "batch 3835: loss 0.021621\n",
      "batch 3836: loss 0.039969\n",
      "batch 3837: loss 0.089042\n",
      "batch 3838: loss 0.259782\n",
      "batch 3839: loss 0.027918\n",
      "batch 3840: loss 0.111492\n",
      "batch 3841: loss 0.045207\n",
      "batch 3842: loss 0.133997\n",
      "batch 3843: loss 0.254728\n",
      "batch 3844: loss 0.027385\n",
      "batch 3845: loss 0.026836\n",
      "batch 3846: loss 0.207231\n",
      "batch 3847: loss 0.047999\n",
      "batch 3848: loss 0.052020\n",
      "batch 3849: loss 0.110401\n",
      "batch 3850: loss 0.040206\n",
      "batch 3851: loss 0.083119\n",
      "batch 3852: loss 0.075398\n",
      "batch 3853: loss 0.064062\n",
      "batch 3854: loss 0.040893\n",
      "batch 3855: loss 0.016013\n",
      "batch 3856: loss 0.042001\n",
      "batch 3857: loss 0.132786\n",
      "batch 3858: loss 0.145790\n",
      "batch 3859: loss 0.211709\n",
      "batch 3860: loss 0.039836\n",
      "batch 3861: loss 0.141820\n",
      "batch 3862: loss 0.076601\n",
      "batch 3863: loss 0.067502\n",
      "batch 3864: loss 0.045311\n",
      "batch 3865: loss 0.012697\n",
      "batch 3866: loss 0.037355\n",
      "batch 3867: loss 0.016183\n",
      "batch 3868: loss 0.029495\n",
      "batch 3869: loss 0.119482\n",
      "batch 3870: loss 0.067352\n",
      "batch 3871: loss 0.304799\n",
      "batch 3872: loss 0.007845\n",
      "batch 3873: loss 0.239295\n",
      "batch 3874: loss 0.155787\n",
      "batch 3875: loss 0.022681\n",
      "batch 3876: loss 0.069392\n",
      "batch 3877: loss 0.029889\n",
      "batch 3878: loss 0.123968\n",
      "batch 3879: loss 0.049825\n",
      "batch 3880: loss 0.084586\n",
      "batch 3881: loss 0.069822\n",
      "batch 3882: loss 0.020291\n",
      "batch 3883: loss 0.152228\n",
      "batch 3884: loss 0.082505\n",
      "batch 3885: loss 0.051804\n",
      "batch 3886: loss 0.055426\n",
      "batch 3887: loss 0.044722\n",
      "batch 3888: loss 0.070439\n",
      "batch 3889: loss 0.034097\n",
      "batch 3890: loss 0.014017\n",
      "batch 3891: loss 0.046226\n",
      "batch 3892: loss 0.012702\n",
      "batch 3893: loss 0.032880\n",
      "batch 3894: loss 0.003601\n",
      "batch 3895: loss 0.121476\n",
      "batch 3896: loss 0.026624\n",
      "batch 3897: loss 0.036446\n",
      "batch 3898: loss 0.008880\n",
      "batch 3899: loss 0.018998\n",
      "batch 3900: loss 0.038484\n",
      "batch 3901: loss 0.091988\n",
      "batch 3902: loss 0.041852\n",
      "batch 3903: loss 0.012013\n",
      "batch 3904: loss 0.066783\n",
      "batch 3905: loss 0.147567\n",
      "batch 3906: loss 0.119309\n",
      "batch 3907: loss 0.030523\n",
      "batch 3908: loss 0.031765\n",
      "batch 3909: loss 0.057306\n",
      "batch 3910: loss 0.042148\n",
      "batch 3911: loss 0.066057\n",
      "batch 3912: loss 0.006616\n",
      "batch 3913: loss 0.040664\n",
      "batch 3914: loss 0.146681\n",
      "batch 3915: loss 0.046256\n",
      "batch 3916: loss 0.083371\n",
      "batch 3917: loss 0.008253\n",
      "batch 3918: loss 0.015794\n",
      "batch 3919: loss 0.045423\n",
      "batch 3920: loss 0.072427\n",
      "batch 3921: loss 0.050825\n",
      "batch 3922: loss 0.111854\n",
      "batch 3923: loss 0.030945\n",
      "batch 3924: loss 0.013943\n",
      "batch 3925: loss 0.046192\n",
      "batch 3926: loss 0.127396\n",
      "batch 3927: loss 0.041199\n",
      "batch 3928: loss 0.045588\n",
      "batch 3929: loss 0.028800\n",
      "batch 3930: loss 0.225044\n",
      "batch 3931: loss 0.019023\n",
      "batch 3932: loss 0.131593\n",
      "batch 3933: loss 0.094906\n",
      "batch 3934: loss 0.214765\n",
      "batch 3935: loss 0.098182\n",
      "batch 3936: loss 0.077918\n",
      "batch 3937: loss 0.017434\n",
      "batch 3938: loss 0.039599\n",
      "batch 3939: loss 0.130924\n",
      "batch 3940: loss 0.017964\n",
      "batch 3941: loss 0.154295\n",
      "batch 3942: loss 0.191349\n",
      "batch 3943: loss 0.025982\n",
      "batch 3944: loss 0.019678\n",
      "batch 3945: loss 0.042816\n",
      "batch 3946: loss 0.034405\n",
      "batch 3947: loss 0.102245\n",
      "batch 3948: loss 0.028623\n",
      "batch 3949: loss 0.047642\n",
      "batch 3950: loss 0.034343\n",
      "batch 3951: loss 0.471212\n",
      "batch 3952: loss 0.076981\n",
      "batch 3953: loss 0.078374\n",
      "batch 3954: loss 0.017941\n",
      "batch 3955: loss 0.251411\n",
      "batch 3956: loss 0.038904\n",
      "batch 3957: loss 0.029726\n",
      "batch 3958: loss 0.056263\n",
      "batch 3959: loss 0.050987\n",
      "batch 3960: loss 0.028765\n",
      "batch 3961: loss 0.128148\n",
      "batch 3962: loss 0.128367\n",
      "batch 3963: loss 0.106512\n",
      "batch 3964: loss 0.010879\n",
      "batch 3965: loss 0.024898\n",
      "batch 3966: loss 0.072151\n",
      "batch 3967: loss 0.055200\n",
      "batch 3968: loss 0.006592\n",
      "batch 3969: loss 0.109720\n",
      "batch 3970: loss 0.241921\n",
      "batch 3971: loss 0.037569\n",
      "batch 3972: loss 0.024605\n",
      "batch 3973: loss 0.034291\n",
      "batch 3974: loss 0.042457\n",
      "batch 3975: loss 0.038890\n",
      "batch 3976: loss 0.151834\n",
      "batch 3977: loss 0.073113\n",
      "batch 3978: loss 0.089259\n",
      "batch 3979: loss 0.053548\n",
      "batch 3980: loss 0.010304\n",
      "batch 3981: loss 0.006026\n",
      "batch 3982: loss 0.077529\n",
      "batch 3983: loss 0.108566\n",
      "batch 3984: loss 0.049855\n",
      "batch 3985: loss 0.043481\n",
      "batch 3986: loss 0.137103\n",
      "batch 3987: loss 0.124951\n",
      "batch 3988: loss 0.216442\n",
      "batch 3989: loss 0.076013\n",
      "batch 3990: loss 0.031715\n",
      "batch 3991: loss 0.018647\n",
      "batch 3992: loss 0.105029\n",
      "batch 3993: loss 0.117514\n",
      "batch 3994: loss 0.086855\n",
      "batch 3995: loss 0.059071\n",
      "batch 3996: loss 0.189493\n",
      "batch 3997: loss 0.005974\n",
      "batch 3998: loss 0.014325\n",
      "batch 3999: loss 0.144381\n",
      "batch 4000: loss 0.013071\n",
      "batch 4001: loss 0.010881\n",
      "batch 4002: loss 0.009953\n",
      "batch 4003: loss 0.039715\n",
      "batch 4004: loss 0.025117\n",
      "batch 4005: loss 0.039712\n",
      "batch 4006: loss 0.046361\n",
      "batch 4007: loss 0.034279\n",
      "batch 4008: loss 0.022925\n",
      "batch 4009: loss 0.012051\n",
      "batch 4010: loss 0.093285\n",
      "batch 4011: loss 0.084575\n",
      "batch 4012: loss 0.038858\n",
      "batch 4013: loss 0.016833\n",
      "batch 4014: loss 0.012086\n",
      "batch 4015: loss 0.023476\n",
      "batch 4016: loss 0.077528\n",
      "batch 4017: loss 0.208933\n",
      "batch 4018: loss 0.021543\n",
      "batch 4019: loss 0.080364\n",
      "batch 4020: loss 0.066127\n",
      "batch 4021: loss 0.086091\n",
      "batch 4022: loss 0.047666\n",
      "batch 4023: loss 0.081332\n",
      "batch 4024: loss 0.040448\n",
      "batch 4025: loss 0.033109\n",
      "batch 4026: loss 0.051705\n",
      "batch 4027: loss 0.083103\n",
      "batch 4028: loss 0.087861\n",
      "batch 4029: loss 0.045390\n",
      "batch 4030: loss 0.042637\n",
      "batch 4031: loss 0.013720\n",
      "batch 4032: loss 0.081079\n",
      "batch 4033: loss 0.184169\n",
      "batch 4034: loss 0.014984\n",
      "batch 4035: loss 0.063368\n",
      "batch 4036: loss 0.006563\n",
      "batch 4037: loss 0.067981\n",
      "batch 4038: loss 0.035536\n",
      "batch 4039: loss 0.020952\n",
      "batch 4040: loss 0.017802\n",
      "batch 4041: loss 0.177872\n",
      "batch 4042: loss 0.088680\n",
      "batch 4043: loss 0.010239\n",
      "batch 4044: loss 0.010682\n",
      "batch 4045: loss 0.055946\n",
      "batch 4046: loss 0.004069\n",
      "batch 4047: loss 0.124238\n",
      "batch 4048: loss 0.048968\n",
      "batch 4049: loss 0.106023\n",
      "batch 4050: loss 0.005711\n",
      "batch 4051: loss 0.102176\n",
      "batch 4052: loss 0.045064\n",
      "batch 4053: loss 0.116782\n",
      "batch 4054: loss 0.095756\n",
      "batch 4055: loss 0.290735\n",
      "batch 4056: loss 0.027881\n",
      "batch 4057: loss 0.105169\n",
      "batch 4058: loss 0.099008\n",
      "batch 4059: loss 0.059529\n",
      "batch 4060: loss 0.085632\n",
      "batch 4061: loss 0.031289\n",
      "batch 4062: loss 0.120473\n",
      "batch 4063: loss 0.020430\n",
      "batch 4064: loss 0.039465\n",
      "batch 4065: loss 0.148405\n",
      "batch 4066: loss 0.047275\n",
      "batch 4067: loss 0.041610\n",
      "batch 4068: loss 0.039123\n",
      "batch 4069: loss 0.065803\n",
      "batch 4070: loss 0.210810\n",
      "batch 4071: loss 0.027782\n",
      "batch 4072: loss 0.122109\n",
      "batch 4073: loss 0.039216\n",
      "batch 4074: loss 0.051112\n",
      "batch 4075: loss 0.048483\n",
      "batch 4076: loss 0.051626\n",
      "batch 4077: loss 0.007211\n",
      "batch 4078: loss 0.042251\n",
      "batch 4079: loss 0.017714\n",
      "batch 4080: loss 0.017033\n",
      "batch 4081: loss 0.038041\n",
      "batch 4082: loss 0.065422\n",
      "batch 4083: loss 0.085894\n",
      "batch 4084: loss 0.039021\n",
      "batch 4085: loss 0.281175\n",
      "batch 4086: loss 0.014810\n",
      "batch 4087: loss 0.050950\n",
      "batch 4088: loss 0.078673\n",
      "batch 4089: loss 0.117072\n",
      "batch 4090: loss 0.172712\n",
      "batch 4091: loss 0.047682\n",
      "batch 4092: loss 0.027022\n",
      "batch 4093: loss 0.108617\n",
      "batch 4094: loss 0.052249\n",
      "batch 4095: loss 0.122865\n",
      "batch 4096: loss 0.037179\n",
      "batch 4097: loss 0.025976\n",
      "batch 4098: loss 0.173858\n",
      "batch 4099: loss 0.140026\n",
      "batch 4100: loss 0.102607\n",
      "batch 4101: loss 0.338516\n",
      "batch 4102: loss 0.098622\n",
      "batch 4103: loss 0.071268\n",
      "batch 4104: loss 0.050149\n",
      "batch 4105: loss 0.037972\n",
      "batch 4106: loss 0.068256\n",
      "batch 4107: loss 0.088638\n",
      "batch 4108: loss 0.100091\n",
      "batch 4109: loss 0.103345\n",
      "batch 4110: loss 0.178140\n",
      "batch 4111: loss 0.108237\n",
      "batch 4112: loss 0.018543\n",
      "batch 4113: loss 0.006553\n",
      "batch 4114: loss 0.071997\n",
      "batch 4115: loss 0.007792\n",
      "batch 4116: loss 0.014467\n",
      "batch 4117: loss 0.018795\n",
      "batch 4118: loss 0.140398\n",
      "batch 4119: loss 0.066366\n",
      "batch 4120: loss 0.009852\n",
      "batch 4121: loss 0.134633\n",
      "batch 4122: loss 0.152274\n",
      "batch 4123: loss 0.033688\n",
      "batch 4124: loss 0.078627\n",
      "batch 4125: loss 0.019394\n",
      "batch 4126: loss 0.028892\n",
      "batch 4127: loss 0.026375\n",
      "batch 4128: loss 0.076798\n",
      "batch 4129: loss 0.041072\n",
      "batch 4130: loss 0.076922\n",
      "batch 4131: loss 0.151845\n",
      "batch 4132: loss 0.046119\n",
      "batch 4133: loss 0.024407\n",
      "batch 4134: loss 0.195929\n",
      "batch 4135: loss 0.013322\n",
      "batch 4136: loss 0.222683\n",
      "batch 4137: loss 0.011919\n",
      "batch 4138: loss 0.023021\n",
      "batch 4139: loss 0.111288\n",
      "batch 4140: loss 0.042964\n",
      "batch 4141: loss 0.166871\n",
      "batch 4142: loss 0.061199\n",
      "batch 4143: loss 0.037601\n",
      "batch 4144: loss 0.005597\n",
      "batch 4145: loss 0.055726\n",
      "batch 4146: loss 0.267404\n",
      "batch 4147: loss 0.011842\n",
      "batch 4148: loss 0.145593\n",
      "batch 4149: loss 0.071957\n",
      "batch 4150: loss 0.084729\n",
      "batch 4151: loss 0.042981\n",
      "batch 4152: loss 0.013579\n",
      "batch 4153: loss 0.039620\n",
      "batch 4154: loss 0.126494\n",
      "batch 4155: loss 0.040753\n",
      "batch 4156: loss 0.020147\n",
      "batch 4157: loss 0.109623\n",
      "batch 4158: loss 0.065635\n",
      "batch 4159: loss 0.147882\n",
      "batch 4160: loss 0.076136\n",
      "batch 4161: loss 0.133298\n",
      "batch 4162: loss 0.042740\n",
      "batch 4163: loss 0.024533\n",
      "batch 4164: loss 0.079059\n",
      "batch 4165: loss 0.061456\n",
      "batch 4166: loss 0.014254\n",
      "batch 4167: loss 0.100911\n",
      "batch 4168: loss 0.040342\n",
      "batch 4169: loss 0.055942\n",
      "batch 4170: loss 0.022352\n",
      "batch 4171: loss 0.036229\n",
      "batch 4172: loss 0.086379\n",
      "batch 4173: loss 0.015518\n",
      "batch 4174: loss 0.018639\n",
      "batch 4175: loss 0.259649\n",
      "batch 4176: loss 0.044835\n",
      "batch 4177: loss 0.079746\n",
      "batch 4178: loss 0.051224\n",
      "batch 4179: loss 0.072554\n",
      "batch 4180: loss 0.015084\n",
      "batch 4181: loss 0.088326\n",
      "batch 4182: loss 0.116404\n",
      "batch 4183: loss 0.152049\n",
      "batch 4184: loss 0.031700\n",
      "batch 4185: loss 0.036815\n",
      "batch 4186: loss 0.024484\n",
      "batch 4187: loss 0.016045\n",
      "batch 4188: loss 0.023779\n",
      "batch 4189: loss 0.044281\n",
      "batch 4190: loss 0.072657\n",
      "batch 4191: loss 0.052615\n",
      "batch 4192: loss 0.155045\n",
      "batch 4193: loss 0.033959\n",
      "batch 4194: loss 0.072475\n",
      "batch 4195: loss 0.009649\n",
      "batch 4196: loss 0.047158\n",
      "batch 4197: loss 0.054599\n",
      "batch 4198: loss 0.176936\n",
      "batch 4199: loss 0.053884\n",
      "batch 4200: loss 0.035354\n",
      "batch 4201: loss 0.072971\n",
      "batch 4202: loss 0.049914\n",
      "batch 4203: loss 0.021178\n",
      "batch 4204: loss 0.019028\n",
      "batch 4205: loss 0.090892\n",
      "batch 4206: loss 0.017534\n",
      "batch 4207: loss 0.047541\n",
      "batch 4208: loss 0.030062\n",
      "batch 4209: loss 0.075576\n",
      "batch 4210: loss 0.028202\n",
      "batch 4211: loss 0.023818\n",
      "batch 4212: loss 0.053232\n",
      "batch 4213: loss 0.047438\n",
      "batch 4214: loss 0.127641\n",
      "batch 4215: loss 0.034488\n",
      "batch 4216: loss 0.045342\n",
      "batch 4217: loss 0.066299\n",
      "batch 4218: loss 0.042217\n",
      "batch 4219: loss 0.049010\n",
      "batch 4220: loss 0.057104\n",
      "batch 4221: loss 0.053600\n",
      "batch 4222: loss 0.056660\n",
      "batch 4223: loss 0.091347\n",
      "batch 4224: loss 0.067036\n",
      "batch 4225: loss 0.156383\n",
      "batch 4226: loss 0.039405\n",
      "batch 4227: loss 0.066476\n",
      "batch 4228: loss 0.048661\n",
      "batch 4229: loss 0.022194\n",
      "batch 4230: loss 0.017123\n",
      "batch 4231: loss 0.174836\n",
      "batch 4232: loss 0.025022\n",
      "batch 4233: loss 0.109997\n",
      "batch 4234: loss 0.014380\n",
      "batch 4235: loss 0.045669\n",
      "batch 4236: loss 0.028615\n",
      "batch 4237: loss 0.016418\n",
      "batch 4238: loss 0.039528\n",
      "batch 4239: loss 0.172427\n",
      "batch 4240: loss 0.140887\n",
      "batch 4241: loss 0.047014\n",
      "batch 4242: loss 0.115541\n",
      "batch 4243: loss 0.032295\n",
      "batch 4244: loss 0.095841\n",
      "batch 4245: loss 0.052700\n",
      "batch 4246: loss 0.074465\n",
      "batch 4247: loss 0.077809\n",
      "batch 4248: loss 0.076307\n",
      "batch 4249: loss 0.052479\n",
      "batch 4250: loss 0.067087\n",
      "batch 4251: loss 0.026867\n",
      "batch 4252: loss 0.055061\n",
      "batch 4253: loss 0.124697\n",
      "batch 4254: loss 0.011244\n",
      "batch 4255: loss 0.028630\n",
      "batch 4256: loss 0.130704\n",
      "batch 4257: loss 0.326946\n",
      "batch 4258: loss 0.020704\n",
      "batch 4259: loss 0.099846\n",
      "batch 4260: loss 0.025228\n",
      "batch 4261: loss 0.009762\n",
      "batch 4262: loss 0.092640\n",
      "batch 4263: loss 0.030712\n",
      "batch 4264: loss 0.022452\n",
      "batch 4265: loss 0.161271\n",
      "batch 4266: loss 0.011715\n",
      "batch 4267: loss 0.010359\n",
      "batch 4268: loss 0.081976\n",
      "batch 4269: loss 0.017746\n",
      "batch 4270: loss 0.031072\n",
      "batch 4271: loss 0.019957\n",
      "batch 4272: loss 0.025717\n",
      "batch 4273: loss 0.048494\n",
      "batch 4274: loss 0.015647\n",
      "batch 4275: loss 0.024482\n",
      "batch 4276: loss 0.148803\n",
      "batch 4277: loss 0.146213\n",
      "batch 4278: loss 0.069707\n",
      "batch 4279: loss 0.100997\n",
      "batch 4280: loss 0.126665\n",
      "batch 4281: loss 0.036049\n",
      "batch 4282: loss 0.136425\n",
      "batch 4283: loss 0.088675\n",
      "batch 4284: loss 0.092807\n",
      "batch 4285: loss 0.014500\n",
      "batch 4286: loss 0.085078\n",
      "batch 4287: loss 0.009271\n",
      "batch 4288: loss 0.017057\n",
      "batch 4289: loss 0.073103\n",
      "batch 4290: loss 0.162934\n",
      "batch 4291: loss 0.007399\n",
      "batch 4292: loss 0.042334\n",
      "batch 4293: loss 0.072045\n",
      "batch 4294: loss 0.053405\n",
      "batch 4295: loss 0.135201\n",
      "batch 4296: loss 0.062004\n",
      "batch 4297: loss 0.032659\n",
      "batch 4298: loss 0.022985\n",
      "batch 4299: loss 0.024397\n",
      "batch 4300: loss 0.037033\n",
      "batch 4301: loss 0.101803\n",
      "batch 4302: loss 0.053091\n",
      "batch 4303: loss 0.100315\n",
      "batch 4304: loss 0.071012\n",
      "batch 4305: loss 0.039914\n",
      "batch 4306: loss 0.122111\n",
      "batch 4307: loss 0.082427\n",
      "batch 4308: loss 0.039419\n",
      "batch 4309: loss 0.067885\n",
      "batch 4310: loss 0.062286\n",
      "batch 4311: loss 0.015561\n",
      "batch 4312: loss 0.154931\n",
      "batch 4313: loss 0.226949\n",
      "batch 4314: loss 0.007585\n",
      "batch 4315: loss 0.074466\n",
      "batch 4316: loss 0.014653\n",
      "batch 4317: loss 0.021309\n",
      "batch 4318: loss 0.018930\n",
      "batch 4319: loss 0.016889\n",
      "batch 4320: loss 0.207104\n",
      "batch 4321: loss 0.101394\n",
      "batch 4322: loss 0.062323\n",
      "batch 4323: loss 0.185568\n",
      "batch 4324: loss 0.068966\n",
      "batch 4325: loss 0.116727\n",
      "batch 4326: loss 0.087262\n",
      "batch 4327: loss 0.030827\n",
      "batch 4328: loss 0.035562\n",
      "batch 4329: loss 0.030480\n",
      "batch 4330: loss 0.060868\n",
      "batch 4331: loss 0.009262\n",
      "batch 4332: loss 0.109607\n",
      "batch 4333: loss 0.409382\n",
      "batch 4334: loss 0.043256\n",
      "batch 4335: loss 0.044212\n",
      "batch 4336: loss 0.075776\n",
      "batch 4337: loss 0.025523\n",
      "batch 4338: loss 0.410725\n",
      "batch 4339: loss 0.081926\n",
      "batch 4340: loss 0.042268\n",
      "batch 4341: loss 0.225104\n",
      "batch 4342: loss 0.030328\n",
      "batch 4343: loss 0.019539\n",
      "batch 4344: loss 0.066520\n",
      "batch 4345: loss 0.082649\n",
      "batch 4346: loss 0.013680\n",
      "batch 4347: loss 0.052501\n",
      "batch 4348: loss 0.133678\n",
      "batch 4349: loss 0.098541\n",
      "batch 4350: loss 0.058639\n",
      "batch 4351: loss 0.018571\n",
      "batch 4352: loss 0.020689\n",
      "batch 4353: loss 0.130191\n",
      "batch 4354: loss 0.063411\n",
      "batch 4355: loss 0.067804\n",
      "batch 4356: loss 0.066039\n",
      "batch 4357: loss 0.074038\n",
      "batch 4358: loss 0.026004\n",
      "batch 4359: loss 0.227641\n",
      "batch 4360: loss 0.093407\n",
      "batch 4361: loss 0.116074\n",
      "batch 4362: loss 0.054777\n",
      "batch 4363: loss 0.120554\n",
      "batch 4364: loss 0.103761\n",
      "batch 4365: loss 0.098242\n",
      "batch 4366: loss 0.059078\n",
      "batch 4367: loss 0.127009\n",
      "batch 4368: loss 0.093124\n",
      "batch 4369: loss 0.047316\n",
      "batch 4370: loss 0.156427\n",
      "batch 4371: loss 0.039018\n",
      "batch 4372: loss 0.095712\n",
      "batch 4373: loss 0.008767\n",
      "batch 4374: loss 0.047214\n",
      "batch 4375: loss 0.130632\n",
      "batch 4376: loss 0.046942\n",
      "batch 4377: loss 0.127440\n",
      "batch 4378: loss 0.028096\n",
      "batch 4379: loss 0.127393\n",
      "batch 4380: loss 0.186350\n",
      "batch 4381: loss 0.023125\n",
      "batch 4382: loss 0.032255\n",
      "batch 4383: loss 0.165249\n",
      "batch 4384: loss 0.207020\n",
      "batch 4385: loss 0.100393\n",
      "batch 4386: loss 0.051251\n",
      "batch 4387: loss 0.229100\n",
      "batch 4388: loss 0.052558\n",
      "batch 4389: loss 0.005376\n",
      "batch 4390: loss 0.047432\n",
      "batch 4391: loss 0.220237\n",
      "batch 4392: loss 0.103887\n",
      "batch 4393: loss 0.031093\n",
      "batch 4394: loss 0.013313\n",
      "batch 4395: loss 0.032002\n",
      "batch 4396: loss 0.035460\n",
      "batch 4397: loss 0.016527\n",
      "batch 4398: loss 0.044043\n",
      "batch 4399: loss 0.012463\n",
      "batch 4400: loss 0.039184\n",
      "batch 4401: loss 0.023986\n",
      "batch 4402: loss 0.052706\n",
      "batch 4403: loss 0.046735\n",
      "batch 4404: loss 0.020623\n",
      "batch 4405: loss 0.036669\n",
      "batch 4406: loss 0.053029\n",
      "batch 4407: loss 0.044007\n",
      "batch 4408: loss 0.045255\n",
      "batch 4409: loss 0.060957\n",
      "batch 4410: loss 0.015802\n",
      "batch 4411: loss 0.116656\n",
      "batch 4412: loss 0.069748\n",
      "batch 4413: loss 0.014301\n",
      "batch 4414: loss 0.077694\n",
      "batch 4415: loss 0.013722\n",
      "batch 4416: loss 0.041593\n",
      "batch 4417: loss 0.030388\n",
      "batch 4418: loss 0.015673\n",
      "batch 4419: loss 0.011825\n",
      "batch 4420: loss 0.196007\n",
      "batch 4421: loss 0.025844\n",
      "batch 4422: loss 0.088408\n",
      "batch 4423: loss 0.086420\n",
      "batch 4424: loss 0.036953\n",
      "batch 4425: loss 0.023367\n",
      "batch 4426: loss 0.010256\n",
      "batch 4427: loss 0.047347\n",
      "batch 4428: loss 0.021385\n",
      "batch 4429: loss 0.021386\n",
      "batch 4430: loss 0.089152\n",
      "batch 4431: loss 0.013351\n",
      "batch 4432: loss 0.015974\n",
      "batch 4433: loss 0.034748\n",
      "batch 4434: loss 0.038671\n",
      "batch 4435: loss 0.258196\n",
      "batch 4436: loss 0.058965\n",
      "batch 4437: loss 0.019181\n",
      "batch 4438: loss 0.013552\n",
      "batch 4439: loss 0.079643\n",
      "batch 4440: loss 0.078518\n",
      "batch 4441: loss 0.020337\n",
      "batch 4442: loss 0.070747\n",
      "batch 4443: loss 0.017603\n",
      "batch 4444: loss 0.166441\n",
      "batch 4445: loss 0.269714\n",
      "batch 4446: loss 0.032951\n",
      "batch 4447: loss 0.008563\n",
      "batch 4448: loss 0.030177\n",
      "batch 4449: loss 0.036065\n",
      "batch 4450: loss 0.024175\n",
      "batch 4451: loss 0.062343\n",
      "batch 4452: loss 0.090479\n",
      "batch 4453: loss 0.007898\n",
      "batch 4454: loss 0.056093\n",
      "batch 4455: loss 0.150451\n",
      "batch 4456: loss 0.084906\n",
      "batch 4457: loss 0.045120\n",
      "batch 4458: loss 0.045370\n",
      "batch 4459: loss 0.126912\n",
      "batch 4460: loss 0.060159\n",
      "batch 4461: loss 0.113359\n",
      "batch 4462: loss 0.024876\n",
      "batch 4463: loss 0.063684\n",
      "batch 4464: loss 0.029054\n",
      "batch 4465: loss 0.036777\n",
      "batch 4466: loss 0.103418\n",
      "batch 4467: loss 0.117885\n",
      "batch 4468: loss 0.077867\n",
      "batch 4469: loss 0.021143\n",
      "batch 4470: loss 0.041003\n",
      "batch 4471: loss 0.034413\n",
      "batch 4472: loss 0.145745\n",
      "batch 4473: loss 0.043195\n",
      "batch 4474: loss 0.059369\n",
      "batch 4475: loss 0.222513\n",
      "batch 4476: loss 0.033951\n",
      "batch 4477: loss 0.062181\n",
      "batch 4478: loss 0.156014\n",
      "batch 4479: loss 0.036174\n",
      "batch 4480: loss 0.042172\n",
      "batch 4481: loss 0.010111\n",
      "batch 4482: loss 0.081735\n",
      "batch 4483: loss 0.039248\n",
      "batch 4484: loss 0.073967\n",
      "batch 4485: loss 0.042226\n",
      "batch 4486: loss 0.021538\n",
      "batch 4487: loss 0.044186\n",
      "batch 4488: loss 0.164326\n",
      "batch 4489: loss 0.013181\n",
      "batch 4490: loss 0.096272\n",
      "batch 4491: loss 0.082156\n",
      "batch 4492: loss 0.015947\n",
      "batch 4493: loss 0.077555\n",
      "batch 4494: loss 0.033637\n",
      "batch 4495: loss 0.088314\n",
      "batch 4496: loss 0.394844\n",
      "batch 4497: loss 0.181768\n",
      "batch 4498: loss 0.081952\n",
      "batch 4499: loss 0.057672\n",
      "batch 4500: loss 0.110093\n",
      "batch 4501: loss 0.097517\n",
      "batch 4502: loss 0.009374\n",
      "batch 4503: loss 0.033583\n",
      "batch 4504: loss 0.055753\n",
      "batch 4505: loss 0.149729\n",
      "batch 4506: loss 0.038321\n",
      "batch 4507: loss 0.049466\n",
      "batch 4508: loss 0.007131\n",
      "batch 4509: loss 0.379220\n",
      "batch 4510: loss 0.043560\n",
      "batch 4511: loss 0.091272\n",
      "batch 4512: loss 0.023399\n",
      "batch 4513: loss 0.125057\n",
      "batch 4514: loss 0.020962\n",
      "batch 4515: loss 0.017969\n",
      "batch 4516: loss 0.008340\n",
      "batch 4517: loss 0.127150\n",
      "batch 4518: loss 0.050226\n",
      "batch 4519: loss 0.097188\n",
      "batch 4520: loss 0.011371\n",
      "batch 4521: loss 0.299928\n",
      "batch 4522: loss 0.018936\n",
      "batch 4523: loss 0.096456\n",
      "batch 4524: loss 0.069369\n",
      "batch 4525: loss 0.017393\n",
      "batch 4526: loss 0.032664\n",
      "batch 4527: loss 0.116514\n",
      "batch 4528: loss 0.033412\n",
      "batch 4529: loss 0.025342\n",
      "batch 4530: loss 0.014800\n",
      "batch 4531: loss 0.079503\n",
      "batch 4532: loss 0.025467\n",
      "batch 4533: loss 0.060241\n",
      "batch 4534: loss 0.022272\n",
      "batch 4535: loss 0.198909\n",
      "batch 4536: loss 0.020978\n",
      "batch 4537: loss 0.015279\n",
      "batch 4538: loss 0.064147\n",
      "batch 4539: loss 0.010824\n",
      "batch 4540: loss 0.026107\n",
      "batch 4541: loss 0.045845\n",
      "batch 4542: loss 0.015021\n",
      "batch 4543: loss 0.044601\n",
      "batch 4544: loss 0.021166\n",
      "batch 4545: loss 0.032248\n",
      "batch 4546: loss 0.034085\n",
      "batch 4547: loss 0.021159\n",
      "batch 4548: loss 0.318680\n",
      "batch 4549: loss 0.185048\n",
      "batch 4550: loss 0.045203\n",
      "batch 4551: loss 0.009381\n",
      "batch 4552: loss 0.041596\n",
      "batch 4553: loss 0.051581\n",
      "batch 4554: loss 0.128764\n",
      "batch 4555: loss 0.028749\n",
      "batch 4556: loss 0.090650\n",
      "batch 4557: loss 0.108230\n",
      "batch 4558: loss 0.066881\n",
      "batch 4559: loss 0.098274\n",
      "batch 4560: loss 0.090894\n",
      "batch 4561: loss 0.009261\n",
      "batch 4562: loss 0.078383\n",
      "batch 4563: loss 0.044506\n",
      "batch 4564: loss 0.019443\n",
      "batch 4565: loss 0.042462\n",
      "batch 4566: loss 0.040059\n",
      "batch 4567: loss 0.018895\n",
      "batch 4568: loss 0.174000\n",
      "batch 4569: loss 0.041269\n",
      "batch 4570: loss 0.087381\n",
      "batch 4571: loss 0.052889\n",
      "batch 4572: loss 0.120838\n",
      "batch 4573: loss 0.050667\n",
      "batch 4574: loss 0.035811\n",
      "batch 4575: loss 0.018481\n",
      "batch 4576: loss 0.015312\n",
      "batch 4577: loss 0.058806\n",
      "batch 4578: loss 0.214386\n",
      "batch 4579: loss 0.024249\n",
      "batch 4580: loss 0.101310\n",
      "batch 4581: loss 0.089674\n",
      "batch 4582: loss 0.112578\n",
      "batch 4583: loss 0.083887\n",
      "batch 4584: loss 0.043223\n",
      "batch 4585: loss 0.034012\n",
      "batch 4586: loss 0.035443\n",
      "batch 4587: loss 0.015590\n",
      "batch 4588: loss 0.208823\n",
      "batch 4589: loss 0.031009\n",
      "batch 4590: loss 0.005991\n",
      "batch 4591: loss 0.042691\n",
      "batch 4592: loss 0.175931\n",
      "batch 4593: loss 0.058504\n",
      "batch 4594: loss 0.056308\n",
      "batch 4595: loss 0.076317\n",
      "batch 4596: loss 0.104651\n",
      "batch 4597: loss 0.076246\n",
      "batch 4598: loss 0.191959\n",
      "batch 4599: loss 0.036035\n",
      "batch 4600: loss 0.111696\n",
      "batch 4601: loss 0.058037\n",
      "batch 4602: loss 0.060404\n",
      "batch 4603: loss 0.083601\n",
      "batch 4604: loss 0.081183\n",
      "batch 4605: loss 0.012167\n",
      "batch 4606: loss 0.024910\n",
      "batch 4607: loss 0.015495\n",
      "batch 4608: loss 0.030764\n",
      "batch 4609: loss 0.086793\n",
      "batch 4610: loss 0.122775\n",
      "batch 4611: loss 0.085012\n",
      "batch 4612: loss 0.180428\n",
      "batch 4613: loss 0.050047\n",
      "batch 4614: loss 0.030412\n",
      "batch 4615: loss 0.066232\n",
      "batch 4616: loss 0.031955\n",
      "batch 4617: loss 0.010335\n",
      "batch 4618: loss 0.111014\n",
      "batch 4619: loss 0.072341\n",
      "batch 4620: loss 0.082913\n",
      "batch 4621: loss 0.073199\n",
      "batch 4622: loss 0.048931\n",
      "batch 4623: loss 0.048560\n",
      "batch 4624: loss 0.120416\n",
      "batch 4625: loss 0.026639\n",
      "batch 4626: loss 0.018770\n",
      "batch 4627: loss 0.028931\n",
      "batch 4628: loss 0.094928\n",
      "batch 4629: loss 0.134641\n",
      "batch 4630: loss 0.087796\n",
      "batch 4631: loss 0.007245\n",
      "batch 4632: loss 0.009036\n",
      "batch 4633: loss 0.013389\n",
      "batch 4634: loss 0.107345\n",
      "batch 4635: loss 0.053905\n",
      "batch 4636: loss 0.037141\n",
      "batch 4637: loss 0.012915\n",
      "batch 4638: loss 0.133233\n",
      "batch 4639: loss 0.043116\n",
      "batch 4640: loss 0.069614\n",
      "batch 4641: loss 0.041807\n",
      "batch 4642: loss 0.148056\n",
      "batch 4643: loss 0.245430\n",
      "batch 4644: loss 0.020758\n",
      "batch 4645: loss 0.051552\n",
      "batch 4646: loss 0.036693\n",
      "batch 4647: loss 0.053803\n",
      "batch 4648: loss 0.038926\n",
      "batch 4649: loss 0.011888\n",
      "batch 4650: loss 0.249052\n",
      "batch 4651: loss 0.075677\n",
      "batch 4652: loss 0.007577\n",
      "batch 4653: loss 0.023068\n",
      "batch 4654: loss 0.065334\n",
      "batch 4655: loss 0.011584\n",
      "batch 4656: loss 0.225304\n",
      "batch 4657: loss 0.045464\n",
      "batch 4658: loss 0.141890\n",
      "batch 4659: loss 0.030657\n",
      "batch 4660: loss 0.015441\n",
      "batch 4661: loss 0.100579\n",
      "batch 4662: loss 0.069816\n",
      "batch 4663: loss 0.072020\n",
      "batch 4664: loss 0.028454\n",
      "batch 4665: loss 0.121200\n",
      "batch 4666: loss 0.024695\n",
      "batch 4667: loss 0.124697\n",
      "batch 4668: loss 0.064386\n",
      "batch 4669: loss 0.038401\n",
      "batch 4670: loss 0.115359\n",
      "batch 4671: loss 0.131443\n",
      "batch 4672: loss 0.084298\n",
      "batch 4673: loss 0.101181\n",
      "batch 4674: loss 0.058235\n",
      "batch 4675: loss 0.026836\n",
      "batch 4676: loss 0.083475\n",
      "batch 4677: loss 0.052812\n",
      "batch 4678: loss 0.164348\n",
      "batch 4679: loss 0.075040\n",
      "batch 4680: loss 0.051791\n",
      "batch 4681: loss 0.091667\n",
      "batch 4682: loss 0.007900\n",
      "batch 4683: loss 0.099716\n",
      "batch 4684: loss 0.033069\n",
      "batch 4685: loss 0.113153\n",
      "batch 4686: loss 0.205722\n",
      "batch 4687: loss 0.019119\n",
      "batch 4688: loss 0.117596\n",
      "batch 4689: loss 0.032944\n",
      "batch 4690: loss 0.123443\n",
      "batch 4691: loss 0.082740\n",
      "batch 4692: loss 0.182200\n",
      "batch 4693: loss 0.080347\n",
      "batch 4694: loss 0.061559\n",
      "batch 4695: loss 0.031866\n",
      "batch 4696: loss 0.019913\n",
      "batch 4697: loss 0.030747\n",
      "batch 4698: loss 0.032448\n",
      "batch 4699: loss 0.080948\n",
      "batch 4700: loss 0.077458\n",
      "batch 4701: loss 0.018342\n",
      "batch 4702: loss 0.153028\n",
      "batch 4703: loss 0.005795\n",
      "batch 4704: loss 0.018146\n",
      "batch 4705: loss 0.098216\n",
      "batch 4706: loss 0.263193\n",
      "batch 4707: loss 0.012054\n",
      "batch 4708: loss 0.012195\n",
      "batch 4709: loss 0.027461\n",
      "batch 4710: loss 0.143041\n",
      "batch 4711: loss 0.033786\n",
      "batch 4712: loss 0.098547\n",
      "batch 4713: loss 0.109017\n",
      "batch 4714: loss 0.016829\n",
      "batch 4715: loss 0.021688\n",
      "batch 4716: loss 0.037873\n",
      "batch 4717: loss 0.115651\n",
      "batch 4718: loss 0.081488\n",
      "batch 4719: loss 0.060311\n",
      "batch 4720: loss 0.038385\n",
      "batch 4721: loss 0.031459\n",
      "batch 4722: loss 0.030181\n",
      "batch 4723: loss 0.164113\n",
      "batch 4724: loss 0.030685\n",
      "batch 4725: loss 0.040692\n",
      "batch 4726: loss 0.093400\n",
      "batch 4727: loss 0.079912\n",
      "batch 4728: loss 0.094622\n",
      "batch 4729: loss 0.028631\n",
      "batch 4730: loss 0.143515\n",
      "batch 4731: loss 0.103369\n",
      "batch 4732: loss 0.055728\n",
      "batch 4733: loss 0.031376\n",
      "batch 4734: loss 0.083258\n",
      "batch 4735: loss 0.038872\n",
      "batch 4736: loss 0.034451\n",
      "batch 4737: loss 0.038103\n",
      "batch 4738: loss 0.012468\n",
      "batch 4739: loss 0.021967\n",
      "batch 4740: loss 0.071340\n",
      "batch 4741: loss 0.105823\n",
      "batch 4742: loss 0.046053\n",
      "batch 4743: loss 0.114708\n",
      "batch 4744: loss 0.013188\n",
      "batch 4745: loss 0.154613\n",
      "batch 4746: loss 0.057293\n",
      "batch 4747: loss 0.040825\n",
      "batch 4748: loss 0.211996\n",
      "batch 4749: loss 0.040304\n",
      "batch 4750: loss 0.004485\n",
      "batch 4751: loss 0.043137\n",
      "batch 4752: loss 0.010235\n",
      "batch 4753: loss 0.065391\n",
      "batch 4754: loss 0.029039\n",
      "batch 4755: loss 0.123811\n",
      "batch 4756: loss 0.015781\n",
      "batch 4757: loss 0.014536\n",
      "batch 4758: loss 0.036444\n",
      "batch 4759: loss 0.111021\n",
      "batch 4760: loss 0.025535\n",
      "batch 4761: loss 0.006946\n",
      "batch 4762: loss 0.079491\n",
      "batch 4763: loss 0.033409\n",
      "batch 4764: loss 0.009452\n",
      "batch 4765: loss 0.019234\n",
      "batch 4766: loss 0.213025\n",
      "batch 4767: loss 0.011513\n",
      "batch 4768: loss 0.029335\n",
      "batch 4769: loss 0.191903\n",
      "batch 4770: loss 0.046849\n",
      "batch 4771: loss 0.173264\n",
      "batch 4772: loss 0.092100\n",
      "batch 4773: loss 0.123021\n",
      "batch 4774: loss 0.024170\n",
      "batch 4775: loss 0.020925\n",
      "batch 4776: loss 0.007230\n",
      "batch 4777: loss 0.009822\n",
      "batch 4778: loss 0.025923\n",
      "batch 4779: loss 0.013372\n",
      "batch 4780: loss 0.020706\n",
      "batch 4781: loss 0.058644\n",
      "batch 4782: loss 0.062916\n",
      "batch 4783: loss 0.044807\n",
      "batch 4784: loss 0.065079\n",
      "batch 4785: loss 0.170472\n",
      "batch 4786: loss 0.044833\n",
      "batch 4787: loss 0.009480\n",
      "batch 4788: loss 0.033222\n",
      "batch 4789: loss 0.253244\n",
      "batch 4790: loss 0.018504\n",
      "batch 4791: loss 0.012123\n",
      "batch 4792: loss 0.109105\n",
      "batch 4793: loss 0.007815\n",
      "batch 4794: loss 0.016683\n",
      "batch 4795: loss 0.087139\n",
      "batch 4796: loss 0.065657\n",
      "batch 4797: loss 0.027064\n",
      "batch 4798: loss 0.031865\n",
      "batch 4799: loss 0.024816\n",
      "batch 4800: loss 0.117953\n",
      "batch 4801: loss 0.004594\n",
      "batch 4802: loss 0.200693\n",
      "batch 4803: loss 0.066190\n",
      "batch 4804: loss 0.115007\n",
      "batch 4805: loss 0.075022\n",
      "batch 4806: loss 0.018476\n",
      "batch 4807: loss 0.002087\n",
      "batch 4808: loss 0.054064\n",
      "batch 4809: loss 0.170278\n",
      "batch 4810: loss 0.028458\n",
      "batch 4811: loss 0.137230\n",
      "batch 4812: loss 0.239985\n",
      "batch 4813: loss 0.090600\n",
      "batch 4814: loss 0.032460\n",
      "batch 4815: loss 0.031322\n",
      "batch 4816: loss 0.005146\n",
      "batch 4817: loss 0.017020\n",
      "batch 4818: loss 0.093767\n",
      "batch 4819: loss 0.008287\n",
      "batch 4820: loss 0.017898\n",
      "batch 4821: loss 0.089371\n",
      "batch 4822: loss 0.020091\n",
      "batch 4823: loss 0.153380\n",
      "batch 4824: loss 0.010224\n",
      "batch 4825: loss 0.276014\n",
      "batch 4826: loss 0.009623\n",
      "batch 4827: loss 0.045233\n",
      "batch 4828: loss 0.064651\n",
      "batch 4829: loss 0.053108\n",
      "batch 4830: loss 0.228558\n",
      "batch 4831: loss 0.193141\n",
      "batch 4832: loss 0.054395\n",
      "batch 4833: loss 0.066041\n",
      "batch 4834: loss 0.030707\n",
      "batch 4835: loss 0.034561\n",
      "batch 4836: loss 0.010599\n",
      "batch 4837: loss 0.117987\n",
      "batch 4838: loss 0.061457\n",
      "batch 4839: loss 0.030542\n",
      "batch 4840: loss 0.070280\n",
      "batch 4841: loss 0.100800\n",
      "batch 4842: loss 0.037854\n",
      "batch 4843: loss 0.035974\n",
      "batch 4844: loss 0.077537\n",
      "batch 4845: loss 0.125585\n",
      "batch 4846: loss 0.006591\n",
      "batch 4847: loss 0.096345\n",
      "batch 4848: loss 0.031010\n",
      "batch 4849: loss 0.023558\n",
      "batch 4850: loss 0.019892\n",
      "batch 4851: loss 0.019865\n",
      "batch 4852: loss 0.012589\n",
      "batch 4853: loss 0.036795\n",
      "batch 4854: loss 0.012751\n",
      "batch 4855: loss 0.069686\n",
      "batch 4856: loss 0.036982\n",
      "batch 4857: loss 0.168059\n",
      "batch 4858: loss 0.039351\n",
      "batch 4859: loss 0.079138\n",
      "batch 4860: loss 0.019036\n",
      "batch 4861: loss 0.037422\n",
      "batch 4862: loss 0.178492\n",
      "batch 4863: loss 0.075342\n",
      "batch 4864: loss 0.018183\n",
      "batch 4865: loss 0.012798\n",
      "batch 4866: loss 0.014067\n",
      "batch 4867: loss 0.031132\n",
      "batch 4868: loss 0.026965\n",
      "batch 4869: loss 0.078885\n",
      "batch 4870: loss 0.068329\n",
      "batch 4871: loss 0.058294\n",
      "batch 4872: loss 0.080261\n",
      "batch 4873: loss 0.143004\n",
      "batch 4874: loss 0.012030\n",
      "batch 4875: loss 0.111102\n",
      "batch 4876: loss 0.473549\n",
      "batch 4877: loss 0.071506\n",
      "batch 4878: loss 0.028967\n",
      "batch 4879: loss 0.076415\n",
      "batch 4880: loss 0.061467\n",
      "batch 4881: loss 0.012999\n",
      "batch 4882: loss 0.076594\n",
      "batch 4883: loss 0.051238\n",
      "batch 4884: loss 0.245024\n",
      "batch 4885: loss 0.016792\n",
      "batch 4886: loss 0.017166\n",
      "batch 4887: loss 0.035320\n",
      "batch 4888: loss 0.040603\n",
      "batch 4889: loss 0.057135\n",
      "batch 4890: loss 0.116974\n",
      "batch 4891: loss 0.015785\n",
      "batch 4892: loss 0.006640\n",
      "batch 4893: loss 0.125487\n",
      "batch 4894: loss 0.043627\n",
      "batch 4895: loss 0.020610\n",
      "batch 4896: loss 0.019261\n",
      "batch 4897: loss 0.063396\n",
      "batch 4898: loss 0.017224\n",
      "batch 4899: loss 0.043778\n",
      "batch 4900: loss 0.012314\n",
      "batch 4901: loss 0.019530\n",
      "batch 4902: loss 0.138118\n",
      "batch 4903: loss 0.020715\n",
      "batch 4904: loss 0.179720\n",
      "batch 4905: loss 0.157431\n",
      "batch 4906: loss 0.006013\n",
      "batch 4907: loss 0.009796\n",
      "batch 4908: loss 0.047484\n",
      "batch 4909: loss 0.063834\n",
      "batch 4910: loss 0.059886\n",
      "batch 4911: loss 0.192518\n",
      "batch 4912: loss 0.015891\n",
      "batch 4913: loss 0.014311\n",
      "batch 4914: loss 0.090476\n",
      "batch 4915: loss 0.068192\n",
      "batch 4916: loss 0.056924\n",
      "batch 4917: loss 0.016972\n",
      "batch 4918: loss 0.017975\n",
      "batch 4919: loss 0.071389\n",
      "batch 4920: loss 0.025616\n",
      "batch 4921: loss 0.047198\n",
      "batch 4922: loss 0.026599\n",
      "batch 4923: loss 0.022605\n",
      "batch 4924: loss 0.297025\n",
      "batch 4925: loss 0.023328\n",
      "batch 4926: loss 0.015506\n",
      "batch 4927: loss 0.009258\n",
      "batch 4928: loss 0.043717\n",
      "batch 4929: loss 0.080922\n",
      "batch 4930: loss 0.091699\n",
      "batch 4931: loss 0.023765\n",
      "batch 4932: loss 0.058296\n",
      "batch 4933: loss 0.041590\n",
      "batch 4934: loss 0.078053\n",
      "batch 4935: loss 0.021091\n",
      "batch 4936: loss 0.057071\n",
      "batch 4937: loss 0.046842\n",
      "batch 4938: loss 0.047090\n",
      "batch 4939: loss 0.059541\n",
      "batch 4940: loss 0.007921\n",
      "batch 4941: loss 0.015764\n",
      "batch 4942: loss 0.071168\n",
      "batch 4943: loss 0.017002\n",
      "batch 4944: loss 0.013463\n",
      "batch 4945: loss 0.023373\n",
      "batch 4946: loss 0.018210\n",
      "batch 4947: loss 0.024561\n",
      "batch 4948: loss 0.044912\n",
      "batch 4949: loss 0.039116\n",
      "batch 4950: loss 0.030407\n",
      "batch 4951: loss 0.015546\n",
      "batch 4952: loss 0.093502\n",
      "batch 4953: loss 0.069967\n",
      "batch 4954: loss 0.031896\n",
      "batch 4955: loss 0.040135\n",
      "batch 4956: loss 0.147138\n",
      "batch 4957: loss 0.049006\n",
      "batch 4958: loss 0.017761\n",
      "batch 4959: loss 0.063717\n",
      "batch 4960: loss 0.182511\n",
      "batch 4961: loss 0.021158\n",
      "batch 4962: loss 0.004640\n",
      "batch 4963: loss 0.052801\n",
      "batch 4964: loss 0.171312\n",
      "batch 4965: loss 0.099332\n",
      "batch 4966: loss 0.044493\n",
      "batch 4967: loss 0.043971\n",
      "batch 4968: loss 0.065879\n",
      "batch 4969: loss 0.076898\n",
      "batch 4970: loss 0.158359\n",
      "batch 4971: loss 0.024830\n",
      "batch 4972: loss 0.044730\n",
      "batch 4973: loss 0.039822\n",
      "batch 4974: loss 0.025452\n",
      "batch 4975: loss 0.104758\n",
      "batch 4976: loss 0.022134\n",
      "batch 4977: loss 0.065487\n",
      "batch 4978: loss 0.025954\n",
      "batch 4979: loss 0.158999\n",
      "batch 4980: loss 0.023074\n",
      "batch 4981: loss 0.013498\n",
      "batch 4982: loss 0.053235\n",
      "batch 4983: loss 0.052819\n",
      "batch 4984: loss 0.075185\n",
      "batch 4985: loss 0.017504\n",
      "batch 4986: loss 0.052595\n",
      "batch 4987: loss 0.314272\n",
      "batch 4988: loss 0.097948\n",
      "batch 4989: loss 0.087430\n",
      "batch 4990: loss 0.144744\n",
      "batch 4991: loss 0.008773\n",
      "batch 4992: loss 0.008521\n",
      "batch 4993: loss 0.017186\n",
      "batch 4994: loss 0.032695\n",
      "batch 4995: loss 0.023929\n",
      "batch 4996: loss 0.010485\n",
      "batch 4997: loss 0.021068\n",
      "batch 4998: loss 0.030092\n",
      "batch 4999: loss 0.051865\n",
      "batch 5000: loss 0.008559\n",
      "batch 5001: loss 0.083765\n",
      "batch 5002: loss 0.040346\n",
      "batch 5003: loss 0.040945\n",
      "batch 5004: loss 0.008640\n",
      "batch 5005: loss 0.020350\n",
      "batch 5006: loss 0.020223\n",
      "batch 5007: loss 0.054007\n",
      "batch 5008: loss 0.010429\n",
      "batch 5009: loss 0.038846\n",
      "batch 5010: loss 0.111261\n",
      "batch 5011: loss 0.012074\n",
      "batch 5012: loss 0.081890\n",
      "batch 5013: loss 0.191604\n",
      "batch 5014: loss 0.019293\n",
      "batch 5015: loss 0.073600\n",
      "batch 5016: loss 0.017396\n",
      "batch 5017: loss 0.012197\n",
      "batch 5018: loss 0.044586\n",
      "batch 5019: loss 0.017894\n",
      "batch 5020: loss 0.013292\n",
      "batch 5021: loss 0.085829\n",
      "batch 5022: loss 0.103226\n",
      "batch 5023: loss 0.014052\n",
      "batch 5024: loss 0.030771\n",
      "batch 5025: loss 0.017308\n",
      "batch 5026: loss 0.026645\n",
      "batch 5027: loss 0.073962\n",
      "batch 5028: loss 0.073448\n",
      "batch 5029: loss 0.048914\n",
      "batch 5030: loss 0.067531\n",
      "batch 5031: loss 0.045686\n",
      "batch 5032: loss 0.147785\n",
      "batch 5033: loss 0.174711\n",
      "batch 5034: loss 0.034088\n",
      "batch 5035: loss 0.022752\n",
      "batch 5036: loss 0.056516\n",
      "batch 5037: loss 0.014885\n",
      "batch 5038: loss 0.070739\n",
      "batch 5039: loss 0.030683\n",
      "batch 5040: loss 0.119917\n",
      "batch 5041: loss 0.110594\n",
      "batch 5042: loss 0.102365\n",
      "batch 5043: loss 0.022431\n",
      "batch 5044: loss 0.016211\n",
      "batch 5045: loss 0.117060\n",
      "batch 5046: loss 0.075144\n",
      "batch 5047: loss 0.150786\n",
      "batch 5048: loss 0.027294\n",
      "batch 5049: loss 0.020379\n",
      "batch 5050: loss 0.015516\n",
      "batch 5051: loss 0.144299\n",
      "batch 5052: loss 0.057945\n",
      "batch 5053: loss 0.027832\n",
      "batch 5054: loss 0.020437\n",
      "batch 5055: loss 0.069239\n",
      "batch 5056: loss 0.055262\n",
      "batch 5057: loss 0.080997\n",
      "batch 5058: loss 0.097841\n",
      "batch 5059: loss 0.044774\n",
      "batch 5060: loss 0.027916\n",
      "batch 5061: loss 0.016908\n",
      "batch 5062: loss 0.145040\n",
      "batch 5063: loss 0.071638\n",
      "batch 5064: loss 0.004663\n",
      "batch 5065: loss 0.054405\n",
      "batch 5066: loss 0.003913\n",
      "batch 5067: loss 0.170958\n",
      "batch 5068: loss 0.132404\n",
      "batch 5069: loss 0.057445\n",
      "batch 5070: loss 0.034600\n",
      "batch 5071: loss 0.012171\n",
      "batch 5072: loss 0.048999\n",
      "batch 5073: loss 0.008540\n",
      "batch 5074: loss 0.055680\n",
      "batch 5075: loss 0.024131\n",
      "batch 5076: loss 0.261635\n",
      "batch 5077: loss 0.018340\n",
      "batch 5078: loss 0.042335\n",
      "batch 5079: loss 0.033756\n",
      "batch 5080: loss 0.067713\n",
      "batch 5081: loss 0.014699\n",
      "batch 5082: loss 0.044176\n",
      "batch 5083: loss 0.024545\n",
      "batch 5084: loss 0.081369\n",
      "batch 5085: loss 0.061314\n",
      "batch 5086: loss 0.051371\n",
      "batch 5087: loss 0.035347\n",
      "batch 5088: loss 0.096706\n",
      "batch 5089: loss 0.075484\n",
      "batch 5090: loss 0.097946\n",
      "batch 5091: loss 0.050457\n",
      "batch 5092: loss 0.016789\n",
      "batch 5093: loss 0.106987\n",
      "batch 5094: loss 0.017025\n",
      "batch 5095: loss 0.054074\n",
      "batch 5096: loss 0.015107\n",
      "batch 5097: loss 0.184507\n",
      "batch 5098: loss 0.199642\n",
      "batch 5099: loss 0.005860\n",
      "batch 5100: loss 0.064626\n",
      "batch 5101: loss 0.015974\n",
      "batch 5102: loss 0.080142\n",
      "batch 5103: loss 0.088937\n",
      "batch 5104: loss 0.057706\n",
      "batch 5105: loss 0.070039\n",
      "batch 5106: loss 0.126600\n",
      "batch 5107: loss 0.076383\n",
      "batch 5108: loss 0.039433\n",
      "batch 5109: loss 0.038554\n",
      "batch 5110: loss 0.076414\n",
      "batch 5111: loss 0.035085\n",
      "batch 5112: loss 0.080044\n",
      "batch 5113: loss 0.079182\n",
      "batch 5114: loss 0.020189\n",
      "batch 5115: loss 0.035322\n",
      "batch 5116: loss 0.097250\n",
      "batch 5117: loss 0.051977\n",
      "batch 5118: loss 0.047757\n",
      "batch 5119: loss 0.080106\n",
      "batch 5120: loss 0.064355\n",
      "batch 5121: loss 0.045471\n",
      "batch 5122: loss 0.028436\n",
      "batch 5123: loss 0.073811\n",
      "batch 5124: loss 0.045147\n",
      "batch 5125: loss 0.247631\n",
      "batch 5126: loss 0.009611\n",
      "batch 5127: loss 0.005592\n",
      "batch 5128: loss 0.254027\n",
      "batch 5129: loss 0.018164\n",
      "batch 5130: loss 0.099585\n",
      "batch 5131: loss 0.032574\n",
      "batch 5132: loss 0.047722\n",
      "batch 5133: loss 0.304831\n",
      "batch 5134: loss 0.012953\n",
      "batch 5135: loss 0.058570\n",
      "batch 5136: loss 0.150497\n",
      "batch 5137: loss 0.001800\n",
      "batch 5138: loss 0.097413\n",
      "batch 5139: loss 0.024146\n",
      "batch 5140: loss 0.040959\n",
      "batch 5141: loss 0.166853\n",
      "batch 5142: loss 0.131250\n",
      "batch 5143: loss 0.088721\n",
      "batch 5144: loss 0.039710\n",
      "batch 5145: loss 0.063053\n",
      "batch 5146: loss 0.040416\n",
      "batch 5147: loss 0.207183\n",
      "batch 5148: loss 0.038367\n",
      "batch 5149: loss 0.052925\n",
      "batch 5150: loss 0.123593\n",
      "batch 5151: loss 0.042198\n",
      "batch 5152: loss 0.028787\n",
      "batch 5153: loss 0.030871\n",
      "batch 5154: loss 0.007211\n",
      "batch 5155: loss 0.042617\n",
      "batch 5156: loss 0.076885\n",
      "batch 5157: loss 0.011209\n",
      "batch 5158: loss 0.028507\n",
      "batch 5159: loss 0.127153\n",
      "batch 5160: loss 0.032245\n",
      "batch 5161: loss 0.007740\n",
      "batch 5162: loss 0.009478\n",
      "batch 5163: loss 0.151101\n",
      "batch 5164: loss 0.027211\n",
      "batch 5165: loss 0.015546\n",
      "batch 5166: loss 0.026063\n",
      "batch 5167: loss 0.012794\n",
      "batch 5168: loss 0.025984\n",
      "batch 5169: loss 0.020428\n",
      "batch 5170: loss 0.009140\n",
      "batch 5171: loss 0.243682\n",
      "batch 5172: loss 0.035912\n",
      "batch 5173: loss 0.054215\n",
      "batch 5174: loss 0.051758\n",
      "batch 5175: loss 0.028349\n",
      "batch 5176: loss 0.058992\n",
      "batch 5177: loss 0.013059\n",
      "batch 5178: loss 0.010201\n",
      "batch 5179: loss 0.083949\n",
      "batch 5180: loss 0.009513\n",
      "batch 5181: loss 0.017655\n",
      "batch 5182: loss 0.021507\n",
      "batch 5183: loss 0.094044\n",
      "batch 5184: loss 0.043024\n",
      "batch 5185: loss 0.150462\n",
      "batch 5186: loss 0.010642\n",
      "batch 5187: loss 0.339599\n",
      "batch 5188: loss 0.025568\n",
      "batch 5189: loss 0.066333\n",
      "batch 5190: loss 0.048661\n",
      "batch 5191: loss 0.018068\n",
      "batch 5192: loss 0.063748\n",
      "batch 5193: loss 0.007430\n",
      "batch 5194: loss 0.011457\n",
      "batch 5195: loss 0.111997\n",
      "batch 5196: loss 0.064438\n",
      "batch 5197: loss 0.027846\n",
      "batch 5198: loss 0.016318\n",
      "batch 5199: loss 0.115186\n",
      "batch 5200: loss 0.023914\n",
      "batch 5201: loss 0.061620\n",
      "batch 5202: loss 0.016157\n",
      "batch 5203: loss 0.016156\n",
      "batch 5204: loss 0.056070\n",
      "batch 5205: loss 0.041218\n",
      "batch 5206: loss 0.019399\n",
      "batch 5207: loss 0.040189\n",
      "batch 5208: loss 0.113227\n",
      "batch 5209: loss 0.012505\n",
      "batch 5210: loss 0.018733\n",
      "batch 5211: loss 0.011675\n",
      "batch 5212: loss 0.003605\n",
      "batch 5213: loss 0.004043\n",
      "batch 5214: loss 0.030990\n",
      "batch 5215: loss 0.061127\n",
      "batch 5216: loss 0.122111\n",
      "batch 5217: loss 0.060779\n",
      "batch 5218: loss 0.026645\n",
      "batch 5219: loss 0.024293\n",
      "batch 5220: loss 0.070890\n",
      "batch 5221: loss 0.022195\n",
      "batch 5222: loss 0.032985\n",
      "batch 5223: loss 0.006787\n",
      "batch 5224: loss 0.034466\n",
      "batch 5225: loss 0.018198\n",
      "batch 5226: loss 0.011576\n",
      "batch 5227: loss 0.135489\n",
      "batch 5228: loss 0.012168\n",
      "batch 5229: loss 0.175182\n",
      "batch 5230: loss 0.115604\n",
      "batch 5231: loss 0.017605\n",
      "batch 5232: loss 0.024994\n",
      "batch 5233: loss 0.055424\n",
      "batch 5234: loss 0.020859\n",
      "batch 5235: loss 0.024721\n",
      "batch 5236: loss 0.066790\n",
      "batch 5237: loss 0.119225\n",
      "batch 5238: loss 0.012930\n",
      "batch 5239: loss 0.013608\n",
      "batch 5240: loss 0.202269\n",
      "batch 5241: loss 0.203679\n",
      "batch 5242: loss 0.148611\n",
      "batch 5243: loss 0.005923\n",
      "batch 5244: loss 0.050126\n",
      "batch 5245: loss 0.153974\n",
      "batch 5246: loss 0.028247\n",
      "batch 5247: loss 0.017221\n",
      "batch 5248: loss 0.109465\n",
      "batch 5249: loss 0.035255\n",
      "batch 5250: loss 0.030385\n",
      "batch 5251: loss 0.045990\n",
      "batch 5252: loss 0.057504\n",
      "batch 5253: loss 0.057898\n",
      "batch 5254: loss 0.058901\n",
      "batch 5255: loss 0.067982\n",
      "batch 5256: loss 0.049609\n",
      "batch 5257: loss 0.023513\n",
      "batch 5258: loss 0.013223\n",
      "batch 5259: loss 0.156351\n",
      "batch 5260: loss 0.020854\n",
      "batch 5261: loss 0.002507\n",
      "batch 5262: loss 0.056403\n",
      "batch 5263: loss 0.067475\n",
      "batch 5264: loss 0.243023\n",
      "batch 5265: loss 0.093773\n",
      "batch 5266: loss 0.028905\n",
      "batch 5267: loss 0.226315\n",
      "batch 5268: loss 0.014786\n",
      "batch 5269: loss 0.011910\n",
      "batch 5270: loss 0.219441\n",
      "batch 5271: loss 0.031084\n",
      "batch 5272: loss 0.026666\n",
      "batch 5273: loss 0.083945\n",
      "batch 5274: loss 0.007631\n",
      "batch 5275: loss 0.090766\n",
      "batch 5276: loss 0.008129\n",
      "batch 5277: loss 0.011296\n",
      "batch 5278: loss 0.007760\n",
      "batch 5279: loss 0.041813\n",
      "batch 5280: loss 0.108885\n",
      "batch 5281: loss 0.077236\n",
      "batch 5282: loss 0.005219\n",
      "batch 5283: loss 0.015161\n",
      "batch 5284: loss 0.036696\n",
      "batch 5285: loss 0.211311\n",
      "batch 5286: loss 0.155227\n",
      "batch 5287: loss 0.145512\n",
      "batch 5288: loss 0.004313\n",
      "batch 5289: loss 0.030477\n",
      "batch 5290: loss 0.050293\n",
      "batch 5291: loss 0.034656\n",
      "batch 5292: loss 0.020187\n",
      "batch 5293: loss 0.132964\n",
      "batch 5294: loss 0.159945\n",
      "batch 5295: loss 0.025120\n",
      "batch 5296: loss 0.147987\n",
      "batch 5297: loss 0.013614\n",
      "batch 5298: loss 0.025845\n",
      "batch 5299: loss 0.062459\n",
      "batch 5300: loss 0.013333\n",
      "batch 5301: loss 0.060442\n",
      "batch 5302: loss 0.019263\n",
      "batch 5303: loss 0.120014\n",
      "batch 5304: loss 0.015299\n",
      "batch 5305: loss 0.033118\n",
      "batch 5306: loss 0.004302\n",
      "batch 5307: loss 0.055003\n",
      "batch 5308: loss 0.040641\n",
      "batch 5309: loss 0.008588\n",
      "batch 5310: loss 0.076369\n",
      "batch 5311: loss 0.040340\n",
      "batch 5312: loss 0.165810\n",
      "batch 5313: loss 0.037136\n",
      "batch 5314: loss 0.059480\n",
      "batch 5315: loss 0.064847\n",
      "batch 5316: loss 0.065063\n",
      "batch 5317: loss 0.015567\n",
      "batch 5318: loss 0.024764\n",
      "batch 5319: loss 0.032675\n",
      "batch 5320: loss 0.030146\n",
      "batch 5321: loss 0.060776\n",
      "batch 5322: loss 0.087928\n",
      "batch 5323: loss 0.027158\n",
      "batch 5324: loss 0.062449\n",
      "batch 5325: loss 0.049450\n",
      "batch 5326: loss 0.209069\n",
      "batch 5327: loss 0.115162\n",
      "batch 5328: loss 0.044608\n",
      "batch 5329: loss 0.064749\n",
      "batch 5330: loss 0.008567\n",
      "batch 5331: loss 0.012504\n",
      "batch 5332: loss 0.038165\n",
      "batch 5333: loss 0.119748\n",
      "batch 5334: loss 0.075690\n",
      "batch 5335: loss 0.062955\n",
      "batch 5336: loss 0.024622\n",
      "batch 5337: loss 0.057985\n",
      "batch 5338: loss 0.033076\n",
      "batch 5339: loss 0.019501\n",
      "batch 5340: loss 0.066124\n",
      "batch 5341: loss 0.097553\n",
      "batch 5342: loss 0.032956\n",
      "batch 5343: loss 0.021899\n",
      "batch 5344: loss 0.022774\n",
      "batch 5345: loss 0.017093\n",
      "batch 5346: loss 0.010411\n",
      "batch 5347: loss 0.022764\n",
      "batch 5348: loss 0.002752\n",
      "batch 5349: loss 0.120198\n",
      "batch 5350: loss 0.083906\n",
      "batch 5351: loss 0.006459\n",
      "batch 5352: loss 0.107224\n",
      "batch 5353: loss 0.124828\n",
      "batch 5354: loss 0.055713\n",
      "batch 5355: loss 0.043774\n",
      "batch 5356: loss 0.004417\n",
      "batch 5357: loss 0.079669\n",
      "batch 5358: loss 0.136629\n",
      "batch 5359: loss 0.041358\n",
      "batch 5360: loss 0.010058\n",
      "batch 5361: loss 0.036542\n",
      "batch 5362: loss 0.017799\n",
      "batch 5363: loss 0.108397\n",
      "batch 5364: loss 0.093857\n",
      "batch 5365: loss 0.103362\n",
      "batch 5366: loss 0.164902\n",
      "batch 5367: loss 0.035420\n",
      "batch 5368: loss 0.065022\n",
      "batch 5369: loss 0.011674\n",
      "batch 5370: loss 0.019732\n",
      "batch 5371: loss 0.056032\n",
      "batch 5372: loss 0.016551\n",
      "batch 5373: loss 0.028417\n",
      "batch 5374: loss 0.022779\n",
      "batch 5375: loss 0.071011\n",
      "batch 5376: loss 0.009192\n",
      "batch 5377: loss 0.005280\n",
      "batch 5378: loss 0.085160\n",
      "batch 5379: loss 0.052330\n",
      "batch 5380: loss 0.017737\n",
      "batch 5381: loss 0.100407\n",
      "batch 5382: loss 0.019019\n",
      "batch 5383: loss 0.012278\n",
      "batch 5384: loss 0.055381\n",
      "batch 5385: loss 0.139949\n",
      "batch 5386: loss 0.019660\n",
      "batch 5387: loss 0.017854\n",
      "batch 5388: loss 0.044250\n",
      "batch 5389: loss 0.024758\n",
      "batch 5390: loss 0.021996\n",
      "batch 5391: loss 0.063762\n",
      "batch 5392: loss 0.018253\n",
      "batch 5393: loss 0.039815\n",
      "batch 5394: loss 0.031163\n",
      "batch 5395: loss 0.004972\n",
      "batch 5396: loss 0.008039\n",
      "batch 5397: loss 0.009681\n",
      "batch 5398: loss 0.025371\n",
      "batch 5399: loss 0.154321\n",
      "batch 5400: loss 0.056491\n",
      "batch 5401: loss 0.016246\n",
      "batch 5402: loss 0.191935\n",
      "batch 5403: loss 0.011344\n",
      "batch 5404: loss 0.043709\n",
      "batch 5405: loss 0.010845\n",
      "batch 5406: loss 0.046010\n",
      "batch 5407: loss 0.020198\n",
      "batch 5408: loss 0.012407\n",
      "batch 5409: loss 0.031370\n",
      "batch 5410: loss 0.015099\n",
      "batch 5411: loss 0.024829\n",
      "batch 5412: loss 0.028457\n",
      "batch 5413: loss 0.011676\n",
      "batch 5414: loss 0.052397\n",
      "batch 5415: loss 0.030262\n",
      "batch 5416: loss 0.076859\n",
      "batch 5417: loss 0.028728\n",
      "batch 5418: loss 0.071540\n",
      "batch 5419: loss 0.008735\n",
      "batch 5420: loss 0.016783\n",
      "batch 5421: loss 0.005967\n",
      "batch 5422: loss 0.060562\n",
      "batch 5423: loss 0.031082\n",
      "batch 5424: loss 0.015918\n",
      "batch 5425: loss 0.213813\n",
      "batch 5426: loss 0.095266\n",
      "batch 5427: loss 0.016935\n",
      "batch 5428: loss 0.014264\n",
      "batch 5429: loss 0.012626\n",
      "batch 5430: loss 0.040705\n",
      "batch 5431: loss 0.181925\n",
      "batch 5432: loss 0.008999\n",
      "batch 5433: loss 0.035376\n",
      "batch 5434: loss 0.044972\n",
      "batch 5435: loss 0.135677\n",
      "batch 5436: loss 0.048558\n",
      "batch 5437: loss 0.019890\n",
      "batch 5438: loss 0.199209\n",
      "batch 5439: loss 0.036916\n",
      "batch 5440: loss 0.015957\n",
      "batch 5441: loss 0.007933\n",
      "batch 5442: loss 0.057500\n",
      "batch 5443: loss 0.067447\n",
      "batch 5444: loss 0.074640\n",
      "batch 5445: loss 0.020333\n",
      "batch 5446: loss 0.095669\n",
      "batch 5447: loss 0.013735\n",
      "batch 5448: loss 0.031391\n",
      "batch 5449: loss 0.031134\n",
      "batch 5450: loss 0.072780\n",
      "batch 5451: loss 0.002085\n",
      "batch 5452: loss 0.126643\n",
      "batch 5453: loss 0.014178\n",
      "batch 5454: loss 0.033846\n",
      "batch 5455: loss 0.054436\n",
      "batch 5456: loss 0.014238\n",
      "batch 5457: loss 0.015806\n",
      "batch 5458: loss 0.044172\n",
      "batch 5459: loss 0.031676\n",
      "batch 5460: loss 0.031727\n",
      "batch 5461: loss 0.010182\n",
      "batch 5462: loss 0.128318\n",
      "batch 5463: loss 0.016548\n",
      "batch 5464: loss 0.076011\n",
      "batch 5465: loss 0.055717\n",
      "batch 5466: loss 0.036897\n",
      "batch 5467: loss 0.019461\n",
      "batch 5468: loss 0.074959\n",
      "batch 5469: loss 0.049829\n",
      "batch 5470: loss 0.004603\n",
      "batch 5471: loss 0.031241\n",
      "batch 5472: loss 0.018661\n",
      "batch 5473: loss 0.105534\n",
      "batch 5474: loss 0.074498\n",
      "batch 5475: loss 0.015053\n",
      "batch 5476: loss 0.063788\n",
      "batch 5477: loss 0.082175\n",
      "batch 5478: loss 0.156905\n",
      "batch 5479: loss 0.004246\n",
      "batch 5480: loss 0.010453\n",
      "batch 5481: loss 0.108142\n",
      "batch 5482: loss 0.094209\n",
      "batch 5483: loss 0.029217\n",
      "batch 5484: loss 0.058706\n",
      "batch 5485: loss 0.115439\n",
      "batch 5486: loss 0.027316\n",
      "batch 5487: loss 0.085079\n",
      "batch 5488: loss 0.023405\n",
      "batch 5489: loss 0.049670\n",
      "batch 5490: loss 0.047073\n",
      "batch 5491: loss 0.018604\n",
      "batch 5492: loss 0.090076\n",
      "batch 5493: loss 0.009878\n",
      "batch 5494: loss 0.020990\n",
      "batch 5495: loss 0.025179\n",
      "batch 5496: loss 0.114826\n",
      "batch 5497: loss 0.019705\n",
      "batch 5498: loss 0.013065\n",
      "batch 5499: loss 0.114221\n",
      "batch 5500: loss 0.021243\n",
      "batch 5501: loss 0.063370\n",
      "batch 5502: loss 0.024491\n",
      "batch 5503: loss 0.054385\n",
      "batch 5504: loss 0.087464\n",
      "batch 5505: loss 0.199957\n",
      "batch 5506: loss 0.014465\n",
      "batch 5507: loss 0.065716\n",
      "batch 5508: loss 0.050680\n",
      "batch 5509: loss 0.233421\n",
      "batch 5510: loss 0.051072\n",
      "batch 5511: loss 0.018415\n",
      "batch 5512: loss 0.021291\n",
      "batch 5513: loss 0.018088\n",
      "batch 5514: loss 0.004869\n",
      "batch 5515: loss 0.117662\n",
      "batch 5516: loss 0.013764\n",
      "batch 5517: loss 0.197685\n",
      "batch 5518: loss 0.011742\n",
      "batch 5519: loss 0.152982\n",
      "batch 5520: loss 0.010951\n",
      "batch 5521: loss 0.137236\n",
      "batch 5522: loss 0.047597\n",
      "batch 5523: loss 0.095715\n",
      "batch 5524: loss 0.059918\n",
      "batch 5525: loss 0.025463\n",
      "batch 5526: loss 0.028999\n",
      "batch 5527: loss 0.122985\n",
      "batch 5528: loss 0.019496\n",
      "batch 5529: loss 0.030304\n",
      "batch 5530: loss 0.006325\n",
      "batch 5531: loss 0.046609\n",
      "batch 5532: loss 0.019512\n",
      "batch 5533: loss 0.094310\n",
      "batch 5534: loss 0.108986\n",
      "batch 5535: loss 0.038175\n",
      "batch 5536: loss 0.039001\n",
      "batch 5537: loss 0.099868\n",
      "batch 5538: loss 0.040500\n",
      "batch 5539: loss 0.116257\n",
      "batch 5540: loss 0.161923\n",
      "batch 5541: loss 0.060388\n",
      "batch 5542: loss 0.015179\n",
      "batch 5543: loss 0.006910\n",
      "batch 5544: loss 0.039513\n",
      "batch 5545: loss 0.152457\n",
      "batch 5546: loss 0.016739\n",
      "batch 5547: loss 0.028427\n",
      "batch 5548: loss 0.035031\n",
      "batch 5549: loss 0.108965\n",
      "batch 5550: loss 0.070408\n",
      "batch 5551: loss 0.021043\n",
      "batch 5552: loss 0.055241\n",
      "batch 5553: loss 0.042118\n",
      "batch 5554: loss 0.143527\n",
      "batch 5555: loss 0.008626\n",
      "batch 5556: loss 0.080829\n",
      "batch 5557: loss 0.036551\n",
      "batch 5558: loss 0.032582\n",
      "batch 5559: loss 0.035643\n",
      "batch 5560: loss 0.026365\n",
      "batch 5561: loss 0.099222\n",
      "batch 5562: loss 0.010835\n",
      "batch 5563: loss 0.020508\n",
      "batch 5564: loss 0.015600\n",
      "batch 5565: loss 0.016928\n",
      "batch 5566: loss 0.048617\n",
      "batch 5567: loss 0.015541\n",
      "batch 5568: loss 0.056841\n",
      "batch 5569: loss 0.131962\n",
      "batch 5570: loss 0.084228\n",
      "batch 5571: loss 0.021308\n",
      "batch 5572: loss 0.040371\n",
      "batch 5573: loss 0.085806\n",
      "batch 5574: loss 0.016518\n",
      "batch 5575: loss 0.085323\n",
      "batch 5576: loss 0.008919\n",
      "batch 5577: loss 0.052980\n",
      "batch 5578: loss 0.121913\n",
      "batch 5579: loss 0.044783\n",
      "batch 5580: loss 0.024347\n",
      "batch 5581: loss 0.042823\n",
      "batch 5582: loss 0.017074\n",
      "batch 5583: loss 0.018274\n",
      "batch 5584: loss 0.005658\n",
      "batch 5585: loss 0.023009\n",
      "batch 5586: loss 0.019205\n",
      "batch 5587: loss 0.006137\n",
      "batch 5588: loss 0.045184\n",
      "batch 5589: loss 0.015966\n",
      "batch 5590: loss 0.029603\n",
      "batch 5591: loss 0.077267\n",
      "batch 5592: loss 0.037768\n",
      "batch 5593: loss 0.034659\n",
      "batch 5594: loss 0.088667\n",
      "batch 5595: loss 0.079628\n",
      "batch 5596: loss 0.055441\n",
      "batch 5597: loss 0.099574\n",
      "batch 5598: loss 0.003446\n",
      "batch 5599: loss 0.063090\n",
      "batch 5600: loss 0.023802\n",
      "batch 5601: loss 0.016499\n",
      "batch 5602: loss 0.032176\n",
      "batch 5603: loss 0.061341\n",
      "batch 5604: loss 0.028573\n",
      "batch 5605: loss 0.032720\n",
      "batch 5606: loss 0.029685\n",
      "batch 5607: loss 0.136062\n",
      "batch 5608: loss 0.108911\n",
      "batch 5609: loss 0.068739\n",
      "batch 5610: loss 0.030199\n",
      "batch 5611: loss 0.024489\n",
      "batch 5612: loss 0.053321\n",
      "batch 5613: loss 0.046718\n",
      "batch 5614: loss 0.252654\n",
      "batch 5615: loss 0.019568\n",
      "batch 5616: loss 0.042320\n",
      "batch 5617: loss 0.003206\n",
      "batch 5618: loss 0.008122\n",
      "batch 5619: loss 0.012110\n",
      "batch 5620: loss 0.007254\n",
      "batch 5621: loss 0.008314\n",
      "batch 5622: loss 0.141772\n",
      "batch 5623: loss 0.005720\n",
      "batch 5624: loss 0.083330\n",
      "batch 5625: loss 0.175213\n",
      "batch 5626: loss 0.044439\n",
      "batch 5627: loss 0.056680\n",
      "batch 5628: loss 0.015351\n",
      "batch 5629: loss 0.008449\n",
      "batch 5630: loss 0.134339\n",
      "batch 5631: loss 0.044932\n",
      "batch 5632: loss 0.053023\n",
      "batch 5633: loss 0.075301\n",
      "batch 5634: loss 0.014503\n",
      "batch 5635: loss 0.006794\n",
      "batch 5636: loss 0.014916\n",
      "batch 5637: loss 0.059392\n",
      "batch 5638: loss 0.016026\n",
      "batch 5639: loss 0.109969\n",
      "batch 5640: loss 0.083014\n",
      "batch 5641: loss 0.022113\n",
      "batch 5642: loss 0.036182\n",
      "batch 5643: loss 0.059886\n",
      "batch 5644: loss 0.024610\n",
      "batch 5645: loss 0.019820\n",
      "batch 5646: loss 0.100155\n",
      "batch 5647: loss 0.049418\n",
      "batch 5648: loss 0.008338\n",
      "batch 5649: loss 0.076610\n",
      "batch 5650: loss 0.027372\n",
      "batch 5651: loss 0.085780\n",
      "batch 5652: loss 0.076237\n",
      "batch 5653: loss 0.090285\n",
      "batch 5654: loss 0.075214\n",
      "batch 5655: loss 0.056859\n",
      "batch 5656: loss 0.025345\n",
      "batch 5657: loss 0.024410\n",
      "batch 5658: loss 0.148090\n",
      "batch 5659: loss 0.080382\n",
      "batch 5660: loss 0.009376\n",
      "batch 5661: loss 0.010266\n",
      "batch 5662: loss 0.095842\n",
      "batch 5663: loss 0.124717\n",
      "batch 5664: loss 0.139861\n",
      "batch 5665: loss 0.007827\n",
      "batch 5666: loss 0.026135\n",
      "batch 5667: loss 0.060903\n",
      "batch 5668: loss 0.026857\n",
      "batch 5669: loss 0.054651\n",
      "batch 5670: loss 0.046861\n",
      "batch 5671: loss 0.019559\n",
      "batch 5672: loss 0.009118\n",
      "batch 5673: loss 0.043035\n",
      "batch 5674: loss 0.049556\n",
      "batch 5675: loss 0.031165\n",
      "batch 5676: loss 0.093585\n",
      "batch 5677: loss 0.020355\n",
      "batch 5678: loss 0.154320\n",
      "batch 5679: loss 0.029239\n",
      "batch 5680: loss 0.040281\n",
      "batch 5681: loss 0.004737\n",
      "batch 5682: loss 0.043516\n",
      "batch 5683: loss 0.078886\n",
      "batch 5684: loss 0.019269\n",
      "batch 5685: loss 0.142036\n",
      "batch 5686: loss 0.051710\n",
      "batch 5687: loss 0.067747\n",
      "batch 5688: loss 0.059433\n",
      "batch 5689: loss 0.109981\n",
      "batch 5690: loss 0.099418\n",
      "batch 5691: loss 0.224221\n",
      "batch 5692: loss 0.008826\n",
      "batch 5693: loss 0.130844\n",
      "batch 5694: loss 0.014650\n",
      "batch 5695: loss 0.125501\n",
      "batch 5696: loss 0.032374\n",
      "batch 5697: loss 0.142683\n",
      "batch 5698: loss 0.024751\n",
      "batch 5699: loss 0.011720\n",
      "batch 5700: loss 0.011096\n",
      "batch 5701: loss 0.040440\n",
      "batch 5702: loss 0.161700\n",
      "batch 5703: loss 0.008845\n",
      "batch 5704: loss 0.019270\n",
      "batch 5705: loss 0.159367\n",
      "batch 5706: loss 0.048304\n",
      "batch 5707: loss 0.003819\n",
      "batch 5708: loss 0.147901\n",
      "batch 5709: loss 0.015191\n",
      "batch 5710: loss 0.009682\n",
      "batch 5711: loss 0.028839\n",
      "batch 5712: loss 0.171263\n",
      "batch 5713: loss 0.077751\n",
      "batch 5714: loss 0.025905\n",
      "batch 5715: loss 0.003735\n",
      "batch 5716: loss 0.098002\n",
      "batch 5717: loss 0.045012\n",
      "batch 5718: loss 0.028499\n",
      "batch 5719: loss 0.080867\n",
      "batch 5720: loss 0.063275\n",
      "batch 5721: loss 0.067845\n",
      "batch 5722: loss 0.020150\n",
      "batch 5723: loss 0.078792\n",
      "batch 5724: loss 0.049186\n",
      "batch 5725: loss 0.048263\n",
      "batch 5726: loss 0.286281\n",
      "batch 5727: loss 0.061675\n",
      "batch 5728: loss 0.030361\n",
      "batch 5729: loss 0.015294\n",
      "batch 5730: loss 0.031464\n",
      "batch 5731: loss 0.011643\n",
      "batch 5732: loss 0.035610\n",
      "batch 5733: loss 0.015129\n",
      "batch 5734: loss 0.010432\n",
      "batch 5735: loss 0.046671\n",
      "batch 5736: loss 0.018203\n",
      "batch 5737: loss 0.010807\n",
      "batch 5738: loss 0.011040\n",
      "batch 5739: loss 0.017391\n",
      "batch 5740: loss 0.173339\n",
      "batch 5741: loss 0.063501\n",
      "batch 5742: loss 0.010895\n",
      "batch 5743: loss 0.004965\n",
      "batch 5744: loss 0.081933\n",
      "batch 5745: loss 0.023291\n",
      "batch 5746: loss 0.029595\n",
      "batch 5747: loss 0.019489\n",
      "batch 5748: loss 0.015877\n",
      "batch 5749: loss 0.014157\n",
      "batch 5750: loss 0.083461\n",
      "batch 5751: loss 0.010459\n",
      "batch 5752: loss 0.028920\n",
      "batch 5753: loss 0.124972\n",
      "batch 5754: loss 0.069077\n",
      "batch 5755: loss 0.004014\n",
      "batch 5756: loss 0.024119\n",
      "batch 5757: loss 0.025125\n",
      "batch 5758: loss 0.015770\n",
      "batch 5759: loss 0.202988\n",
      "batch 5760: loss 0.248818\n",
      "batch 5761: loss 0.077586\n",
      "batch 5762: loss 0.007672\n",
      "batch 5763: loss 0.020724\n",
      "batch 5764: loss 0.015582\n",
      "batch 5765: loss 0.047919\n",
      "batch 5766: loss 0.028195\n",
      "batch 5767: loss 0.061529\n",
      "batch 5768: loss 0.003985\n",
      "batch 5769: loss 0.107416\n",
      "batch 5770: loss 0.088999\n",
      "batch 5771: loss 0.049626\n",
      "batch 5772: loss 0.045000\n",
      "batch 5773: loss 0.092377\n",
      "batch 5774: loss 0.037362\n",
      "batch 5775: loss 0.008747\n",
      "batch 5776: loss 0.155891\n",
      "batch 5777: loss 0.098974\n",
      "batch 5778: loss 0.146498\n",
      "batch 5779: loss 0.083864\n",
      "batch 5780: loss 0.213959\n",
      "batch 5781: loss 0.186320\n",
      "batch 5782: loss 0.008662\n",
      "batch 5783: loss 0.046413\n",
      "batch 5784: loss 0.013625\n",
      "batch 5785: loss 0.082184\n",
      "batch 5786: loss 0.104774\n",
      "batch 5787: loss 0.042039\n",
      "batch 5788: loss 0.100750\n",
      "batch 5789: loss 0.002007\n",
      "batch 5790: loss 0.015818\n",
      "batch 5791: loss 0.007558\n",
      "batch 5792: loss 0.081899\n",
      "batch 5793: loss 0.002927\n",
      "batch 5794: loss 0.003530\n",
      "batch 5795: loss 0.079298\n",
      "batch 5796: loss 0.019619\n",
      "batch 5797: loss 0.055755\n",
      "batch 5798: loss 0.015572\n",
      "batch 5799: loss 0.071372\n",
      "batch 5800: loss 0.011529\n",
      "batch 5801: loss 0.100613\n",
      "batch 5802: loss 0.176210\n",
      "batch 5803: loss 0.067844\n",
      "batch 5804: loss 0.039941\n",
      "batch 5805: loss 0.225166\n",
      "batch 5806: loss 0.037292\n",
      "batch 5807: loss 0.009870\n",
      "batch 5808: loss 0.037537\n",
      "batch 5809: loss 0.045961\n",
      "batch 5810: loss 0.025553\n",
      "batch 5811: loss 0.037385\n",
      "batch 5812: loss 0.280452\n",
      "batch 5813: loss 0.017487\n",
      "batch 5814: loss 0.030600\n",
      "batch 5815: loss 0.007120\n",
      "batch 5816: loss 0.026982\n",
      "batch 5817: loss 0.026887\n",
      "batch 5818: loss 0.165714\n",
      "batch 5819: loss 0.034218\n",
      "batch 5820: loss 0.067226\n",
      "batch 5821: loss 0.039480\n",
      "batch 5822: loss 0.219724\n",
      "batch 5823: loss 0.067706\n",
      "batch 5824: loss 0.023600\n",
      "batch 5825: loss 0.077113\n",
      "batch 5826: loss 0.075737\n",
      "batch 5827: loss 0.083749\n",
      "batch 5828: loss 0.127613\n",
      "batch 5829: loss 0.018227\n",
      "batch 5830: loss 0.047822\n",
      "batch 5831: loss 0.072812\n",
      "batch 5832: loss 0.048879\n",
      "batch 5833: loss 0.010981\n",
      "batch 5834: loss 0.014148\n",
      "batch 5835: loss 0.047644\n",
      "batch 5836: loss 0.033239\n",
      "batch 5837: loss 0.169946\n",
      "batch 5838: loss 0.022380\n",
      "batch 5839: loss 0.067785\n",
      "batch 5840: loss 0.189853\n",
      "batch 5841: loss 0.070750\n",
      "batch 5842: loss 0.041399\n",
      "batch 5843: loss 0.034405\n",
      "batch 5844: loss 0.032314\n",
      "batch 5845: loss 0.186073\n",
      "batch 5846: loss 0.037270\n",
      "batch 5847: loss 0.038573\n",
      "batch 5848: loss 0.028070\n",
      "batch 5849: loss 0.062840\n",
      "batch 5850: loss 0.247774\n",
      "batch 5851: loss 0.286963\n",
      "batch 5852: loss 0.073434\n",
      "batch 5853: loss 0.020955\n",
      "batch 5854: loss 0.038716\n",
      "batch 5855: loss 0.046479\n",
      "batch 5856: loss 0.004545\n",
      "batch 5857: loss 0.157267\n",
      "batch 5858: loss 0.012779\n",
      "batch 5859: loss 0.013665\n",
      "batch 5860: loss 0.124362\n",
      "batch 5861: loss 0.009334\n",
      "batch 5862: loss 0.078275\n",
      "batch 5863: loss 0.016907\n",
      "batch 5864: loss 0.046802\n",
      "batch 5865: loss 0.214494\n",
      "batch 5866: loss 0.019501\n",
      "batch 5867: loss 0.073758\n",
      "batch 5868: loss 0.164977\n",
      "batch 5869: loss 0.008359\n",
      "batch 5870: loss 0.094133\n",
      "batch 5871: loss 0.013931\n",
      "batch 5872: loss 0.075589\n",
      "batch 5873: loss 0.011292\n",
      "batch 5874: loss 0.031187\n",
      "batch 5875: loss 0.013389\n",
      "batch 5876: loss 0.044268\n",
      "batch 5877: loss 0.003488\n",
      "batch 5878: loss 0.048485\n",
      "batch 5879: loss 0.036496\n",
      "batch 5880: loss 0.045852\n",
      "batch 5881: loss 0.097108\n",
      "batch 5882: loss 0.013067\n",
      "batch 5883: loss 0.091328\n",
      "batch 5884: loss 0.114457\n",
      "batch 5885: loss 0.022161\n",
      "batch 5886: loss 0.010320\n",
      "batch 5887: loss 0.038269\n",
      "batch 5888: loss 0.200281\n",
      "batch 5889: loss 0.020456\n",
      "batch 5890: loss 0.020105\n",
      "batch 5891: loss 0.058193\n",
      "batch 5892: loss 0.009627\n",
      "batch 5893: loss 0.026242\n",
      "batch 5894: loss 0.008068\n",
      "batch 5895: loss 0.092240\n",
      "batch 5896: loss 0.013471\n",
      "batch 5897: loss 0.064265\n",
      "batch 5898: loss 0.116264\n",
      "batch 5899: loss 0.012055\n",
      "batch 5900: loss 0.217770\n",
      "batch 5901: loss 0.043777\n",
      "batch 5902: loss 0.097538\n",
      "batch 5903: loss 0.006938\n",
      "batch 5904: loss 0.024410\n",
      "batch 5905: loss 0.040793\n",
      "batch 5906: loss 0.025189\n",
      "batch 5907: loss 0.032263\n",
      "batch 5908: loss 0.016977\n",
      "batch 5909: loss 0.140031\n",
      "batch 5910: loss 0.055217\n",
      "batch 5911: loss 0.013012\n",
      "batch 5912: loss 0.014054\n",
      "batch 5913: loss 0.066771\n",
      "batch 5914: loss 0.137983\n",
      "batch 5915: loss 0.006907\n",
      "batch 5916: loss 0.224073\n",
      "batch 5917: loss 0.046366\n",
      "batch 5918: loss 0.005568\n",
      "batch 5919: loss 0.055002\n",
      "batch 5920: loss 0.023068\n",
      "batch 5921: loss 0.061784\n",
      "batch 5922: loss 0.023934\n",
      "batch 5923: loss 0.156368\n",
      "batch 5924: loss 0.021412\n",
      "batch 5925: loss 0.047706\n",
      "batch 5926: loss 0.049938\n",
      "batch 5927: loss 0.022219\n",
      "batch 5928: loss 0.022294\n",
      "batch 5929: loss 0.005391\n",
      "batch 5930: loss 0.028766\n",
      "batch 5931: loss 0.100072\n",
      "batch 5932: loss 0.180542\n",
      "batch 5933: loss 0.025461\n",
      "batch 5934: loss 0.193629\n",
      "batch 5935: loss 0.040111\n",
      "batch 5936: loss 0.021595\n",
      "batch 5937: loss 0.008975\n",
      "batch 5938: loss 0.040715\n",
      "batch 5939: loss 0.168659\n",
      "batch 5940: loss 0.044105\n",
      "batch 5941: loss 0.034650\n",
      "batch 5942: loss 0.019167\n",
      "batch 5943: loss 0.041213\n",
      "batch 5944: loss 0.037543\n",
      "batch 5945: loss 0.020950\n",
      "batch 5946: loss 0.016937\n",
      "batch 5947: loss 0.061251\n",
      "batch 5948: loss 0.052368\n",
      "batch 5949: loss 0.034084\n",
      "batch 5950: loss 0.012723\n",
      "batch 5951: loss 0.173987\n",
      "batch 5952: loss 0.027742\n",
      "batch 5953: loss 0.024027\n",
      "batch 5954: loss 0.072922\n",
      "batch 5955: loss 0.085990\n",
      "batch 5956: loss 0.080330\n",
      "batch 5957: loss 0.106474\n",
      "batch 5958: loss 0.057649\n",
      "batch 5959: loss 0.012865\n",
      "batch 5960: loss 0.037749\n",
      "batch 5961: loss 0.031763\n",
      "batch 5962: loss 0.042855\n",
      "batch 5963: loss 0.018941\n",
      "batch 5964: loss 0.120311\n",
      "batch 5965: loss 0.044742\n",
      "batch 5966: loss 0.030087\n",
      "batch 5967: loss 0.004418\n",
      "batch 5968: loss 0.046045\n",
      "batch 5969: loss 0.335566\n",
      "batch 5970: loss 0.016411\n",
      "batch 5971: loss 0.033184\n",
      "batch 5972: loss 0.081225\n",
      "batch 5973: loss 0.049297\n",
      "batch 5974: loss 0.005398\n",
      "batch 5975: loss 0.007714\n",
      "batch 5976: loss 0.010316\n",
      "batch 5977: loss 0.188893\n",
      "batch 5978: loss 0.078109\n",
      "batch 5979: loss 0.157318\n",
      "batch 5980: loss 0.012776\n",
      "batch 5981: loss 0.010217\n",
      "batch 5982: loss 0.027590\n",
      "batch 5983: loss 0.005805\n",
      "batch 5984: loss 0.035288\n",
      "batch 5985: loss 0.063923\n",
      "batch 5986: loss 0.023935\n",
      "batch 5987: loss 0.020456\n",
      "batch 5988: loss 0.054405\n",
      "batch 5989: loss 0.015250\n",
      "batch 5990: loss 0.003892\n",
      "batch 5991: loss 0.022728\n",
      "batch 5992: loss 0.094774\n",
      "batch 5993: loss 0.022700\n",
      "batch 5994: loss 0.159076\n",
      "batch 5995: loss 0.005058\n",
      "batch 5996: loss 0.011916\n",
      "batch 5997: loss 0.029114\n",
      "batch 5998: loss 0.022300\n",
      "batch 5999: loss 0.025928\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "class MNISTLoader():\n",
    "    def __init__(self):\n",
    "        mnist = tf.keras.datasets.mnist\n",
    "        (self.train_data, self.train_label), (self.test_data, self.test_label) = mnist.load_data()\n",
    "\n",
    "        self.train_data = np.expand_dims(self.train_data.astype(np.float32) / 255.0, axis=-1)      # [60000, 28, 28, 1]\n",
    "        self.test_data = np.expand_dims(self.test_data.astype(np.float32) / 255.0, axis=-1)        # [10000, 28, 28, 1]\n",
    "        self.train_label = self.train_label.astype(np.int32)    # [60000]\n",
    "        self.test_label = self.test_label.astype(np.int32)      # [10000]\n",
    "        self.num_train_data, self.num_test_data = self.train_data.shape[0], self.test_data.shape[0]\n",
    "\n",
    "    def get_batch(self, batch_size):\n",
    "\n",
    "        index = np.random.randint(0, self.num_train_data, batch_size)\n",
    "        return self.train_data[index, :], self.train_label[index]\n",
    "    \n",
    "class MLP(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = tf.keras.layers.Flatten()    \n",
    "        self.dense1 = tf.keras.layers.Dense(units=100, activation=tf.nn.relu)\n",
    "        self.dense2 = tf.keras.layers.Dense(units=10)\n",
    "\n",
    "    def call(self, inputs):         # [batch_size, 28, 28, 1]\n",
    "        x = self.flatten(inputs)    # [batch_size, 784]\n",
    "        x = self.dense1(x)          # [batch_size, 100]\n",
    "        x = self.dense2(x)          # [batch_size, 10]\n",
    "        output = tf.nn.softmax(x)\n",
    "        return output\n",
    "\n",
    "num_epochs = 5\n",
    "batch_size = 50\n",
    "learning_rate = 0.001\n",
    "\n",
    "model = MLP()\n",
    "data_loader = MNISTLoader()\n",
    "#optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "#optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate)\n",
    "optimizer = tf.keras.optimizers.RMSprop(learning_rate=learning_rate)\n",
    "\n",
    "num_batches = int(data_loader.num_train_data // batch_size * num_epochs)\n",
    "for batch_index in range(num_batches):\n",
    "    X, y = data_loader.get_batch(batch_size)\n",
    "    with tf.GradientTape() as tape:\n",
    "        y_pred = model(X)\n",
    "        loss = tf.keras.losses.sparse_categorical_crossentropy(y_true=y, y_pred=y_pred)\n",
    "        loss = tf.reduce_mean(loss)\n",
    "        print(\"batch %d: loss %f\" % (batch_index, loss.numpy()))\n",
    "    grads = tape.gradient(loss, model.variables)\n",
    "    optimizer.apply_gradients(grads_and_vars=zip(grads, model.variables))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 優化神經網路(keras版本)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)   (60000,)\n",
      "(10000, 784)   (10000,)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "tf.compat.v1.reset_default_graph()\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "x_train = x_train.reshape([x_train.shape[0], -1])\n",
    "x_test = x_test.reshape([x_test.shape[0], -1])\n",
    "print(x_train.shape, ' ', y_train.shape)\n",
    "print(x_test.shape, ' ', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Users\\shise\\anaconda3\\envs\\py39\\lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">50,240</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">4,160</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">4,160</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">650</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │        \u001b[38;5;34m50,240\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_5 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m4,160\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_6 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m4,160\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_7 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │           \u001b[38;5;34m650\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">59,210</span> (231.29 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m59,210\u001b[0m (231.29 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">59,210</span> (231.29 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m59,210\u001b[0m (231.29 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = keras.Sequential([\n",
    "    layers.Dense(64, activation='relu', input_shape=(784,)),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "\n",
    "#keras.optimizers.Adagrad(learning_rate=0.01)\n",
    "#keras.optimizers.Adam(learning_rate=0.01)\n",
    "#keras.optimizers.RMSprop(learning_rate=0.01)\n",
    "\n",
    "# provide labels as one_hot representation => tf.keras.losses.CategoricalCrossentropy\n",
    "# provide labels as integers => tf.keras.losses.SparseCategoricalCrossentropy \n",
    "model.compile(optimizer=keras.optimizers.Adam(),\n",
    "             loss=keras.losses.SparseCategoricalCrossentropy(),\n",
    "             metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "235/235 - 2s - 8ms/step - accuracy: 0.7573 - loss: 2.6060 - val_accuracy: 0.8610 - val_loss: 0.6462\n",
      "Epoch 2/100\n",
      "235/235 - 1s - 3ms/step - accuracy: 0.8838 - loss: 0.4757 - val_accuracy: 0.8985 - val_loss: 0.4269\n",
      "Epoch 3/100\n",
      "235/235 - 1s - 2ms/step - accuracy: 0.9154 - loss: 0.3157 - val_accuracy: 0.9106 - val_loss: 0.3523\n",
      "Epoch 4/100\n",
      "235/235 - 1s - 2ms/step - accuracy: 0.9319 - loss: 0.2427 - val_accuracy: 0.9198 - val_loss: 0.3006\n",
      "Epoch 5/100\n",
      "235/235 - 1s - 2ms/step - accuracy: 0.9406 - loss: 0.2037 - val_accuracy: 0.9303 - val_loss: 0.2669\n",
      "Epoch 6/100\n",
      "235/235 - 1s - 2ms/step - accuracy: 0.9498 - loss: 0.1729 - val_accuracy: 0.9348 - val_loss: 0.2560\n",
      "Epoch 7/100\n",
      "235/235 - 1s - 2ms/step - accuracy: 0.9553 - loss: 0.1508 - val_accuracy: 0.9360 - val_loss: 0.2452\n",
      "Epoch 8/100\n",
      "235/235 - 1s - 2ms/step - accuracy: 0.9604 - loss: 0.1328 - val_accuracy: 0.9407 - val_loss: 0.2315\n",
      "Epoch 9/100\n",
      "235/235 - 1s - 3ms/step - accuracy: 0.9630 - loss: 0.1199 - val_accuracy: 0.9406 - val_loss: 0.2194\n",
      "Epoch 10/100\n",
      "235/235 - 1s - 3ms/step - accuracy: 0.9669 - loss: 0.1078 - val_accuracy: 0.9428 - val_loss: 0.2254\n",
      "Epoch 11/100\n",
      "235/235 - 1s - 3ms/step - accuracy: 0.9684 - loss: 0.1027 - val_accuracy: 0.9476 - val_loss: 0.2064\n",
      "Epoch 12/100\n",
      "235/235 - 1s - 3ms/step - accuracy: 0.9704 - loss: 0.0974 - val_accuracy: 0.9511 - val_loss: 0.1896\n",
      "Epoch 13/100\n",
      "235/235 - 1s - 3ms/step - accuracy: 0.9741 - loss: 0.0825 - val_accuracy: 0.9495 - val_loss: 0.2154\n",
      "Epoch 14/100\n",
      "235/235 - 1s - 3ms/step - accuracy: 0.9739 - loss: 0.0830 - val_accuracy: 0.9484 - val_loss: 0.2297\n",
      "Epoch 15/100\n",
      "235/235 - 1s - 2ms/step - accuracy: 0.9739 - loss: 0.0804 - val_accuracy: 0.9529 - val_loss: 0.2093\n",
      "Epoch 16/100\n",
      "235/235 - 1s - 2ms/step - accuracy: 0.9771 - loss: 0.0698 - val_accuracy: 0.9570 - val_loss: 0.2099\n",
      "Epoch 17/100\n",
      "235/235 - 1s - 3ms/step - accuracy: 0.9751 - loss: 0.0784 - val_accuracy: 0.9470 - val_loss: 0.2415\n",
      "Epoch 18/100\n",
      "235/235 - 1s - 3ms/step - accuracy: 0.9777 - loss: 0.0698 - val_accuracy: 0.9535 - val_loss: 0.2250\n",
      "Epoch 19/100\n",
      "235/235 - 1s - 3ms/step - accuracy: 0.9798 - loss: 0.0633 - val_accuracy: 0.9542 - val_loss: 0.2344\n",
      "Epoch 20/100\n",
      "235/235 - 1s - 3ms/step - accuracy: 0.9775 - loss: 0.0725 - val_accuracy: 0.9565 - val_loss: 0.2120\n",
      "Epoch 21/100\n",
      "235/235 - 1s - 2ms/step - accuracy: 0.9807 - loss: 0.0631 - val_accuracy: 0.9571 - val_loss: 0.2156\n",
      "Epoch 22/100\n",
      "235/235 - 1s - 3ms/step - accuracy: 0.9822 - loss: 0.0546 - val_accuracy: 0.9559 - val_loss: 0.2326\n",
      "Epoch 23/100\n",
      "235/235 - 1s - 3ms/step - accuracy: 0.9826 - loss: 0.0558 - val_accuracy: 0.9601 - val_loss: 0.2197\n",
      "Epoch 24/100\n",
      "235/235 - 1s - 3ms/step - accuracy: 0.9835 - loss: 0.0511 - val_accuracy: 0.9624 - val_loss: 0.2328\n",
      "Epoch 25/100\n",
      "235/235 - 1s - 3ms/step - accuracy: 0.9827 - loss: 0.0555 - val_accuracy: 0.9569 - val_loss: 0.2338\n",
      "Epoch 26/100\n",
      "235/235 - 1s - 3ms/step - accuracy: 0.9834 - loss: 0.0537 - val_accuracy: 0.9544 - val_loss: 0.2572\n",
      "Epoch 27/100\n",
      "235/235 - 1s - 3ms/step - accuracy: 0.9808 - loss: 0.0641 - val_accuracy: 0.9597 - val_loss: 0.2469\n",
      "Epoch 28/100\n",
      "235/235 - 1s - 3ms/step - accuracy: 0.9869 - loss: 0.0399 - val_accuracy: 0.9597 - val_loss: 0.2380\n",
      "Epoch 29/100\n",
      "235/235 - 1s - 3ms/step - accuracy: 0.9871 - loss: 0.0403 - val_accuracy: 0.9582 - val_loss: 0.2468\n",
      "Epoch 30/100\n",
      "235/235 - 1s - 3ms/step - accuracy: 0.9853 - loss: 0.0465 - val_accuracy: 0.9621 - val_loss: 0.2230\n",
      "Epoch 31/100\n",
      "235/235 - 1s - 2ms/step - accuracy: 0.9871 - loss: 0.0398 - val_accuracy: 0.9599 - val_loss: 0.2526\n",
      "Epoch 32/100\n",
      "235/235 - 1s - 2ms/step - accuracy: 0.9878 - loss: 0.0390 - val_accuracy: 0.9620 - val_loss: 0.2613\n",
      "Epoch 33/100\n",
      "235/235 - 1s - 3ms/step - accuracy: 0.9875 - loss: 0.0383 - val_accuracy: 0.9628 - val_loss: 0.2380\n",
      "Epoch 34/100\n",
      "235/235 - 1s - 3ms/step - accuracy: 0.9894 - loss: 0.0323 - val_accuracy: 0.9617 - val_loss: 0.2520\n",
      "Epoch 35/100\n",
      "235/235 - 1s - 2ms/step - accuracy: 0.9868 - loss: 0.0445 - val_accuracy: 0.9607 - val_loss: 0.2680\n",
      "Epoch 36/100\n",
      "235/235 - 1s - 3ms/step - accuracy: 0.9873 - loss: 0.0410 - val_accuracy: 0.9609 - val_loss: 0.2809\n",
      "Epoch 37/100\n",
      "235/235 - 1s - 3ms/step - accuracy: 0.9883 - loss: 0.0379 - val_accuracy: 0.9630 - val_loss: 0.2402\n",
      "Epoch 38/100\n",
      "235/235 - 1s - 3ms/step - accuracy: 0.9883 - loss: 0.0371 - val_accuracy: 0.9638 - val_loss: 0.2529\n",
      "Epoch 39/100\n",
      "235/235 - 1s - 3ms/step - accuracy: 0.9899 - loss: 0.0317 - val_accuracy: 0.9624 - val_loss: 0.2485\n",
      "Epoch 40/100\n",
      "235/235 - 1s - 3ms/step - accuracy: 0.9910 - loss: 0.0286 - val_accuracy: 0.9624 - val_loss: 0.2561\n",
      "Epoch 41/100\n",
      "235/235 - 1s - 2ms/step - accuracy: 0.9917 - loss: 0.0259 - val_accuracy: 0.9631 - val_loss: 0.2499\n",
      "Epoch 42/100\n",
      "235/235 - 1s - 3ms/step - accuracy: 0.9897 - loss: 0.0349 - val_accuracy: 0.9619 - val_loss: 0.2558\n",
      "Epoch 43/100\n",
      "235/235 - 1s - 2ms/step - accuracy: 0.9906 - loss: 0.0318 - val_accuracy: 0.9648 - val_loss: 0.2623\n",
      "Epoch 44/100\n",
      "235/235 - 1s - 3ms/step - accuracy: 0.9884 - loss: 0.0377 - val_accuracy: 0.9629 - val_loss: 0.2700\n",
      "Epoch 45/100\n",
      "235/235 - 1s - 2ms/step - accuracy: 0.9904 - loss: 0.0323 - val_accuracy: 0.9650 - val_loss: 0.2587\n",
      "Epoch 46/100\n",
      "235/235 - 1s - 2ms/step - accuracy: 0.9921 - loss: 0.0264 - val_accuracy: 0.9671 - val_loss: 0.2467\n",
      "Epoch 47/100\n",
      "235/235 - 1s - 2ms/step - accuracy: 0.9941 - loss: 0.0191 - val_accuracy: 0.9659 - val_loss: 0.2589\n",
      "Epoch 48/100\n",
      "235/235 - 1s - 2ms/step - accuracy: 0.9919 - loss: 0.0274 - val_accuracy: 0.9639 - val_loss: 0.2655\n",
      "Epoch 49/100\n",
      "235/235 - 1s - 2ms/step - accuracy: 0.9907 - loss: 0.0323 - val_accuracy: 0.9646 - val_loss: 0.2608\n",
      "Epoch 50/100\n",
      "235/235 - 1s - 2ms/step - accuracy: 0.9916 - loss: 0.0266 - val_accuracy: 0.9653 - val_loss: 0.2725\n",
      "Epoch 51/100\n",
      "235/235 - 1s - 3ms/step - accuracy: 0.9918 - loss: 0.0270 - val_accuracy: 0.9656 - val_loss: 0.2686\n",
      "Epoch 52/100\n",
      "235/235 - 1s - 2ms/step - accuracy: 0.9931 - loss: 0.0224 - val_accuracy: 0.9642 - val_loss: 0.2701\n",
      "Epoch 53/100\n",
      "235/235 - 1s - 2ms/step - accuracy: 0.9944 - loss: 0.0193 - val_accuracy: 0.9665 - val_loss: 0.2732\n",
      "Epoch 54/100\n",
      "235/235 - 1s - 2ms/step - accuracy: 0.9926 - loss: 0.0260 - val_accuracy: 0.9641 - val_loss: 0.2900\n",
      "Epoch 55/100\n",
      "235/235 - 1s - 2ms/step - accuracy: 0.9933 - loss: 0.0232 - val_accuracy: 0.9648 - val_loss: 0.2634\n",
      "Epoch 56/100\n",
      "235/235 - 1s - 2ms/step - accuracy: 0.9930 - loss: 0.0251 - val_accuracy: 0.9630 - val_loss: 0.3030\n",
      "Epoch 57/100\n",
      "235/235 - 1s - 2ms/step - accuracy: 0.9919 - loss: 0.0297 - val_accuracy: 0.9672 - val_loss: 0.2953\n",
      "Epoch 58/100\n",
      "235/235 - 1s - 2ms/step - accuracy: 0.9915 - loss: 0.0313 - val_accuracy: 0.9627 - val_loss: 0.3155\n",
      "Epoch 59/100\n",
      "235/235 - 1s - 2ms/step - accuracy: 0.9937 - loss: 0.0205 - val_accuracy: 0.9658 - val_loss: 0.2955\n",
      "Epoch 60/100\n",
      "235/235 - 1s - 3ms/step - accuracy: 0.9937 - loss: 0.0215 - val_accuracy: 0.9678 - val_loss: 0.2768\n",
      "Epoch 61/100\n",
      "235/235 - 1s - 2ms/step - accuracy: 0.9961 - loss: 0.0122 - val_accuracy: 0.9662 - val_loss: 0.2824\n",
      "Epoch 62/100\n",
      "235/235 - 1s - 3ms/step - accuracy: 0.9955 - loss: 0.0143 - val_accuracy: 0.9642 - val_loss: 0.3069\n",
      "Epoch 63/100\n",
      "235/235 - 1s - 3ms/step - accuracy: 0.9926 - loss: 0.0254 - val_accuracy: 0.9650 - val_loss: 0.2791\n",
      "Epoch 64/100\n",
      "235/235 - 1s - 3ms/step - accuracy: 0.9918 - loss: 0.0296 - val_accuracy: 0.9644 - val_loss: 0.2891\n",
      "Epoch 65/100\n",
      "235/235 - 1s - 3ms/step - accuracy: 0.9947 - loss: 0.0207 - val_accuracy: 0.9644 - val_loss: 0.3000\n",
      "Epoch 66/100\n",
      "235/235 - 1s - 3ms/step - accuracy: 0.9939 - loss: 0.0242 - val_accuracy: 0.9670 - val_loss: 0.2542\n",
      "Epoch 67/100\n",
      "235/235 - 1s - 3ms/step - accuracy: 0.9938 - loss: 0.0212 - val_accuracy: 0.9687 - val_loss: 0.2540\n",
      "Epoch 68/100\n",
      "235/235 - 1s - 3ms/step - accuracy: 0.9953 - loss: 0.0159 - val_accuracy: 0.9679 - val_loss: 0.2609\n",
      "Epoch 69/100\n",
      "235/235 - 1s - 3ms/step - accuracy: 0.9959 - loss: 0.0133 - val_accuracy: 0.9662 - val_loss: 0.2897\n",
      "Epoch 70/100\n",
      "235/235 - 1s - 2ms/step - accuracy: 0.9947 - loss: 0.0189 - val_accuracy: 0.9681 - val_loss: 0.2654\n",
      "Epoch 71/100\n",
      "235/235 - 1s - 3ms/step - accuracy: 0.9951 - loss: 0.0167 - val_accuracy: 0.9665 - val_loss: 0.2881\n",
      "Epoch 72/100\n",
      "235/235 - 1s - 3ms/step - accuracy: 0.9947 - loss: 0.0185 - val_accuracy: 0.9656 - val_loss: 0.3189\n",
      "Epoch 73/100\n",
      "235/235 - 1s - 3ms/step - accuracy: 0.9922 - loss: 0.0307 - val_accuracy: 0.9637 - val_loss: 0.3035\n",
      "Epoch 74/100\n",
      "235/235 - 1s - 2ms/step - accuracy: 0.9933 - loss: 0.0243 - val_accuracy: 0.9664 - val_loss: 0.2820\n",
      "Epoch 75/100\n",
      "235/235 - 1s - 2ms/step - accuracy: 0.9957 - loss: 0.0138 - val_accuracy: 0.9628 - val_loss: 0.3225\n",
      "Epoch 76/100\n",
      "235/235 - 1s - 3ms/step - accuracy: 0.9954 - loss: 0.0171 - val_accuracy: 0.9645 - val_loss: 0.3241\n",
      "Epoch 77/100\n",
      "235/235 - 1s - 3ms/step - accuracy: 0.9934 - loss: 0.0241 - val_accuracy: 0.9641 - val_loss: 0.3190\n",
      "Epoch 78/100\n",
      "235/235 - 1s - 3ms/step - accuracy: 0.9954 - loss: 0.0159 - val_accuracy: 0.9682 - val_loss: 0.2742\n",
      "Epoch 79/100\n",
      "235/235 - 1s - 3ms/step - accuracy: 0.9954 - loss: 0.0151 - val_accuracy: 0.9684 - val_loss: 0.2860\n",
      "Epoch 80/100\n",
      "235/235 - 1s - 3ms/step - accuracy: 0.9969 - loss: 0.0100 - val_accuracy: 0.9671 - val_loss: 0.3202\n",
      "Epoch 81/100\n",
      "235/235 - 1s - 3ms/step - accuracy: 0.9973 - loss: 0.0096 - val_accuracy: 0.9688 - val_loss: 0.3117\n",
      "Epoch 82/100\n",
      "235/235 - 1s - 2ms/step - accuracy: 0.9948 - loss: 0.0204 - val_accuracy: 0.9633 - val_loss: 0.3429\n",
      "Epoch 83/100\n",
      "235/235 - 1s - 2ms/step - accuracy: 0.9906 - loss: 0.0381 - val_accuracy: 0.9682 - val_loss: 0.2658\n",
      "Epoch 84/100\n",
      "235/235 - 1s - 3ms/step - accuracy: 0.9959 - loss: 0.0142 - val_accuracy: 0.9682 - val_loss: 0.2839\n",
      "Epoch 85/100\n",
      "235/235 - 1s - 3ms/step - accuracy: 0.9963 - loss: 0.0119 - val_accuracy: 0.9690 - val_loss: 0.2779\n",
      "Epoch 86/100\n",
      "235/235 - 1s - 3ms/step - accuracy: 0.9962 - loss: 0.0122 - val_accuracy: 0.9682 - val_loss: 0.2911\n",
      "Epoch 87/100\n",
      "235/235 - 1s - 3ms/step - accuracy: 0.9970 - loss: 0.0098 - val_accuracy: 0.9695 - val_loss: 0.2879\n",
      "Epoch 88/100\n",
      "235/235 - 1s - 3ms/step - accuracy: 0.9962 - loss: 0.0135 - val_accuracy: 0.9662 - val_loss: 0.3237\n",
      "Epoch 89/100\n",
      "235/235 - 1s - 3ms/step - accuracy: 0.9956 - loss: 0.0146 - val_accuracy: 0.9688 - val_loss: 0.2948\n",
      "Epoch 90/100\n",
      "235/235 - 1s - 3ms/step - accuracy: 0.9954 - loss: 0.0171 - val_accuracy: 0.9651 - val_loss: 0.3337\n",
      "Epoch 91/100\n",
      "235/235 - 1s - 3ms/step - accuracy: 0.9942 - loss: 0.0213 - val_accuracy: 0.9690 - val_loss: 0.2971\n",
      "Epoch 92/100\n",
      "235/235 - 1s - 3ms/step - accuracy: 0.9948 - loss: 0.0197 - val_accuracy: 0.9676 - val_loss: 0.2863\n",
      "Epoch 93/100\n",
      "235/235 - 1s - 3ms/step - accuracy: 0.9960 - loss: 0.0135 - val_accuracy: 0.9676 - val_loss: 0.3038\n",
      "Epoch 94/100\n",
      "235/235 - 1s - 3ms/step - accuracy: 0.9959 - loss: 0.0155 - val_accuracy: 0.9697 - val_loss: 0.2813\n",
      "Epoch 95/100\n",
      "235/235 - 1s - 2ms/step - accuracy: 0.9961 - loss: 0.0131 - val_accuracy: 0.9673 - val_loss: 0.2958\n",
      "Epoch 96/100\n",
      "235/235 - 1s - 2ms/step - accuracy: 0.9957 - loss: 0.0152 - val_accuracy: 0.9700 - val_loss: 0.2907\n",
      "Epoch 97/100\n",
      "235/235 - 1s - 2ms/step - accuracy: 0.9959 - loss: 0.0138 - val_accuracy: 0.9692 - val_loss: 0.3261\n",
      "Epoch 98/100\n",
      "235/235 - 1s - 2ms/step - accuracy: 0.9966 - loss: 0.0126 - val_accuracy: 0.9700 - val_loss: 0.3108\n",
      "Epoch 99/100\n",
      "235/235 - 1s - 2ms/step - accuracy: 0.9947 - loss: 0.0208 - val_accuracy: 0.9711 - val_loss: 0.3050\n",
      "Epoch 100/100\n",
      "235/235 - 1s - 2ms/step - accuracy: 0.9979 - loss: 0.0071 - val_accuracy: 0.9705 - val_loss: 0.2988\n"
     ]
    }
   ],
   "source": [
    "#history = model.fit(x_train, y_train, batch_size=256, epochs=100, validation_split=0.3, verbose=2)\n",
    "history = model.fit(x_train, y_train, batch_size=256, epochs=100, validation_data=(x_test, y_test), verbose=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
