{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate two matrix multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matrix multiplication: [[-29.699999  14.599999]\n",
      " [ 36.       -20.      ]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "# create two 2*2 constand matrix and do matrix multiplication\n",
    "#[[-1, 2.1]]\n",
    "#[[1,-3]]\n",
    "\n",
    "#[[15, -2]]\n",
    "#[[-7, 6]]\n",
    "\n",
    "\n",
    "matrix1 = tf.constant([[-1, 2.1],[1, -3]], name='matrix1', dtype=tf.float32)\n",
    "matrix2 = tf.constant([[15, -2],[-7, 6]], name='matrix2', dtype=tf.float32)\n",
    "\n",
    "result = tf.linalg.matmul(matrix1, matrix2)\n",
    "\n",
    "print(\"matrix multiplication: {}\\n\".format(result))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate two matrix element-wise multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matrix element-wise multiplication: [[-15.   -4.2]\n",
      " [ -7.  -18. ]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# create two 2*2 constand matrix and do element-wise multiplication\n",
    "#[[-1, 2.1]]\n",
    "#[[1,-3]]\n",
    "\n",
    "#[[15, -2]]\n",
    "#[[-7, 6]]\n",
    "\n",
    "matrix1 = tf.constant([[-1, 2.1],[1, -3]], name='matrix1', dtype=tf.float32)\n",
    "matrix2 = tf.constant([[15, -2],[-7, 6]], name='matrix2', dtype=tf.float32)\n",
    "\n",
    "result = tf.math.multiply(matrix1, matrix2)\n",
    "\n",
    "\n",
    "print(\"matrix element-wise multiplication: {}\\n\".format(result))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write a program that use placeholder to feed (x, y) and rotate 45 degree coordinates "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result is: [[0.707]\n",
      " [0.707]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "origin_coordinates = tf.constant([[1.0,0.0]])\n",
    "\n",
    "origin_coordinates_t = tf.linalg.matrix_transpose(origin_coordinates)\n",
    "transform_matrix = tf.constant([[0.707,-0.707],[0.707,0.707]])\n",
    "result_coordinates = tf.linalg.matmul(transform_matrix, origin_coordinates_t)\n",
    "\n",
    "print('result is: {}'.format(result_coordinates))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate manhattan distance between vector1 and vector2  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vector1: [1. 3.]\n",
      "vector2: [-2. -5.]\n",
      "manhattan distance is: 11.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "vector1 = tf.constant([1.0, 3.0])\n",
    "vector2 = tf.constant([-2.0, -5.0])\n",
    "\n",
    "diff = vector1 - vector2\n",
    "abs_diff = tf.math.abs(diff)\n",
    "result = tf.reduce_sum(abs_diff)\n",
    "\n",
    "print('vector1: {}'.format(vector1.numpy()))\n",
    "print('vector2: {}'.format(vector2.numpy()))\n",
    "print('manhattan distance is: {}'.format(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# minimize function f(x) = (x1)^2 - 3*(x1) + (x2)^2 (assume initial x1=10, x2=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x1 = 1.500000238418579, x2= 5.254662970084115e-38, f have min value -2.250000238418579\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def fu(x1, x2): \n",
    "    return x1 ** 2.0 - x1 * 3  + x2 ** 2\n",
    "def fu_minimzie():\n",
    "    return x1 ** 2.0 - x1 * 3  + x2 ** 2\n",
    "\n",
    "x1 = tf.Variable(10.0) \n",
    "x2 = tf.Variable(10.0) \n",
    "\n",
    "\n",
    "opt = tf.keras.optimizers.SGD(learning_rate=0.1)\n",
    "for i in range(1000):\n",
    "    with tf.GradientTape() as tape:\n",
    "        y = fu(x1, x2)\n",
    "    grads = tape.gradient(y, [x1, x2])\n",
    "    processed_grads = [g for g in grads]\n",
    "    grads_and_vars = zip(processed_grads, [x1, x2])\n",
    "    opt.apply_gradients(grads_and_vars)\n",
    "\n",
    "print('x1 = {}, x2= {}, f have min value {}'.format(x1.numpy(), x2.numpy(), y.numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear regression to  House Price Dataset\n",
    "ref: https://www.kaggle.com/c/boston-housing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Dependencies\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_boston\n",
    "import tensorflow as tf\n",
    "\n",
    "# Load Data\n",
    "# Load Dataset\n",
    "boston = load_boston()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1.0</td>\n",
       "      <td>296.0</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.33</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      CRIM    ZN INDUS CHAS    NOX     RM   AGE     DIS  RAD    TAX PTRATIO  \\\n",
       "0  0.00632  18.0  2.31  0.0  0.538  6.575  65.2  4.0900  1.0  296.0    15.3   \n",
       "1  0.02731   0.0  7.07  0.0  0.469  6.421  78.9  4.9671  2.0  242.0    17.8   \n",
       "2  0.02729   0.0  7.07  0.0  0.469  7.185  61.1  4.9671  2.0  242.0    17.8   \n",
       "3  0.03237   0.0  2.18  0.0  0.458  6.998  45.8  6.0622  3.0  222.0    18.7   \n",
       "4  0.06905   0.0  2.18  0.0  0.458  7.147  54.2  6.0622  3.0  222.0    18.7   \n",
       "\n",
       "        B LSTAT  \n",
       "0  396.90  4.98  \n",
       "1  396.90  9.14  \n",
       "2  392.83  4.03  \n",
       "3  394.63  2.94  \n",
       "4  396.90  5.33  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Seperate Data into Features and Labels and load them as a Pandas Dataframe\n",
    "# Features\n",
    "features_df = pd.DataFrame(np.array(boston.data), columns=[boston.feature_names])\n",
    "\n",
    "features_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>21.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>34.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>33.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>36.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   labels\n",
       "0    24.0\n",
       "1    21.6\n",
       "2    34.7\n",
       "3    33.4\n",
       "4    36.2"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Labels\n",
    "labels_df = pd.DataFrame(np.array(boston.target), columns=['labels'])\n",
    "labels_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (404, 13)\n",
      "y_train shape: (404, 1)\n",
      "X_test shape: (102, 13)\n",
      "y_test shape: (102, 1)\n"
     ]
    }
   ],
   "source": [
    "# Train Test Split\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Train Test Split\n",
    "# Training Data = 80% of Dataset\n",
    "# Test Data = 20% of Dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(features_df, labels_df, test_size=0.2, random_state=101)\n",
    "\n",
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)\n",
    "X_test = np.array(X_test)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "print('X_train shape: {}'.format(X_train.shape))\n",
    "print('y_train shape: {}'.format(y_train.shape))\n",
    "print('X_test shape: {}'.format(X_test.shape))\n",
    "print('y_test shape: {}'.format(y_test.shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "# Define the Preprocessing Method and Fit Training Data to it\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train_normal = scaler.transform(X_train)\n",
    "X_test_normal = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = tf.Variable(tf.random.normal([X_train.shape[1],1], seed=1))\n",
    "b = tf.Variable(tf.random.normal([1], seed=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 587.60107421875\n",
      "loss: 351.2682189941406\n",
      "loss: 230.2494659423828\n",
      "loss: 154.72976684570312\n",
      "loss: 106.66656494140625\n",
      "loss: 75.96826934814453\n",
      "loss: 56.312828063964844\n",
      "loss: 43.69415283203125\n",
      "loss: 35.5672721862793\n",
      "loss: 30.312728881835938\n",
      "loss: 26.898462295532227\n",
      "loss: 24.665807723999023\n",
      "loss: 23.193796157836914\n",
      "loss: 22.212968826293945\n",
      "loss: 21.55057144165039\n",
      "loss: 21.095630645751953\n",
      "loss: 20.776702880859375\n",
      "loss: 20.547672271728516\n",
      "loss: 20.3786678314209\n",
      "loss: 20.25026512145996\n",
      "loss: 20.149751663208008\n",
      "loss: 20.068767547607422\n",
      "loss: 20.001758575439453\n",
      "loss: 19.944988250732422\n",
      "loss: 19.895931243896484\n",
      "loss: 19.852832794189453\n",
      "loss: 19.8144588470459\n",
      "loss: 19.779926300048828\n",
      "loss: 19.748577117919922\n",
      "loss: 19.719928741455078\n",
      "loss: 19.693601608276367\n",
      "loss: 19.669296264648438\n",
      "loss: 19.646766662597656\n",
      "loss: 19.625823974609375\n",
      "loss: 19.60629653930664\n",
      "loss: 19.588045120239258\n",
      "loss: 19.570947647094727\n",
      "loss: 19.554903030395508\n",
      "loss: 19.539813995361328\n",
      "loss: 19.525606155395508\n",
      "loss: 19.5122013092041\n",
      "loss: 19.499540328979492\n",
      "loss: 19.487564086914062\n",
      "loss: 19.476221084594727\n",
      "loss: 19.465469360351562\n",
      "loss: 19.455259323120117\n",
      "loss: 19.4455623626709\n",
      "loss: 19.43633460998535\n",
      "loss: 19.427553176879883\n",
      "loss: 19.419187545776367\n",
      "loss: 19.41120719909668\n",
      "loss: 19.40359115600586\n",
      "loss: 19.396318435668945\n",
      "loss: 19.389366149902344\n",
      "loss: 19.382715225219727\n",
      "loss: 19.376352310180664\n",
      "loss: 19.370258331298828\n",
      "loss: 19.364418029785156\n",
      "loss: 19.35881996154785\n",
      "loss: 19.35344886779785\n",
      "loss: 19.34829330444336\n",
      "loss: 19.34334373474121\n",
      "loss: 19.33858871459961\n",
      "loss: 19.334014892578125\n",
      "loss: 19.329622268676758\n",
      "loss: 19.325395584106445\n",
      "loss: 19.321325302124023\n",
      "loss: 19.317405700683594\n",
      "loss: 19.31363296508789\n",
      "loss: 19.30999755859375\n",
      "loss: 19.306493759155273\n",
      "loss: 19.303117752075195\n",
      "loss: 19.29986000061035\n",
      "loss: 19.296716690063477\n",
      "loss: 19.293682098388672\n",
      "loss: 19.290754318237305\n",
      "loss: 19.287927627563477\n",
      "loss: 19.285198211669922\n",
      "loss: 19.28255844116211\n",
      "loss: 19.280010223388672\n",
      "loss: 19.277545928955078\n",
      "loss: 19.275163650512695\n",
      "loss: 19.27286148071289\n",
      "loss: 19.2706356048584\n",
      "loss: 19.26848030090332\n",
      "loss: 19.266393661499023\n",
      "loss: 19.264373779296875\n",
      "loss: 19.262420654296875\n",
      "loss: 19.260530471801758\n",
      "loss: 19.258697509765625\n",
      "loss: 19.256925582885742\n",
      "loss: 19.25520896911621\n",
      "loss: 19.253543853759766\n",
      "loss: 19.251934051513672\n",
      "loss: 19.2503719329834\n",
      "loss: 19.248857498168945\n",
      "loss: 19.247390747070312\n",
      "loss: 19.245967864990234\n",
      "loss: 19.244590759277344\n",
      "loss: 19.24325180053711\n",
      "loss: 19.241954803466797\n",
      "loss: 19.240697860717773\n",
      "loss: 19.239477157592773\n",
      "loss: 19.23829460144043\n",
      "loss: 19.237146377563477\n",
      "loss: 19.236034393310547\n",
      "loss: 19.234954833984375\n",
      "loss: 19.233905792236328\n",
      "loss: 19.23288917541504\n",
      "loss: 19.231901168823242\n",
      "loss: 19.230941772460938\n",
      "loss: 19.23001480102539\n",
      "loss: 19.229110717773438\n",
      "loss: 19.228233337402344\n",
      "loss: 19.22738265991211\n",
      "loss: 19.226558685302734\n",
      "loss: 19.22575569152832\n",
      "loss: 19.224977493286133\n",
      "loss: 19.224220275878906\n",
      "loss: 19.223485946655273\n",
      "loss: 19.222774505615234\n",
      "loss: 19.22208023071289\n",
      "loss: 19.22140884399414\n",
      "loss: 19.220754623413086\n",
      "loss: 19.220121383666992\n",
      "loss: 19.21950340270996\n",
      "loss: 19.21890640258789\n",
      "loss: 19.218324661254883\n",
      "loss: 19.217758178710938\n",
      "loss: 19.217212677001953\n",
      "loss: 19.216676712036133\n",
      "loss: 19.216161727905273\n",
      "loss: 19.215656280517578\n",
      "loss: 19.215167999267578\n",
      "loss: 19.214691162109375\n",
      "loss: 19.214231491088867\n",
      "loss: 19.213783264160156\n",
      "loss: 19.213348388671875\n",
      "loss: 19.212923049926758\n",
      "loss: 19.212512969970703\n",
      "loss: 19.212114334106445\n",
      "loss: 19.21172523498535\n",
      "loss: 19.211345672607422\n",
      "loss: 19.210979461669922\n",
      "loss: 19.210622787475586\n",
      "loss: 19.210275650024414\n",
      "loss: 19.20993995666504\n",
      "loss: 19.209611892700195\n",
      "loss: 19.209293365478516\n",
      "loss: 19.208984375\n",
      "loss: 19.20868492126465\n",
      "loss: 19.208393096923828\n",
      "loss: 19.20810890197754\n",
      "loss: 19.20783042907715\n",
      "loss: 19.207563400268555\n",
      "loss: 19.207304000854492\n",
      "loss: 19.207050323486328\n",
      "loss: 19.206802368164062\n",
      "loss: 19.20656394958496\n",
      "loss: 19.20633316040039\n",
      "loss: 19.206106185913086\n",
      "loss: 19.20588493347168\n",
      "loss: 19.205673217773438\n",
      "loss: 19.205463409423828\n",
      "loss: 19.205263137817383\n",
      "loss: 19.20506477355957\n",
      "loss: 19.204875946044922\n",
      "loss: 19.20469093322754\n",
      "loss: 19.204509735107422\n",
      "loss: 19.204336166381836\n",
      "loss: 19.204164505004883\n",
      "loss: 19.203998565673828\n",
      "loss: 19.20383644104004\n",
      "loss: 19.20368003845215\n",
      "loss: 19.203529357910156\n",
      "loss: 19.203380584716797\n",
      "loss: 19.203237533569336\n",
      "loss: 19.203096389770508\n",
      "loss: 19.202960968017578\n",
      "loss: 19.202829360961914\n",
      "loss: 19.20270347595215\n",
      "loss: 19.20257568359375\n",
      "loss: 19.202455520629883\n",
      "loss: 19.20233726501465\n",
      "loss: 19.20222282409668\n",
      "loss: 19.202112197875977\n",
      "loss: 19.202001571655273\n",
      "loss: 19.2018985748291\n",
      "loss: 19.201797485351562\n",
      "loss: 19.201698303222656\n",
      "loss: 19.201601028442383\n",
      "loss: 19.201505661010742\n",
      "loss: 19.201416015625\n",
      "loss: 19.201324462890625\n",
      "loss: 19.20123863220215\n",
      "loss: 19.201156616210938\n",
      "loss: 19.201074600219727\n",
      "loss: 19.20099449157715\n",
      "loss: 19.200916290283203\n",
      "loss: 19.200841903686523\n",
      "loss: 19.200767517089844\n",
      "loss: 19.20069694519043\n",
      "loss: 19.20062828063965\n",
      "loss: 19.200563430786133\n",
      "loss: 19.200496673583984\n",
      "loss: 19.2004337310791\n",
      "loss: 19.20037078857422\n",
      "loss: 19.2003116607666\n",
      "loss: 19.200254440307617\n",
      "loss: 19.200197219848633\n",
      "loss: 19.20014190673828\n",
      "loss: 19.200088500976562\n",
      "loss: 19.200037002563477\n",
      "loss: 19.199987411499023\n",
      "loss: 19.199935913085938\n",
      "loss: 19.19989013671875\n",
      "loss: 19.19984245300293\n",
      "loss: 19.199796676635742\n",
      "loss: 19.19975471496582\n",
      "loss: 19.1997127532959\n",
      "loss: 19.199668884277344\n",
      "loss: 19.199630737304688\n",
      "loss: 19.1995906829834\n",
      "loss: 19.19955062866211\n",
      "loss: 19.199514389038086\n",
      "loss: 19.199478149414062\n",
      "loss: 19.199443817138672\n",
      "loss: 19.19940757751465\n",
      "loss: 19.19937515258789\n",
      "loss: 19.199344635009766\n",
      "loss: 19.199312210083008\n",
      "loss: 19.199283599853516\n",
      "loss: 19.19925308227539\n",
      "loss: 19.1992244720459\n",
      "loss: 19.19919776916504\n",
      "loss: 19.199167251586914\n",
      "loss: 19.199142456054688\n",
      "loss: 19.19911766052246\n",
      "loss: 19.199092864990234\n",
      "loss: 19.199068069458008\n",
      "loss: 19.19904136657715\n",
      "loss: 19.199020385742188\n",
      "loss: 19.198999404907227\n",
      "loss: 19.198976516723633\n",
      "loss: 19.198957443237305\n",
      "loss: 19.198936462402344\n",
      "loss: 19.198915481567383\n",
      "loss: 19.198896408081055\n",
      "loss: 19.198877334594727\n",
      "loss: 19.198856353759766\n",
      "loss: 19.198841094970703\n",
      "loss: 19.198823928833008\n",
      "loss: 19.198806762695312\n",
      "loss: 19.19879150390625\n",
      "loss: 19.198774337768555\n",
      "loss: 19.198759078979492\n",
      "loss: 19.198745727539062\n",
      "loss: 19.198728561401367\n",
      "loss: 19.198715209960938\n",
      "loss: 19.198701858520508\n",
      "loss: 19.198686599731445\n",
      "loss: 19.19867515563965\n",
      "loss: 19.19866371154785\n",
      "loss: 19.198650360107422\n",
      "loss: 19.198637008666992\n",
      "loss: 19.198627471923828\n",
      "loss: 19.1986141204834\n",
      "loss: 19.1986026763916\n",
      "loss: 19.198591232299805\n",
      "loss: 19.198583602905273\n",
      "loss: 19.198572158813477\n",
      "loss: 19.198562622070312\n",
      "loss: 19.19855308532715\n",
      "loss: 19.198545455932617\n",
      "loss: 19.19853401184082\n",
      "loss: 19.198528289794922\n",
      "loss: 19.198516845703125\n",
      "loss: 19.198511123657227\n",
      "loss: 19.198501586914062\n",
      "loss: 19.1984920501709\n",
      "loss: 19.198486328125\n",
      "loss: 19.19847869873047\n",
      "loss: 19.198471069335938\n",
      "loss: 19.19846534729004\n",
      "loss: 19.198457717895508\n",
      "loss: 19.198450088500977\n",
      "loss: 19.198442459106445\n",
      "loss: 19.198434829711914\n",
      "loss: 19.19843101501465\n",
      "loss: 19.198423385620117\n",
      "loss: 19.19841957092285\n",
      "loss: 19.198413848876953\n",
      "loss: 19.198408126831055\n",
      "loss: 19.19840431213379\n",
      "loss: 19.198400497436523\n",
      "loss: 19.198394775390625\n",
      "loss: 19.198389053344727\n",
      "loss: 19.198383331298828\n",
      "loss: 19.198379516601562\n",
      "loss: 19.198373794555664\n",
      "loss: 19.1983699798584\n",
      "loss: 19.198366165161133\n",
      "loss: 19.198362350463867\n",
      "loss: 19.1983585357666\n",
      "loss: 19.198352813720703\n",
      "loss: 19.198348999023438\n",
      "loss: 19.198345184326172\n",
      "loss: 19.19834327697754\n",
      "loss: 19.198339462280273\n",
      "loss: 19.19833755493164\n",
      "loss: 19.198333740234375\n",
      "loss: 19.19832992553711\n",
      "loss: 19.198328018188477\n",
      "loss: 19.198322296142578\n",
      "loss: 19.198320388793945\n",
      "loss: 19.19831657409668\n",
      "loss: 19.198312759399414\n",
      "loss: 19.198312759399414\n",
      "loss: 19.19830894470215\n",
      "loss: 19.198307037353516\n",
      "loss: 19.19830322265625\n",
      "loss: 19.198301315307617\n",
      "loss: 19.198299407958984\n",
      "loss: 19.19829750061035\n",
      "loss: 19.19829559326172\n",
      "loss: 19.198293685913086\n",
      "loss: 19.198291778564453\n",
      "loss: 19.198287963867188\n",
      "loss: 19.198286056518555\n",
      "loss: 19.198286056518555\n",
      "loss: 19.198284149169922\n",
      "loss: 19.19828224182129\n",
      "loss: 19.198280334472656\n",
      "loss: 19.19827651977539\n",
      "loss: 19.198278427124023\n",
      "loss: 19.198274612426758\n",
      "loss: 19.198272705078125\n",
      "loss: 19.198270797729492\n",
      "loss: 19.198270797729492\n",
      "loss: 19.19826889038086\n",
      "loss: 19.198266983032227\n",
      "loss: 19.198266983032227\n",
      "loss: 19.198266983032227\n",
      "loss: 19.19826316833496\n",
      "loss: 19.19826316833496\n",
      "loss: 19.19826316833496\n",
      "loss: 19.198259353637695\n",
      "loss: 19.198261260986328\n",
      "loss: 19.198257446289062\n",
      "loss: 19.198257446289062\n",
      "loss: 19.19825553894043\n",
      "loss: 19.198253631591797\n",
      "loss: 19.198253631591797\n",
      "loss: 19.198251724243164\n",
      "loss: 19.198251724243164\n",
      "loss: 19.19824981689453\n",
      "loss: 19.1982479095459\n",
      "loss: 19.1982479095459\n",
      "loss: 19.1982479095459\n",
      "loss: 19.198246002197266\n",
      "loss: 19.198244094848633\n",
      "loss: 19.198246002197266\n",
      "loss: 19.198244094848633\n",
      "loss: 19.198244094848633\n",
      "loss: 19.1982421875\n",
      "loss: 19.198240280151367\n",
      "loss: 19.1982421875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 19.198240280151367\n",
      "loss: 19.198240280151367\n",
      "loss: 19.198240280151367\n",
      "loss: 19.198240280151367\n",
      "loss: 19.198238372802734\n",
      "loss: 19.198238372802734\n",
      "loss: 19.1982364654541\n",
      "loss: 19.1982364654541\n",
      "loss: 19.1982364654541\n",
      "loss: 19.19823455810547\n",
      "loss: 19.19823455810547\n",
      "loss: 19.19823455810547\n",
      "loss: 19.19823455810547\n",
      "loss: 19.19823455810547\n",
      "loss: 19.19823455810547\n",
      "loss: 19.198230743408203\n",
      "loss: 19.198232650756836\n",
      "loss: 19.198230743408203\n",
      "loss: 19.198230743408203\n",
      "loss: 19.198230743408203\n",
      "loss: 19.198230743408203\n",
      "loss: 19.198230743408203\n",
      "loss: 19.198230743408203\n",
      "loss: 19.198230743408203\n",
      "loss: 19.19822883605957\n",
      "loss: 19.198230743408203\n",
      "loss: 19.19822883605957\n",
      "loss: 19.19822883605957\n",
      "loss: 19.198226928710938\n",
      "loss: 19.19822883605957\n",
      "loss: 19.198226928710938\n",
      "loss: 19.19822883605957\n",
      "loss: 19.198226928710938\n",
      "loss: 19.198226928710938\n",
      "loss: 19.19822883605957\n",
      "loss: 19.198226928710938\n",
      "loss: 19.198226928710938\n",
      "loss: 19.198226928710938\n",
      "loss: 19.198225021362305\n",
      "loss: 19.198226928710938\n",
      "loss: 19.198225021362305\n",
      "loss: 19.198226928710938\n",
      "loss: 19.198223114013672\n",
      "loss: 19.198225021362305\n",
      "loss: 19.198223114013672\n",
      "loss: 19.198223114013672\n",
      "loss: 19.198223114013672\n",
      "loss: 19.198225021362305\n",
      "loss: 19.198223114013672\n",
      "loss: 19.198223114013672\n",
      "loss: 19.198223114013672\n",
      "loss: 19.198223114013672\n",
      "loss: 19.198223114013672\n",
      "loss: 19.198223114013672\n",
      "loss: 19.198223114013672\n",
      "loss: 19.198223114013672\n",
      "loss: 19.198223114013672\n",
      "loss: 19.198223114013672\n",
      "loss: 19.19822120666504\n",
      "loss: 19.19822120666504\n",
      "loss: 19.19822120666504\n",
      "loss: 19.198223114013672\n",
      "loss: 19.198223114013672\n",
      "loss: 19.19822120666504\n",
      "loss: 19.19822120666504\n",
      "loss: 19.19822120666504\n",
      "loss: 19.19822120666504\n",
      "loss: 19.198223114013672\n",
      "loss: 19.19822120666504\n",
      "loss: 19.19822120666504\n",
      "loss: 19.19822120666504\n",
      "loss: 19.198219299316406\n",
      "loss: 19.19822120666504\n",
      "loss: 19.19822120666504\n",
      "loss: 19.19822120666504\n",
      "loss: 19.19822120666504\n",
      "loss: 19.19822120666504\n",
      "loss: 19.19822120666504\n",
      "loss: 19.198223114013672\n",
      "loss: 19.19822120666504\n",
      "loss: 19.19822120666504\n",
      "loss: 19.19822120666504\n",
      "loss: 19.19822120666504\n",
      "loss: 19.19822120666504\n",
      "loss: 19.19822120666504\n",
      "loss: 19.198219299316406\n",
      "loss: 19.19822120666504\n",
      "loss: 19.19822120666504\n",
      "loss: 19.198219299316406\n",
      "loss: 19.19822120666504\n",
      "loss: 19.198219299316406\n",
      "loss: 19.198219299316406\n",
      "loss: 19.19822120666504\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198219299316406\n",
      "loss: 19.198219299316406\n",
      "loss: 19.198219299316406\n",
      "loss: 19.19822120666504\n",
      "loss: 19.198219299316406\n",
      "loss: 19.19822120666504\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198219299316406\n",
      "loss: 19.19822120666504\n",
      "loss: 19.198219299316406\n",
      "loss: 19.198219299316406\n",
      "loss: 19.198219299316406\n",
      "loss: 19.198219299316406\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198219299316406\n",
      "loss: 19.198219299316406\n",
      "loss: 19.19822120666504\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198219299316406\n",
      "loss: 19.198219299316406\n",
      "loss: 19.198219299316406\n",
      "loss: 19.198219299316406\n",
      "loss: 19.198219299316406\n",
      "loss: 19.198219299316406\n",
      "loss: 19.198219299316406\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198219299316406\n",
      "loss: 19.198219299316406\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.19822120666504\n",
      "loss: 19.198219299316406\n",
      "loss: 19.198219299316406\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198219299316406\n",
      "loss: 19.198219299316406\n",
      "loss: 19.198219299316406\n",
      "loss: 19.198219299316406\n",
      "loss: 19.198219299316406\n",
      "loss: 19.19822120666504\n",
      "loss: 19.19821548461914\n",
      "loss: 19.198219299316406\n",
      "loss: 19.198219299316406\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198219299316406\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198219299316406\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198219299316406\n",
      "loss: 19.198219299316406\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198219299316406\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198219299316406\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198219299316406\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.19822120666504\n",
      "loss: 19.198219299316406\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198219299316406\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198219299316406\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198219299316406\n",
      "loss: 19.198217391967773\n",
      "loss: 19.19822120666504\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198219299316406\n",
      "loss: 19.198219299316406\n",
      "loss: 19.198219299316406\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198219299316406\n",
      "loss: 19.198219299316406\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198219299316406\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.19821548461914\n",
      "loss: 19.198219299316406\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198219299316406\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198219299316406\n",
      "loss: 19.198219299316406\n",
      "loss: 19.198219299316406\n",
      "loss: 19.198219299316406\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.19821548461914\n",
      "loss: 19.19821548461914\n",
      "loss: 19.198219299316406\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198219299316406\n",
      "loss: 19.198219299316406\n",
      "loss: 19.198219299316406\n",
      "loss: 19.198219299316406\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198219299316406\n",
      "loss: 19.198219299316406\n",
      "loss: 19.198219299316406\n",
      "loss: 19.198219299316406\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.19822120666504\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198219299316406\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198219299316406\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198219299316406\n",
      "loss: 19.198219299316406\n",
      "loss: 19.198219299316406\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198219299316406\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198219299316406\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.19821548461914\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198219299316406\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198219299316406\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198219299316406\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198219299316406\n",
      "loss: 19.198219299316406\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198219299316406\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198219299316406\n",
      "loss: 19.198219299316406\n",
      "loss: 19.198219299316406\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198219299316406\n",
      "loss: 19.198219299316406\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198219299316406\n",
      "loss: 19.198219299316406\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198219299316406\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198219299316406\n",
      "loss: 19.198219299316406\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198219299316406\n",
      "loss: 19.198219299316406\n",
      "loss: 19.198219299316406\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198219299316406\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198219299316406\n",
      "loss: 19.198219299316406\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198219299316406\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198219299316406\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198219299316406\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198219299316406\n",
      "loss: 19.198219299316406\n",
      "loss: 19.198219299316406\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198219299316406\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198219299316406\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198219299316406\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198219299316406\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198219299316406\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198219299316406\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198219299316406\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198219299316406\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198219299316406\n",
      "loss: 19.198219299316406\n",
      "loss: 19.198219299316406\n",
      "loss: 19.198219299316406\n",
      "loss: 19.198219299316406\n",
      "loss: 19.198219299316406\n",
      "loss: 19.198219299316406\n",
      "loss: 19.19822120666504\n",
      "loss: 19.198219299316406\n",
      "loss: 19.198219299316406\n",
      "loss: 19.198219299316406\n",
      "loss: 19.198219299316406\n",
      "loss: 19.198219299316406\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198219299316406\n",
      "loss: 19.198219299316406\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198219299316406\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198219299316406\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n",
      "loss: 19.198217391967773\n"
     ]
    }
   ],
   "source": [
    "\n",
    "opt = tf.keras.optimizers.SGD(learning_rate=0.1)\n",
    "for i in range(1000):\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        y_pred = tf.linalg.matmul(tf.cast(X_train_normal, dtype=tf.float32), W) + b\n",
    "        loss = tf.reduce_mean(tf.square(y_pred - y_train))\n",
    "        print('loss: {}'.format(loss))\n",
    "    grads = tape.gradient(loss, [W, b])\n",
    "    processed_grads = [g for g in grads]\n",
    "    grads_and_vars = zip(processed_grads, [W, b])\n",
    "    \n",
    "    opt.apply_gradients(grads_and_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new_W: [[-0.79122496]\n",
      " [ 0.95297587]\n",
      " [ 0.11484008]\n",
      " [ 0.7652688 ]\n",
      " [-1.7739142 ]\n",
      " [ 2.703863  ]\n",
      " [-0.02347507]\n",
      " [-3.0118444 ]\n",
      " [ 2.3180292 ]\n",
      " [-1.7692683 ]\n",
      " [-1.9079447 ]\n",
      " [ 0.7569065 ]\n",
      " [-3.7553952 ]]\n",
      "new_b: [22.336878]\n"
     ]
    }
   ],
   "source": [
    "print('new_W: {}'.format(W.numpy()))\n",
    "print('new_b: {}'.format(b.numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing data mse: 33.268924713134766\n"
     ]
    }
   ],
   "source": [
    "y_test_pred = tf.linalg.matmul(tf.cast(X_test_normal, dtype=tf.float32), W) + b\n",
    "loss = tf.reduce_mean(tf.square(y_test_pred - y_test))\n",
    "print('testing data mse: {}'.format(loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
