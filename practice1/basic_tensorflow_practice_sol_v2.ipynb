{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate two matrix multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matrix multiplication: [[-29.699999  14.599999]\n",
      " [ 36.       -20.      ]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "# create two 2*2 constand matrix and do matrix multiplication\n",
    "#[[-1, 2.1]]\n",
    "#[[1,-3]]\n",
    "\n",
    "#[[15, -2]]\n",
    "#[[-7, 6]]\n",
    "\n",
    "\n",
    "matrix1 = tf.constant([[-1, 2.1],[1, -3]], name='matrix1', dtype=tf.float32)\n",
    "matrix2 = tf.constant([[15, -2],[-7, 6]], name='matrix2', dtype=tf.float32)\n",
    "\n",
    "result = tf.linalg.matmul(matrix1, matrix2)\n",
    "\n",
    "print(\"matrix multiplication: {}\\n\".format(result))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate two matrix element-wise multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matrix element-wise multiplication: [[-15.   -4.2]\n",
      " [ -7.  -18. ]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# create two 2*2 constand matrix and do element-wise multiplication\n",
    "#[[-1, 2.1]]\n",
    "#[[1,-3]]\n",
    "\n",
    "#[[15, -2]]\n",
    "#[[-7, 6]]\n",
    "\n",
    "matrix1 = tf.constant([[-1, 2.1],[1, -3]], name='matrix1', dtype=tf.float32)\n",
    "matrix2 = tf.constant([[15, -2],[-7, 6]], name='matrix2', dtype=tf.float32)\n",
    "\n",
    "result = tf.math.multiply(matrix1, matrix2)\n",
    "\n",
    "\n",
    "print(\"matrix element-wise multiplication: {}\\n\".format(result))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write a program that use placeholder to feed (x, y) and rotate 45 degree coordinates "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result is: [[0.707]\n",
      " [0.707]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "origin_coordinates = tf.constant([[1.0,0.0]])\n",
    "\n",
    "origin_coordinates_t = tf.linalg.matrix_transpose(origin_coordinates)\n",
    "transform_matrix = tf.constant([[0.707,-0.707],[0.707,0.707]])\n",
    "result_coordinates = tf.linalg.matmul(transform_matrix, origin_coordinates_t)\n",
    "\n",
    "print('result is: {}'.format(result_coordinates))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate manhattan distance between vector1 and vector2  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vector1: [1. 3.]\n",
      "vector2: [-2. -5.]\n",
      "manhattan distance is: 11.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "vector1 = tf.constant([1.0, 3.0])\n",
    "vector2 = tf.constant([-2.0, -5.0])\n",
    "\n",
    "diff = vector1 - vector2\n",
    "abs_diff = tf.math.abs(diff)\n",
    "result = tf.reduce_sum(abs_diff)\n",
    "\n",
    "print('vector1: {}'.format(vector1.numpy()))\n",
    "print('vector2: {}'.format(vector2.numpy()))\n",
    "print('manhattan distance is: {}'.format(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# minimize function f(x) = (x1)^2 - 3*(x1) + (x2)^2 (assume initial x1=10, x2=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x1 = 1.500000238418579, x2= 5.254662970084115e-38, f have min value -2.250000238418579\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def fu(x1, x2): \n",
    "    return x1 ** 2.0 - x1 * 3  + x2 ** 2\n",
    "#def fu_minimzie():\n",
    "#    return x1 ** 2.0 - x1 * 3  + x2 ** 2\n",
    "\n",
    "x1 = tf.Variable(10.0) \n",
    "x2 = tf.Variable(10.0) \n",
    "\n",
    "\n",
    "opt = tf.keras.optimizers.SGD(learning_rate=0.1)\n",
    "for i in range(1000):\n",
    "    with tf.GradientTape() as tape:\n",
    "        y = fu(x1, x2)\n",
    "    grads = tape.gradient(y, [x1, x2])\n",
    "    processed_grads = [g for g in grads]\n",
    "    grads_and_vars = zip(processed_grads, [x1, x2])\n",
    "    opt.apply_gradients(grads_and_vars)\n",
    "\n",
    "print('x1 = {}, x2= {}, f have min value {}'.format(x1.numpy(), x2.numpy(), y.numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear regression to  House Price Dataset\n",
    "ref: https://www.kaggle.com/c/boston-housing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "      <th>MEDV</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1</td>\n",
       "      <td>296</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2</td>\n",
       "      <td>242</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "      <td>21.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2</td>\n",
       "      <td>242</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "      <td>34.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3</td>\n",
       "      <td>222</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "      <td>33.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.02985</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.430</td>\n",
       "      <td>58.7</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3</td>\n",
       "      <td>222</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.12</td>\n",
       "      <td>5.21</td>\n",
       "      <td>28.7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      CRIM    ZN  INDUS  CHAS    NOX     RM   AGE     DIS  RAD  TAX  PTRATIO  \\\n",
       "0  0.00632  18.0   2.31   0.0  0.538  6.575  65.2  4.0900    1  296     15.3   \n",
       "1  0.02731   0.0   7.07   0.0  0.469  6.421  78.9  4.9671    2  242     17.8   \n",
       "2  0.02729   0.0   7.07   0.0  0.469  7.185  61.1  4.9671    2  242     17.8   \n",
       "3  0.03237   0.0   2.18   0.0  0.458  6.998  45.8  6.0622    3  222     18.7   \n",
       "5  0.02985   0.0   2.18   0.0  0.458  6.430  58.7  6.0622    3  222     18.7   \n",
       "\n",
       "        B  LSTAT  MEDV  \n",
       "0  396.90   4.98  24.0  \n",
       "1  396.90   9.14  21.6  \n",
       "2  392.83   4.03  34.7  \n",
       "3  394.63   2.94  33.4  \n",
       "5  394.12   5.21  28.7  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import Dependencies\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "#from sklearn.datasets import load_boston\n",
    "import tensorflow as tf\n",
    "\n",
    "# Load Data\n",
    "# Load Dataset\n",
    "#boston = load_boston()\n",
    "df = pd.read_csv('./HousingData.csv')\n",
    "df = df.dropna()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_df = df[['MEDV']]\n",
    "features_df = df.drop('MEDV', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MEDV</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>21.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>34.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>33.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>28.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>17.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500</th>\n",
       "      <td>16.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>502</th>\n",
       "      <td>20.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>503</th>\n",
       "      <td>23.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>504</th>\n",
       "      <td>22.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>394 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     MEDV\n",
       "0    24.0\n",
       "1    21.6\n",
       "2    34.7\n",
       "3    33.4\n",
       "5    28.7\n",
       "..    ...\n",
       "499  17.5\n",
       "500  16.8\n",
       "502  20.6\n",
       "503  23.9\n",
       "504  22.0\n",
       "\n",
       "[394 rows x 1 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1</td>\n",
       "      <td>296</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2</td>\n",
       "      <td>242</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2</td>\n",
       "      <td>242</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3</td>\n",
       "      <td>222</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.02985</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.430</td>\n",
       "      <td>58.7</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3</td>\n",
       "      <td>222</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.12</td>\n",
       "      <td>5.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>0.17783</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.69</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.585</td>\n",
       "      <td>5.569</td>\n",
       "      <td>73.5</td>\n",
       "      <td>2.3999</td>\n",
       "      <td>6</td>\n",
       "      <td>391</td>\n",
       "      <td>19.2</td>\n",
       "      <td>395.77</td>\n",
       "      <td>15.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500</th>\n",
       "      <td>0.22438</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.69</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.585</td>\n",
       "      <td>6.027</td>\n",
       "      <td>79.7</td>\n",
       "      <td>2.4982</td>\n",
       "      <td>6</td>\n",
       "      <td>391</td>\n",
       "      <td>19.2</td>\n",
       "      <td>396.90</td>\n",
       "      <td>14.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>502</th>\n",
       "      <td>0.04527</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.120</td>\n",
       "      <td>76.7</td>\n",
       "      <td>2.2875</td>\n",
       "      <td>1</td>\n",
       "      <td>273</td>\n",
       "      <td>21.0</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>503</th>\n",
       "      <td>0.06076</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.976</td>\n",
       "      <td>91.0</td>\n",
       "      <td>2.1675</td>\n",
       "      <td>1</td>\n",
       "      <td>273</td>\n",
       "      <td>21.0</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>504</th>\n",
       "      <td>0.10959</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.794</td>\n",
       "      <td>89.3</td>\n",
       "      <td>2.3889</td>\n",
       "      <td>1</td>\n",
       "      <td>273</td>\n",
       "      <td>21.0</td>\n",
       "      <td>393.45</td>\n",
       "      <td>6.48</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>394 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        CRIM    ZN  INDUS  CHAS    NOX     RM   AGE     DIS  RAD  TAX  \\\n",
       "0    0.00632  18.0   2.31   0.0  0.538  6.575  65.2  4.0900    1  296   \n",
       "1    0.02731   0.0   7.07   0.0  0.469  6.421  78.9  4.9671    2  242   \n",
       "2    0.02729   0.0   7.07   0.0  0.469  7.185  61.1  4.9671    2  242   \n",
       "3    0.03237   0.0   2.18   0.0  0.458  6.998  45.8  6.0622    3  222   \n",
       "5    0.02985   0.0   2.18   0.0  0.458  6.430  58.7  6.0622    3  222   \n",
       "..       ...   ...    ...   ...    ...    ...   ...     ...  ...  ...   \n",
       "499  0.17783   0.0   9.69   0.0  0.585  5.569  73.5  2.3999    6  391   \n",
       "500  0.22438   0.0   9.69   0.0  0.585  6.027  79.7  2.4982    6  391   \n",
       "502  0.04527   0.0  11.93   0.0  0.573  6.120  76.7  2.2875    1  273   \n",
       "503  0.06076   0.0  11.93   0.0  0.573  6.976  91.0  2.1675    1  273   \n",
       "504  0.10959   0.0  11.93   0.0  0.573  6.794  89.3  2.3889    1  273   \n",
       "\n",
       "     PTRATIO       B  LSTAT  \n",
       "0       15.3  396.90   4.98  \n",
       "1       17.8  396.90   9.14  \n",
       "2       17.8  392.83   4.03  \n",
       "3       18.7  394.63   2.94  \n",
       "5       18.7  394.12   5.21  \n",
       "..       ...     ...    ...  \n",
       "499     19.2  395.77  15.10  \n",
       "500     19.2  396.90  14.33  \n",
       "502     21.0  396.90   9.08  \n",
       "503     21.0  396.90   5.64  \n",
       "504     21.0  393.45   6.48  \n",
       "\n",
       "[394 rows x 13 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.8700e-02 8.5000e+01 4.1500e+00 0.0000e+00 4.2900e-01 6.5160e+00\n",
      "  2.7700e+01 8.5353e+00 4.0000e+00 3.5100e+02 1.7900e+01 3.9243e+02\n",
      "  6.3600e+00]\n",
      " [1.3810e-02 8.0000e+01 4.6000e-01 0.0000e+00 4.2200e-01 7.8750e+00\n",
      "  3.2000e+01 5.6484e+00 4.0000e+00 2.5500e+02 1.4400e+01 3.9423e+02\n",
      "  2.9700e+00]\n",
      " [7.0130e-02 0.0000e+00 1.3890e+01 0.0000e+00 5.5000e-01 6.6420e+00\n",
      "  8.5100e+01 3.4211e+00 5.0000e+00 2.7600e+02 1.6400e+01 3.9278e+02\n",
      "  9.6900e+00]]\n",
      "X_train shape: (315, 13)\n",
      "y_train shape: (315, 1)\n",
      "X_test shape: (79, 13)\n",
      "y_test shape: (79, 1)\n"
     ]
    }
   ],
   "source": [
    "# Train Test Split\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Train Test Split\n",
    "# Training Data = 80% of Dataset\n",
    "# Test Data = 20% of Dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(features_df, labels_df, test_size=0.2, random_state=101)\n",
    "\n",
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)\n",
    "X_test = np.array(X_test)\n",
    "y_test = np.array(y_test)\n",
    "print(X_train[:3])\n",
    "print('X_train shape: {}'.format(X_train.shape))\n",
    "print('y_train shape: {}'.format(y_train.shape))\n",
    "print('X_test shape: {}'.format(X_test.shape))\n",
    "print('y_test shape: {}'.format(y_test.shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "# Define the Preprocessing Method and Fit Training Data to it\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train_normal = scaler.transform(X_train)\n",
    "X_test_normal = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = tf.Variable(tf.random.normal([X_train.shape[1],1], seed=1))\n",
    "b = tf.Variable(tf.random.normal([1], seed=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 580.3335571289062\n",
      "loss: 376.5743408203125\n",
      "loss: 247.31219482421875\n",
      "loss: 165.15151977539062\n",
      "loss: 112.84578704833984\n",
      "loss: 79.49857330322266\n",
      "loss: 58.208824157714844\n",
      "loss: 44.597412109375\n",
      "loss: 35.8811149597168\n",
      "loss: 30.288616180419922\n",
      "loss: 26.69144058227539\n",
      "loss: 24.369943618774414\n",
      "loss: 22.86485481262207\n",
      "loss: 21.882844924926758\n",
      "loss: 21.2364559173584\n",
      "loss: 20.805809020996094\n",
      "loss: 20.514188766479492\n",
      "loss: 20.312461853027344\n",
      "loss: 20.169126510620117\n",
      "loss: 20.063962936401367\n",
      "loss: 19.98398208618164\n",
      "loss: 19.920804977416992\n",
      "loss: 19.8690242767334\n",
      "loss: 19.825124740600586\n",
      "loss: 19.78682518005371\n",
      "loss: 19.75261878967285\n",
      "loss: 19.721506118774414\n",
      "loss: 19.692819595336914\n",
      "loss: 19.666095733642578\n",
      "loss: 19.641019821166992\n",
      "loss: 19.617359161376953\n",
      "loss: 19.59494400024414\n",
      "loss: 19.57364273071289\n",
      "loss: 19.55335807800293\n",
      "loss: 19.53400421142578\n",
      "loss: 19.515512466430664\n",
      "loss: 19.49782371520996\n",
      "loss: 19.48088836669922\n",
      "loss: 19.464656829833984\n",
      "loss: 19.4490909576416\n",
      "loss: 19.43415069580078\n",
      "loss: 19.419801712036133\n",
      "loss: 19.40601348876953\n",
      "loss: 19.39275550842285\n",
      "loss: 19.380002975463867\n",
      "loss: 19.36773109436035\n",
      "loss: 19.355913162231445\n",
      "loss: 19.344526290893555\n",
      "loss: 19.333553314208984\n",
      "loss: 19.32297134399414\n",
      "loss: 19.31276512145996\n",
      "loss: 19.30291748046875\n",
      "loss: 19.293411254882812\n",
      "loss: 19.28423500061035\n",
      "loss: 19.27536964416504\n",
      "loss: 19.266803741455078\n",
      "loss: 19.25852394104004\n",
      "loss: 19.250520706176758\n",
      "loss: 19.242778778076172\n",
      "loss: 19.235292434692383\n",
      "loss: 19.228046417236328\n",
      "loss: 19.22103500366211\n",
      "loss: 19.21424674987793\n",
      "loss: 19.20767593383789\n",
      "loss: 19.201311111450195\n",
      "loss: 19.195146560668945\n",
      "loss: 19.18917465209961\n",
      "loss: 19.183387756347656\n",
      "loss: 19.177778244018555\n",
      "loss: 19.172338485717773\n",
      "loss: 19.16706657409668\n",
      "loss: 19.16195297241211\n",
      "loss: 19.15699577331543\n",
      "loss: 19.152189254760742\n",
      "loss: 19.147520065307617\n",
      "loss: 19.142995834350586\n",
      "loss: 19.138601303100586\n",
      "loss: 19.13433837890625\n",
      "loss: 19.13020133972168\n",
      "loss: 19.126182556152344\n",
      "loss: 19.122283935546875\n",
      "loss: 19.118499755859375\n",
      "loss: 19.114822387695312\n",
      "loss: 19.111251831054688\n",
      "loss: 19.107786178588867\n",
      "loss: 19.10441780090332\n",
      "loss: 19.101144790649414\n",
      "loss: 19.09796714782715\n",
      "loss: 19.094879150390625\n",
      "loss: 19.09187889099121\n",
      "loss: 19.088964462280273\n",
      "loss: 19.086132049560547\n",
      "loss: 19.083377838134766\n",
      "loss: 19.080703735351562\n",
      "loss: 19.078102111816406\n",
      "loss: 19.075578689575195\n",
      "loss: 19.0731201171875\n",
      "loss: 19.07073211669922\n",
      "loss: 19.068408966064453\n",
      "loss: 19.06615447998047\n",
      "loss: 19.063961029052734\n",
      "loss: 19.061826705932617\n",
      "loss: 19.059749603271484\n",
      "loss: 19.057737350463867\n",
      "loss: 19.055774688720703\n",
      "loss: 19.053865432739258\n",
      "loss: 19.052013397216797\n",
      "loss: 19.050209045410156\n",
      "loss: 19.048452377319336\n",
      "loss: 19.046749114990234\n",
      "loss: 19.045089721679688\n",
      "loss: 19.043476104736328\n",
      "loss: 19.041908264160156\n",
      "loss: 19.040380477905273\n",
      "loss: 19.03889274597168\n",
      "loss: 19.03744888305664\n",
      "loss: 19.036046981811523\n",
      "loss: 19.034679412841797\n",
      "loss: 19.033349990844727\n",
      "loss: 19.03205680847168\n",
      "loss: 19.030799865722656\n",
      "loss: 19.029573440551758\n",
      "loss: 19.028383255004883\n",
      "loss: 19.0272274017334\n",
      "loss: 19.026100158691406\n",
      "loss: 19.024999618530273\n",
      "loss: 19.023935317993164\n",
      "loss: 19.02289581298828\n",
      "loss: 19.021886825561523\n",
      "loss: 19.020906448364258\n",
      "loss: 19.019948959350586\n",
      "loss: 19.019020080566406\n",
      "loss: 19.018112182617188\n",
      "loss: 19.017234802246094\n",
      "loss: 19.016374588012695\n",
      "loss: 19.01554298400879\n",
      "loss: 19.014732360839844\n",
      "loss: 19.013940811157227\n",
      "loss: 19.013174057006836\n",
      "loss: 19.012428283691406\n",
      "loss: 19.011699676513672\n",
      "loss: 19.01099395751953\n",
      "loss: 19.01030158996582\n",
      "loss: 19.009634017944336\n",
      "loss: 19.008981704711914\n",
      "loss: 19.008346557617188\n",
      "loss: 19.007726669311523\n",
      "loss: 19.00712776184082\n",
      "loss: 19.006542205810547\n",
      "loss: 19.005971908569336\n",
      "loss: 19.005420684814453\n",
      "loss: 19.0048828125\n",
      "loss: 19.004358291625977\n",
      "loss: 19.003849029541016\n",
      "loss: 19.00334930419922\n",
      "loss: 19.002866744995117\n",
      "loss: 19.002395629882812\n",
      "loss: 19.00193977355957\n",
      "loss: 19.001493453979492\n",
      "loss: 19.001060485839844\n",
      "loss: 19.000638961791992\n",
      "loss: 19.000228881835938\n",
      "loss: 18.999826431274414\n",
      "loss: 18.999441146850586\n",
      "loss: 18.999061584472656\n",
      "loss: 18.998693466186523\n",
      "loss: 18.998332977294922\n",
      "loss: 18.997987747192383\n",
      "loss: 18.99764633178711\n",
      "loss: 18.997316360473633\n",
      "loss: 18.996994018554688\n",
      "loss: 18.996681213378906\n",
      "loss: 18.99637794494629\n",
      "loss: 18.99608039855957\n",
      "loss: 18.995792388916016\n",
      "loss: 18.99551010131836\n",
      "loss: 18.9952392578125\n",
      "loss: 18.99497413635254\n",
      "loss: 18.99471664428711\n",
      "loss: 18.994462966918945\n",
      "loss: 18.994216918945312\n",
      "loss: 18.99397850036621\n",
      "loss: 18.99374771118164\n",
      "loss: 18.9935245513916\n",
      "loss: 18.993301391601562\n",
      "loss: 18.993087768554688\n",
      "loss: 18.99287986755371\n",
      "loss: 18.992679595947266\n",
      "loss: 18.99247932434082\n",
      "loss: 18.99228858947754\n",
      "loss: 18.99209976196289\n",
      "loss: 18.991918563842773\n",
      "loss: 18.991744995117188\n",
      "loss: 18.99156951904297\n",
      "loss: 18.99139976501465\n",
      "loss: 18.991241455078125\n",
      "loss: 18.99108123779297\n",
      "loss: 18.990924835205078\n",
      "loss: 18.99077606201172\n",
      "loss: 18.990629196166992\n",
      "loss: 18.99048614501953\n",
      "loss: 18.99034881591797\n",
      "loss: 18.99021339416504\n",
      "loss: 18.990081787109375\n",
      "loss: 18.989953994750977\n",
      "loss: 18.989830017089844\n",
      "loss: 18.98971176147461\n",
      "loss: 18.989593505859375\n",
      "loss: 18.989477157592773\n",
      "loss: 18.98936653137207\n",
      "loss: 18.9892578125\n",
      "loss: 18.989151000976562\n",
      "loss: 18.98904800415039\n",
      "loss: 18.988948822021484\n",
      "loss: 18.98885154724121\n",
      "loss: 18.98875617980957\n",
      "loss: 18.988664627075195\n",
      "loss: 18.988574981689453\n",
      "loss: 18.988487243652344\n",
      "loss: 18.988399505615234\n",
      "loss: 18.988319396972656\n",
      "loss: 18.988239288330078\n",
      "loss: 18.9881591796875\n",
      "loss: 18.98808479309082\n",
      "loss: 18.988008499145508\n",
      "loss: 18.987937927246094\n",
      "loss: 18.987865447998047\n",
      "loss: 18.987796783447266\n",
      "loss: 18.987733840942383\n",
      "loss: 18.9876651763916\n",
      "loss: 18.98760414123535\n",
      "loss: 18.987545013427734\n",
      "loss: 18.98748207092285\n",
      "loss: 18.987424850463867\n",
      "loss: 18.987367630004883\n",
      "loss: 18.98731231689453\n",
      "loss: 18.987260818481445\n",
      "loss: 18.987205505371094\n",
      "loss: 18.98715591430664\n",
      "loss: 18.987106323242188\n",
      "loss: 18.987056732177734\n",
      "loss: 18.98701286315918\n",
      "loss: 18.98696517944336\n",
      "loss: 18.986921310424805\n",
      "loss: 18.986879348754883\n",
      "loss: 18.98683738708496\n",
      "loss: 18.98679542541504\n",
      "loss: 18.98675537109375\n",
      "loss: 18.986719131469727\n",
      "loss: 18.986679077148438\n",
      "loss: 18.986642837524414\n",
      "loss: 18.986604690551758\n",
      "loss: 18.986572265625\n",
      "loss: 18.98653793334961\n",
      "loss: 18.98650550842285\n",
      "loss: 18.98647117614746\n",
      "loss: 18.98644256591797\n",
      "loss: 18.98641014099121\n",
      "loss: 18.98638153076172\n",
      "loss: 18.986352920532227\n",
      "loss: 18.986324310302734\n",
      "loss: 18.986299514770508\n",
      "loss: 18.98627281188965\n",
      "loss: 18.98624610900879\n",
      "loss: 18.986221313476562\n",
      "loss: 18.98619842529297\n",
      "loss: 18.98617172241211\n",
      "loss: 18.98615074157715\n",
      "loss: 18.986125946044922\n",
      "loss: 18.986103057861328\n",
      "loss: 18.986080169677734\n",
      "loss: 18.986064910888672\n",
      "loss: 18.986042022705078\n",
      "loss: 18.986021041870117\n",
      "loss: 18.986003875732422\n",
      "loss: 18.985984802246094\n",
      "loss: 18.985965728759766\n",
      "loss: 18.985946655273438\n",
      "loss: 18.985929489135742\n",
      "loss: 18.985916137695312\n",
      "loss: 18.98590087890625\n",
      "loss: 18.985883712768555\n",
      "loss: 18.985868453979492\n",
      "loss: 18.98585319519043\n",
      "loss: 18.985837936401367\n",
      "loss: 18.985824584960938\n",
      "loss: 18.985811233520508\n",
      "loss: 18.985795974731445\n",
      "loss: 18.985782623291016\n",
      "loss: 18.98577117919922\n",
      "loss: 18.985755920410156\n",
      "loss: 18.98574447631836\n",
      "loss: 18.985733032226562\n",
      "loss: 18.9857234954834\n",
      "loss: 18.9857120513916\n",
      "loss: 18.985700607299805\n",
      "loss: 18.98569107055664\n",
      "loss: 18.985681533813477\n",
      "loss: 18.98567008972168\n",
      "loss: 18.985660552978516\n",
      "loss: 18.98565101623535\n",
      "loss: 18.985641479492188\n",
      "loss: 18.985633850097656\n",
      "loss: 18.985624313354492\n",
      "loss: 18.985614776611328\n",
      "loss: 18.985605239868164\n",
      "loss: 18.985599517822266\n",
      "loss: 18.98558807373047\n",
      "loss: 18.985584259033203\n",
      "loss: 18.985576629638672\n",
      "loss: 18.98556900024414\n",
      "loss: 18.98556137084961\n",
      "loss: 18.985553741455078\n",
      "loss: 18.985549926757812\n",
      "loss: 18.98554229736328\n",
      "loss: 18.985536575317383\n",
      "loss: 18.98552894592285\n",
      "loss: 18.98552131652832\n",
      "loss: 18.985517501831055\n",
      "loss: 18.98551368713379\n",
      "loss: 18.985504150390625\n",
      "loss: 18.985502243041992\n",
      "loss: 18.985496520996094\n",
      "loss: 18.985490798950195\n",
      "loss: 18.98548698425293\n",
      "loss: 18.9854793548584\n",
      "loss: 18.985475540161133\n",
      "loss: 18.985471725463867\n",
      "loss: 18.985469818115234\n",
      "loss: 18.985464096069336\n",
      "loss: 18.98546028137207\n",
      "loss: 18.985454559326172\n",
      "loss: 18.985448837280273\n",
      "loss: 18.98544692993164\n",
      "loss: 18.985445022583008\n",
      "loss: 18.98543930053711\n",
      "loss: 18.985437393188477\n",
      "loss: 18.98543357849121\n",
      "loss: 18.985429763793945\n",
      "loss: 18.98542594909668\n",
      "loss: 18.985422134399414\n",
      "loss: 18.98542022705078\n",
      "loss: 18.985416412353516\n",
      "loss: 18.985414505004883\n",
      "loss: 18.985410690307617\n",
      "loss: 18.985408782958984\n",
      "loss: 18.98540687561035\n",
      "loss: 18.985403060913086\n",
      "loss: 18.98539924621582\n",
      "loss: 18.985397338867188\n",
      "loss: 18.985397338867188\n",
      "loss: 18.98539161682129\n",
      "loss: 18.98539161682129\n",
      "loss: 18.985387802124023\n",
      "loss: 18.98538589477539\n",
      "loss: 18.985382080078125\n",
      "loss: 18.985382080078125\n",
      "loss: 18.985380172729492\n",
      "loss: 18.985380172729492\n",
      "loss: 18.985376358032227\n",
      "loss: 18.98537254333496\n",
      "loss: 18.98537254333496\n",
      "loss: 18.985370635986328\n",
      "loss: 18.985366821289062\n",
      "loss: 18.98536491394043\n",
      "loss: 18.98536491394043\n",
      "loss: 18.98536491394043\n",
      "loss: 18.985363006591797\n",
      "loss: 18.985361099243164\n",
      "loss: 18.98535919189453\n",
      "loss: 18.9853572845459\n",
      "loss: 18.985355377197266\n",
      "loss: 18.985355377197266\n",
      "loss: 18.985353469848633\n",
      "loss: 18.985353469848633\n",
      "loss: 18.9853515625\n",
      "loss: 18.9853515625\n",
      "loss: 18.985349655151367\n",
      "loss: 18.985347747802734\n",
      "loss: 18.985347747802734\n",
      "loss: 18.98534393310547\n",
      "loss: 18.9853458404541\n",
      "loss: 18.98534393310547\n",
      "loss: 18.985342025756836\n",
      "loss: 18.985340118408203\n",
      "loss: 18.985340118408203\n",
      "loss: 18.98533821105957\n",
      "loss: 18.98533821105957\n",
      "loss: 18.98533821105957\n",
      "loss: 18.985336303710938\n",
      "loss: 18.98533821105957\n",
      "loss: 18.985334396362305\n",
      "loss: 18.985332489013672\n",
      "loss: 18.985332489013672\n",
      "loss: 18.985334396362305\n",
      "loss: 18.98533058166504\n",
      "loss: 18.98533058166504\n",
      "loss: 18.98533058166504\n",
      "loss: 18.98533058166504\n",
      "loss: 18.98533058166504\n",
      "loss: 18.98533058166504\n",
      "loss: 18.985326766967773\n",
      "loss: 18.985328674316406\n",
      "loss: 18.985328674316406\n",
      "loss: 18.98532485961914\n",
      "loss: 18.98532485961914\n",
      "loss: 18.98532485961914\n",
      "loss: 18.985326766967773\n",
      "loss: 18.985326766967773\n",
      "loss: 18.985322952270508\n",
      "loss: 18.985322952270508\n",
      "loss: 18.985322952270508\n",
      "loss: 18.985322952270508\n",
      "loss: 18.98532485961914\n",
      "loss: 18.985322952270508\n",
      "loss: 18.985321044921875\n",
      "loss: 18.985322952270508\n",
      "loss: 18.985322952270508\n",
      "loss: 18.985319137573242\n",
      "loss: 18.985321044921875\n",
      "loss: 18.985321044921875\n",
      "loss: 18.98531723022461\n",
      "loss: 18.985319137573242\n",
      "loss: 18.985319137573242\n",
      "loss: 18.98531723022461\n",
      "loss: 18.985319137573242\n",
      "loss: 18.985319137573242\n",
      "loss: 18.98531723022461\n",
      "loss: 18.98531723022461\n",
      "loss: 18.98531723022461\n",
      "loss: 18.985315322875977\n",
      "loss: 18.985313415527344\n",
      "loss: 18.98531723022461\n",
      "loss: 18.985315322875977\n",
      "loss: 18.985315322875977\n",
      "loss: 18.985313415527344\n",
      "loss: 18.985313415527344\n",
      "loss: 18.985313415527344\n",
      "loss: 18.985313415527344\n",
      "loss: 18.985313415527344\n",
      "loss: 18.985315322875977\n",
      "loss: 18.985313415527344\n",
      "loss: 18.985313415527344\n",
      "loss: 18.985313415527344\n",
      "loss: 18.985313415527344\n",
      "loss: 18.985313415527344\n",
      "loss: 18.985313415527344\n",
      "loss: 18.985313415527344\n",
      "loss: 18.985309600830078\n",
      "loss: 18.985313415527344\n",
      "loss: 18.98531150817871\n",
      "loss: 18.985313415527344\n",
      "loss: 18.98531150817871\n",
      "loss: 18.98531150817871\n",
      "loss: 18.98531150817871\n",
      "loss: 18.98531150817871\n",
      "loss: 18.98531150817871\n",
      "loss: 18.98531150817871\n",
      "loss: 18.98531150817871\n",
      "loss: 18.98531150817871\n",
      "loss: 18.98531150817871\n",
      "loss: 18.985313415527344\n",
      "loss: 18.985309600830078\n",
      "loss: 18.98531150817871\n",
      "loss: 18.985309600830078\n",
      "loss: 18.985309600830078\n",
      "loss: 18.985309600830078\n",
      "loss: 18.985309600830078\n",
      "loss: 18.985309600830078\n",
      "loss: 18.985309600830078\n",
      "loss: 18.985307693481445\n",
      "loss: 18.98531150817871\n",
      "loss: 18.985307693481445\n",
      "loss: 18.985309600830078\n",
      "loss: 18.985307693481445\n",
      "loss: 18.98531150817871\n",
      "loss: 18.985307693481445\n",
      "loss: 18.985309600830078\n",
      "loss: 18.985307693481445\n",
      "loss: 18.985307693481445\n",
      "loss: 18.985307693481445\n",
      "loss: 18.985307693481445\n",
      "loss: 18.985307693481445\n",
      "loss: 18.985307693481445\n",
      "loss: 18.985309600830078\n",
      "loss: 18.985309600830078\n",
      "loss: 18.985307693481445\n",
      "loss: 18.985309600830078\n",
      "loss: 18.985307693481445\n",
      "loss: 18.985309600830078\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985309600830078\n",
      "loss: 18.985309600830078\n",
      "loss: 18.985309600830078\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985309600830078\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985309600830078\n",
      "loss: 18.985307693481445\n",
      "loss: 18.98531150817871\n",
      "loss: 18.98531150817871\n",
      "loss: 18.985307693481445\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985307693481445\n",
      "loss: 18.985307693481445\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985307693481445\n",
      "loss: 18.985307693481445\n",
      "loss: 18.985307693481445\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985309600830078\n",
      "loss: 18.985307693481445\n",
      "loss: 18.985307693481445\n",
      "loss: 18.985307693481445\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985307693481445\n",
      "loss: 18.985307693481445\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985307693481445\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985309600830078\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985307693481445\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985307693481445\n",
      "loss: 18.985307693481445\n",
      "loss: 18.985307693481445\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985307693481445\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985307693481445\n",
      "loss: 18.985307693481445\n",
      "loss: 18.985307693481445\n",
      "loss: 18.985307693481445\n",
      "loss: 18.985307693481445\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985307693481445\n",
      "loss: 18.985307693481445\n",
      "loss: 18.985307693481445\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985307693481445\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985307693481445\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985307693481445\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985307693481445\n",
      "loss: 18.985307693481445\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985307693481445\n",
      "loss: 18.985307693481445\n",
      "loss: 18.985307693481445\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985307693481445\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985307693481445\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985307693481445\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985307693481445\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985309600830078\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985307693481445\n",
      "loss: 18.985307693481445\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.98530387878418\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.98530387878418\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.98530387878418\n",
      "loss: 18.985305786132812\n",
      "loss: 18.98530387878418\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.98530387878418\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985307693481445\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985307693481445\n",
      "loss: 18.985309600830078\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985307693481445\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.98530387878418\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.98530387878418\n",
      "loss: 18.98530387878418\n",
      "loss: 18.98530387878418\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.98530387878418\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.98530387878418\n",
      "loss: 18.98530387878418\n",
      "loss: 18.98530387878418\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.98530387878418\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.98530387878418\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985307693481445\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985307693481445\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n",
      "loss: 18.985305786132812\n"
     ]
    }
   ],
   "source": [
    "\n",
    "opt = tf.keras.optimizers.SGD(learning_rate=0.1)\n",
    "for i in range(1000):\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        y_pred = tf.linalg.matmul(tf.cast(X_train_normal, dtype=tf.float32), W) + b\n",
    "        loss = tf.reduce_mean(tf.square(y_pred - y_train))\n",
    "        print('loss: {}'.format(loss))\n",
    "    grads = tape.gradient(loss, [W, b])\n",
    "    processed_grads = [g for g in grads]\n",
    "    grads_and_vars = zip(processed_grads, [W, b])\n",
    "    \n",
    "    opt.apply_gradients(grads_and_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new_W: [[-0.73502016]\n",
      " [ 0.983833  ]\n",
      " [ 0.39395586]\n",
      " [ 0.75658643]\n",
      " [-2.1291883 ]\n",
      " [ 3.0072005 ]\n",
      " [-0.46047762]\n",
      " [-2.7603276 ]\n",
      " [ 2.301846  ]\n",
      " [-2.1114612 ]\n",
      " [-2.1529114 ]\n",
      " [ 0.8609485 ]\n",
      " [-3.0763924 ]]\n",
      "new_b: [22.294916]\n"
     ]
    }
   ],
   "source": [
    "print('new_W: {}'.format(W.numpy()))\n",
    "print('new_b: {}'.format(b.numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing data mse: 21.72530174255371\n"
     ]
    }
   ],
   "source": [
    "y_test_pred = tf.linalg.matmul(tf.cast(X_test_normal, dtype=tf.float32), W) + b\n",
    "loss = tf.reduce_mean(tf.square(y_test_pred - y_test))\n",
    "print('testing data mse: {}'.format(loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
